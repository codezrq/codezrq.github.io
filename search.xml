<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Docker安装以及Docker常用命令</title>
    <url>/2025/02/03/Docker-install/</url>
    <content><![CDATA[<p>Linux下的Docker安装过程，包括Docker的安装，卸载，更换软件源，配置用户组，已经Docker使用过程中的常见命令。</p>
<span id="more"></span>
<h3 id="Ubuntu-下-Docker-安装以及常见操作"><a href="#Ubuntu-下-Docker-安装以及常见操作" class="headerlink" title="Ubuntu 下 Docker 安装以及常见操作"></a><center>Ubuntu 下 Docker 安装以及常见操作</center></h3><h4 id="一-Docker-安装"><a href="#一-Docker-安装" class="headerlink" title="一. Docker 安装"></a>一. Docker 安装</h4><h5 id="1-卸载旧Docker"><a href="#1-卸载旧Docker" class="headerlink" title="1. 卸载旧Docker"></a>1. 卸载旧Docker</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo apt-get remove docker docker-engine docker.io containerd runc</span><br></pre></td></tr></table></figure>
<h5 id="2-安装docker依赖"><a href="#2-安装docker依赖" class="headerlink" title="2. 安装docker依赖"></a>2. 安装docker依赖</h5><p>Docker在Ubuntu上依赖一些软件包。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo apt update sudo apt upgrade</span><br><span class="line">sudo apt-get install ca-certificates curl gnupg lsb-release</span><br></pre></td></tr></table></figure>
<h5 id="3-添加秘钥"><a href="#3-添加秘钥" class="headerlink" title="3. 添加秘钥"></a>3. 添加秘钥</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">curl -fsSL http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -</span><br></pre></td></tr></table></figure>
<h5 id="4-添加软件源"><a href="#4-添加软件源" class="headerlink" title="4. 添加软件源"></a>4. 添加软件源</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository &quot;deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable&quot;</span><br></pre></td></tr></table></figure>
<h5 id="5-安装Docker"><a href="#5-安装Docker" class="headerlink" title="5. 安装Docker"></a>5. 安装Docker</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">apt-get install docker-ce docker-ce-cli containerd.io</span><br></pre></td></tr></table></figure>
<h4 id="二-配置用户组"><a href="#二-配置用户组" class="headerlink" title="二. 配置用户组"></a>二. 配置用户组</h4><p>默认情况下，只有root用户和docker组的用户才能运行Docker命令。我们可以将当前用户添加到docker组，以避免每次使用Docker时都需要使用sudo。</p>
<p>如果出现“启动“docker.service”需要认证。Multiple identities can be used for authentication:”的报错，说明是没有将当前用户加入到docker用户组中。</p>
<p><strong>创建docker组</strong>(如果已经有，则不用创建)</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo groupadd docker</span><br></pre></td></tr></table></figure>
<p><strong>将用户加入用户组</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo usermod -aG docker $USER</span><br></pre></td></tr></table></figure>
<p><strong>重新登陆</strong></p>
<p><strong>刷新用户组</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">newgrp docker</span><br></pre></td></tr></table></figure>
<p>可以通过一下命令查看存在的用户组</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">newgrp docker</span><br></pre></td></tr></table></figure>
<p><strong>测试能否使用docker:</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker run hello-world</span><br></pre></td></tr></table></figure>
<h4 id="三-常用命令"><a href="#三-常用命令" class="headerlink" title="三.常用命令"></a>三.常用命令</h4><h5 id="1-基础命令"><a href="#1-基础命令" class="headerlink" title="1. 基础命令"></a>1. 基础命令</h5><p><strong>查看docker版本信息</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker version</span><br><span class="line">docker info</span><br></pre></td></tr></table></figure>
<p><strong>启动 docker</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl start docker</span><br></pre></td></tr></table></figure>
<p><strong>关闭 docker</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl stop docker</span><br></pre></td></tr></table></figure>
<p><strong>重启 docker</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl restart docker</span><br></pre></td></tr></table></figure>
<p><strong>设置docker随服务启动而启动</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl enable docker</span><br></pre></td></tr></table></figure>
<p><strong>查看docker运行状态</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl status docker</span><br></pre></td></tr></table></figure>
<p>如果在运行中，输入命令后会看到绿色的 active(running)</p>
<h5 id="2-镜像命令"><a href="#2-镜像命令" class="headerlink" title="2. 镜像命令"></a>2. 镜像命令</h5><p><strong>查看镜像列表</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker images</span><br></pre></td></tr></table></figure>
<p><strong>搜索镜像</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker search 镜像名</span><br><span class="line">docker search --filter=STARS=9000 mysql 搜索 STARS &gt;9000的 mysql 镜像</span><br></pre></td></tr></table></figure>
<p><strong>拉取镜像</strong></p>
<p><strong>拉取镜像</strong> 不加tag(版本号) 即拉取docker仓库中该镜像的最新版本latest，加:tag则是拉取指定版本</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker pull 镜像名 </span><br><span class="line">docker pull 镜像名:tag</span><br></pre></td></tr></table></figure>
<p><strong>运行镜像</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker run 镜像名</span><br><span class="line">docker run 镜像名:Tag</span><br></pre></td></tr></table></figure>
<h5 id="3-容器命令"><a href="#3-容器命令" class="headerlink" title="3. 容器命令"></a>3. 容器命令</h5><p><strong>查看运行中的容器</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker ps</span><br></pre></td></tr></table></figure>
<p><strong>查看所有容器</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker ps -a</span><br></pre></td></tr></table></figure>
<p><strong>启动容器</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker start 容器id或容器名</span><br></pre></td></tr></table></figure>
<p><strong>停止容器</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker stop 容器id或容器名</span><br></pre></td></tr></table></figure>
<p><strong>查看容器的所有信息</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker inspect 容器id</span><br></pre></td></tr></table></figure>
<p><strong>查看容器日志</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">doker container logs 容器id</span><br></pre></td></tr></table></figure>
<p><strong>查看容器里的进程</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker top 容器id</span><br></pre></td></tr></table></figure>
<p><strong>退出容器</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">exit</span><br></pre></td></tr></table></figure>
<p><strong>删除已停止的容器</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker rm 容器id或name</span><br></pre></td></tr></table></figure>
<p><strong>删除正在运行的容器</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker rm -f 容器id</span><br></pre></td></tr></table></figure>
<p><strong>进入容器</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker exec -it 容器ID sh</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>配置</category>
      </categories>
      <tags>
        <tag>configure</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux下vim的使用</title>
    <url>/2025/02/03/Linux%E4%B8%8Bvim%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<p>ubuntu 下 vim 的使用。包括下载命令，打开文件命令，修改文件命令。</p>
<span id="more"></span>
<h3 id="ubuntu-下-vim-的使用"><a href="#ubuntu-下-vim-的使用" class="headerlink" title="ubuntu 下 vim 的使用"></a><center>ubuntu 下 vim 的使用</center></h3><h4 id="1-vim-下载"><a href="#1-vim-下载" class="headerlink" title="1. vim 下载"></a>1. vim 下载</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo apt-get install vim</span><br></pre></td></tr></table></figure>
<h4 id="2-vim-使用命令"><a href="#2-vim-使用命令" class="headerlink" title="2. vim 使用命令"></a>2. vim 使用命令</h4><p>打开文件<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo vim /etc/apt/sources.list</span><br><span class="line">//格式为sudo vim 文件位置（精确到文件名称）</span><br></pre></td></tr></table></figure></p>
<p>1.命令模式<br><code>i</code>   切换到输入模式，左下角出现–输入–<br><code>x</code>    删除当前光标所在处字符</p>
<p>2.输入模式<br>删除、换行、上下移动翻页、退格、输入等和平时输入一样。<br><code>esc</code>    退出输入模式</p>
<p>3.底线命令模式</p>
<p>修改完后，按 <code>esc</code> 后，输入<br><code>:wq</code>  退出程序并保存文件<br><code>:w</code> 保存文件<br><code>:q</code> 退出程序</p>
<p>如果加上 <code>!</code>，则表示强制命令，(<code>:!wq</code>，<code>:!w</code>，<code>:!q</code>)</p>
]]></content>
      <categories>
        <category>配置</category>
      </categories>
      <tags>
        <tag>configure</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Anaconda 常见命令</title>
    <url>/2025/02/03/anaconda-opts/</url>
    <content><![CDATA[<p>conda 常见命令，包括环境管理(创建，激活，退出，删除，导出配置)、包管理（查看、安装、更新、搜索、卸载）</p>
<span id="more"></span>
<h3 id="conda-常见命令"><a href="#conda-常见命令" class="headerlink" title="conda 常见命令"></a><center>conda 常见命令</center></h3><h4 id="一-环境管理"><a href="#一-环境管理" class="headerlink" title="一.环境管理"></a>一.环境管理</h4><h5 id="1-创建环境"><a href="#1-创建环境" class="headerlink" title="1. 创建环境"></a>1. 创建环境</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda create -n [your_env_name] python=x.x   #创建指定版本的python环境</span><br><span class="line">conda env create -n [your_env_name] -f environment.ymal # 从文件中创建环境</span><br><span class="line">conda create -n [your_env_name] --clone [source env] # 复制已有环境</span><br></pre></td></tr></table></figure>
<h5 id="2-查看有哪些环境"><a href="#2-查看有哪些环境" class="headerlink" title="2. 查看有哪些环境"></a>2. 查看有哪些环境</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda info --env</span><br></pre></td></tr></table></figure>
<h5 id="3-激活环境"><a href="#3-激活环境" class="headerlink" title="3. 激活环境"></a>3. 激活环境</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda activate [your_env_name]</span><br></pre></td></tr></table></figure>
<h5 id="4-退出环境"><a href="#4-退出环境" class="headerlink" title="4. 退出环境"></a>4. 退出环境</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda deactivate</span><br></pre></td></tr></table></figure>
<h5 id="5-删除环境"><a href="#5-删除环境" class="headerlink" title="5. 删除环境"></a>5. 删除环境</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda remove -n [your_env_name] --all</span><br></pre></td></tr></table></figure>
<h5 id="6-导出环境配置"><a href="#6-导出环境配置" class="headerlink" title="6. 导出环境配置"></a>6. 导出环境配置</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda env export &gt; environment.yaml</span><br></pre></td></tr></table></figure>
<h4 id="二-包管理"><a href="#二-包管理" class="headerlink" title="二.包管理"></a>二.包管理</h4><h5 id="1-查看安装了哪些包"><a href="#1-查看安装了哪些包" class="headerlink" title="1. 查看安装了哪些包"></a>1. 查看安装了哪些包</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda list</span><br></pre></td></tr></table></figure>
<h5 id="2-安装包"><a href="#2-安装包" class="headerlink" title="2. 安装包"></a>2. 安装包</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda install [package_name] # 安装最新版本</span><br><span class="line">conda install [package_name] # 安装特定版本</span><br></pre></td></tr></table></figure>
<h5 id="3-更新包"><a href="#3-更新包" class="headerlink" title="3. 更新包"></a>3. 更新包</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda remove [package_name] # 更新一个包</span><br><span class="line">conda update --all # 更新所有包</span><br></pre></td></tr></table></figure>
<h5 id="4-搜索包"><a href="#4-搜索包" class="headerlink" title="4. 搜索包"></a>4. 搜索包</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda search [pachage_name]</span><br></pre></td></tr></table></figure>
<h5 id="5-卸载包"><a href="#5-卸载包" class="headerlink" title="5. 卸载包"></a>5. 卸载包</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda remove [package_name]</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>配置</category>
      </categories>
      <tags>
        <tag>configure</tag>
      </tags>
  </entry>
  <entry>
    <title>使用hexo创建个人博客</title>
    <url>/2025/01/30/build-my-blog/</url>
    <content><![CDATA[<p>使用<code>github+hexo</code>创建个人博客</p>
<span id="more"></span>
<h4 id="1-下载工具"><a href="#1-下载工具" class="headerlink" title="1. 下载工具"></a>1. 下载工具</h4><p>自行下载node.js npm ,使用以下命令检查是否正确安装</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">node -v</span><br><span class="line">npm -v</span><br></pre></td></tr></table></figure>
<h4 id="2-在github上创建仓库"><a href="#2-在github上创建仓库" class="headerlink" title="2. 在github上创建仓库"></a>2. 在github上创建仓库</h4><p><img src="/2025/01/30/build-my-blog/image-20250128013605341.png" alt="image-20250128013605341"></p>
<h4 id="3-安装hexo"><a href="#3-安装hexo" class="headerlink" title="3.  安装hexo"></a>3.  安装hexo</h4><p>使用以下命令暗转hexo</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">npm install -g hexo -cli</span><br></pre></td></tr></table></figure>
<h4 id="4-创建Hexo文件夹"><a href="#4-创建Hexo文件夹" class="headerlink" title="4. 创建Hexo文件夹"></a>4. 创建Hexo文件夹</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hexo init &lt;folder&gt;</span><br><span class="line">cd &lt;folder&gt;</span><br><span class="line">npm install</span><br></pre></td></tr></table></figure>
<p>使用以下命令可以本地查看效果</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hexo server</span><br></pre></td></tr></table></figure>
<p>网页浏览: localhost:4000查看效果</p>
<p>初次登录会要求登录github, 现在github不支持密码登录了，只能通过token登录，去GitHub首页</p>
<p> <img src="/2025/01/30/build-my-blog/image-20250128014633141.png" alt="image-20250128014633141"></p>
<p>进入setting后拉到最下面的<code>Developer settings</code> 进行生成，期限选择永久。</p>
<p>修改<code>&lt;folder&gt;</code>下的配置文件<code>_config.yml</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repo: &lt;repository url&gt; # 库地址</span><br><span class="line">  branch: [branch]  # 分支，如main</span><br><span class="line">  message: [message] # 缺省为创建时间</span><br></pre></td></tr></table></figure>
<h4 id="5-发布文章"><a href="#5-发布文章" class="headerlink" title="5. 发布文章"></a>5. 发布文章</h4><p>进入site目录, 右键打开Git Bash, 创建博文:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hexo new &quot;My New Post&quot;</span><br></pre></td></tr></table></figure>
<p>然后source文件夹中会出现一个My New Post.md 文件，可以使用Markdown写文章。写完之后运行下面代码将文章渲染到GitHub Pages 上完成发布。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hexo g # 生成静态页面</span><br><span class="line">hexo d # 部署页面</span><br></pre></td></tr></table></figure>
<p>也可以不通过命令直接自己生成页面，但要在页面开始加入如下格式:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: Hello World # 标题</span><br><span class="line">date: 2019/3/26 hh:mm:ss # 时间</span><br><span class="line">categories: # 分类</span><br><span class="line">- Diary</span><br><span class="line">tags: # 标签</span><br><span class="line">- PS3</span><br><span class="line">- Games</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">摘要</span><br><span class="line">&lt;!--more--&gt;</span><br><span class="line">正文</span><br></pre></td></tr></table></figure>
<p>常用命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hexo new &quot;name&quot;       # 新建文章</span><br><span class="line">hexo new page &quot;name&quot;  # 新建页面</span><br><span class="line">hexo g                # 生成页面</span><br><span class="line">hexo d                # 部署</span><br><span class="line">hexo g -d             # 生成页面并部署</span><br><span class="line">hexo s                # 本地预览</span><br><span class="line">hexo clean            # 清除缓存和已生成的静态文件</span><br><span class="line">hexo help             # 帮助</span><br></pre></td></tr></table></figure>
<h4 id="6-访问"><a href="#6-访问" class="headerlink" title="6. 访问"></a>6. 访问</h4><p><a href="https://codezrq.github.io/">Hexo</a></p>
<h4 id="8-更换主题"><a href="#8-更换主题" class="headerlink" title="8. 更换主题"></a>8. 更换主题</h4><p>在<a href="https://hexo.io/themes/">Themes | Hexo</a>上选择主题，进入网站目录下载主题、</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git clone https://github.com/iissnan/hexo-theme-next themes/next</span><br></pre></td></tr></table></figure>
<p>然后修改 _config.yml 中的 theme 为新主题名称 next，发布。（有的主题需要将 _config.yml 替换为主题自带的，参考主题说明。）</p>
<h4 id="9-更改设置"><a href="#9-更改设置" class="headerlink" title="9. 更改设置"></a>9. 更改设置</h4><p>所有设置都在<code>_config.yml</code>目录下，参考官方文档<a href="https://hexo.io/zh-cn/docs/configuration">配置 | Hexo</a></p>
<h4 id="可能遇到的问题"><a href="#可能遇到的问题" class="headerlink" title="可能遇到的问题"></a>可能遇到的问题</h4><h5 id="1-在执行-hexo-d-时报错"><a href="#1-在执行-hexo-d-时报错" class="headerlink" title="1. 在执行 hexo d 时报错"></a>1. 在执行 hexo d 时报错</h5><p>fatal: 无法访问 ‘<a href="https://github.com/chixinn/chixinn.github.io.git/">https://github.com/chixinn/chixinn.github.io.git/</a>‘</p>
<p>解决办法： 其实时git push 的问题。先执行以下命令，再执行<code>hexo d</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git config --global --unset http.proxy</span><br></pre></td></tr></table></figure>
<h5 id="2-在执行-hexo-d-时报错"><a href="#2-在执行-hexo-d-时报错" class="headerlink" title="2.在执行 hexo d 时报错"></a>2.在执行 hexo d 时报错</h5><p>ERROR Deployer not found: git</p>
<p>解决办法: 没有安装<code>hexo-deployer-git</code></p>
<p>使用以下命令安装</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure>
<h5 id="3-修改并部署后没有效果"><a href="#3-修改并部署后没有效果" class="headerlink" title="3. 修改并部署后没有效果"></a>3. 修改并部署后没有效果</h5><p>使用 <code>hexo clean</code>清理后重新部署</p>
]]></content>
      <categories>
        <category>配置</category>
      </categories>
      <tags>
        <tag>configure</tag>
      </tags>
  </entry>
  <entry>
    <title>Energy Efficient Real-time Task Scheduling on CPU-GPU Hybrid Clusters</title>
    <url>/2025/02/03/scheduler-2/</url>
    <content><![CDATA[<p>通过动态电压和频率缩放研究了新兴CPU-GPU混合集群的节能问题。</p>
<span id="more"></span>
<h4 id="Energy-Efficient-Real-time-Task-Scheduling-on-CPU-GPU-Hybrid-Clusters"><a href="#Energy-Efficient-Real-time-Task-Scheduling-on-CPU-GPU-Hybrid-Clusters" class="headerlink" title="Energy Efficient Real-time Task Scheduling on CPU-GPU Hybrid Clusters"></a>Energy Efficient Real-time Task Scheduling on CPU-GPU Hybrid Clusters</h4><ul>
<li><p>出处：2017IEEE Xplore    <a href="https://ieeexplore.ieee.org/document/8057205">基于CPU-GPU混合集群的高效实时任务调度</a></p>
</li>
<li><p>主要工作：通过动态电压和频率缩放研究了新兴CPU-GPU混合集群的节能问题。</p>
<ul>
<li><p>首次分析GPU特定的DVFS模型。</p>
</li>
<li><p>设计了一种新的调度算法：1)利用GPU DVFS来节省能源而不违反任务期限；2)有效将一组任务打包到多个服务器上，以减少动态能耗；3)智能调节DVFS设定，更有效地节省能源。</p>
</li>
<li>仿真测试，可以节省多达36%的能耗。</li>
</ul>
</li>
<li><p>做出的假设：<strong>集群中只有一种GPU/CPU，但不同服务器可能有不同数量的GPU-CPU对，且每个任务只能分配给一个CPU-GPU对，每个CPU-GPU对一次只能执行一个任务。</strong></p>
</li>
<li><p>目标：最小化在截止日期限制下处理一系列实时任务的总能耗。考虑了对任务执行时间和功耗有显著影响的三个缩放变量：GPU核心电压、CPU核心频率、GPU内存频率。</p>
</li>
<li><p>方法：通过数学优化计算每个任务的合适电压/频率设置，并使用启发式调度算法将多个任务分配给集群。</p>
</li>
<li><p>数据中心常用的两种节能技术：</p>
<ul>
<li>DVFS：dynamic voltage and frequency scaling(动态电压和频率缩放)</li>
<li>DRS：dynamic resource sleep(动态资源休眠)</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
        <category>调度</category>
      </categories>
      <tags>
        <tag>scheduler</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文阅读】Efficient Memory Management for Large Language Model Serving with PagedAttention</title>
    <url>/2025/01/28/llm_1/</url>
    <content><![CDATA[<p>从虚拟内存的分页管理得到的灵感，用分页管理管理llm的kvcache，使其能够动态扩缩容，重用，碎片小。</p>
<span id="more"></span>
<h3 id="【论文阅读】Efficient-Memory-Management-for-Large-Language-Model-Serving-with-PagedAttention"><a href="#【论文阅读】Efficient-Memory-Management-for-Large-Language-Model-Serving-with-PagedAttention" class="headerlink" title="【论文阅读】Efficient Memory Management for Large Language Model Serving with PagedAttention"></a>【论文阅读】Efficient Memory Management for Large Language Model Serving with PagedAttention</h3><p>出处: ACM-2023   <a href="https://arxiv.org/abs/2309.06180">[2309.06180] Efficient Memory Management for Large Language Model Serving with PagedAttention (arxiv.org)</a></p>
<ul>
<li>源码: <a href="https://github.com/vllm-project/vllm.git">项目地址</a></li>
</ul>
<h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><p>高吞吐量的LLM服务需同时处理多个请求。但是现有系统非常困难，因为KV cache非常巨大并且是动态伸缩的，因为显存管理不善，导致碎片和重复，造成显存的巨大浪费，从而限制了batch的大小和吞吐量。为了解决这个问题，本文借鉴操作系统的分页内存管理方法，提出PagedAttention。基于这个方法，实现了vLLM，它能够实现：1) 接近零的KV cache浪费；2) 同一请求内和不同请求间KV cache的灵活共享。实验证明本方法的吞吐量是SOTA系统的2-4倍。</p>
<h4 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h4><p>像GPT和PaLM这样的大型语言模型(llm)的出现使编程助理和通用聊天机器人等新应用成为可能，它们开始深刻地影响我们的工作和日常生活。许多云计算公司[34,44]正在竞相提供这些应用程序作为托管服务。然而，运行这些应用程序是非常昂贵的，需要大量的硬件加速器，如gpu。</p>
<p>llm的核心是自回归的Transformer模型。该模型基于输入(提示)和迄今为止生成的输出标记的前一个序列，一次生成一个单词(token)。对于每个请求，重复这个昂贵的过程，直到模型输出一个终止令牌。这种顺序生成过程使工作负载受到内存限制，使gpu的计算能力得不到充分利用，并限制了服务吞吐量。</p>
<p> <img src="/2025/01/28/llm_1/90f34ab726a84868a23baf38716f1415.png" alt="在这里插入图片描述"></p>
<p>通过将多个请求批处理在一起，可以提高吞吐量。但是，为了批量处理许多请求，应该有效地管理每个请求的内存空间。例如，图1(左)说明了在具有40GB RAM的NVIDIA A100 GPU上13B参数LLM的内存分布。大约65%的内存分配给模型参数，这些权重在服务期间保持静态。接近30%的内存用于存储请求的动态状态。对于transformer，这些状态由与注意力机制相关的key值和value值组成，通常被称为KV cache，它表示来自早期令牌的上下文，以按顺序生成新的输出令牌。剩下的一小部分用于其他数据，包括激活——在评估LLM时产生的短暂张量。由于模型权重是恒定的，并且激活仅占用GPU内存的一小部分，因此管理KV缓存的方式对于确定最大批大小至关重要。当管理效率低下时，KV高速缓存会显著限制批处理大小，从而限制LLM的吞吐量，如图1(右)所示。</p>
<p>在本文中，我们观察到现有的LLM服务系统无法有效地管理KV缓存。这主要是因为它们将请求的KV缓存存储在连续内存空间中，因为大多数深度学习框架要求将张量存储在连续内存中。然而，与传统深度学习工作负载中的张量不同，KV cache 具有独特的特征:随着模型生成新的token，它会随着时间的推移动态增长和缩小，并且它的生命周期和长度是未知的。这些特点使现有系统的方法在两个方面显着效率低下：</p>
<p> <img src="/2025/01/28/llm_1/5b4d27afa1d44d4ba1fe56329043ad0d.png" alt="在这里插入图片描述"></p>
<p>首先，现有系统存在内部和外部内存碎片。为了在连续空间中存储请求的KV cache，它们预先分配了一个具有请求最大长度的连续内存块(例如，2048个token)。这可能导致严重的内部碎片，因为请求的实际长度可能比它的最大长度短得多(例如，图11)。此外，即使预先知道实际长度，预分配仍然是低效的:由于在请求的生命周期内保留了整个块，其他较短的请求不能利用当前未使用的块的任何部分。此外，外部内存碎片也很重要，因为每个请求的预分配大小可能不同。事实上，我们在图2中的分析结果显示，在现有系统中，只有20.4% - 38.2%的KV cache内存用于存储实际 token 状态。</p>
<p>其次，现有系统无法利用内存共享的机会。LLM服务通常使用高级解码算法，例如parallel sampling 和 beam search，每个请求生成多个输出。在这些场景中，请求由多个序列组成，这些序列可以部分共享它们的KV cache。然而，在现有系统中，内存共享是不可能的，因为序列的KV cache存储在单独的连续空间中。</p>
<p>为了解决上述限制，我们提出了PagedAttention，这是一种注意力算法，灵感来自于操作系统(OS)对内存碎片和共享的解决方案:带分页的虚拟内存。PagedAttention将请求的KV缓存划分为块，每个块可以包含固定数量的token的key和value。在PagedAttention中，KV cache的块不一定存储在连续的空间中。因此，我们可以像在OS的虚拟内存中那样更灵活地管理KV cache:可以将块视为页，token视为字节，request视为进程。这种设计通过使用相对较小的块并按需分配来减轻内部碎片。此外，它消除了外部碎片，因为所有块都具有相同的大小。最后，它支持以块粒度、跨与相同请求关联的不同序列甚至跨不同请求共享内存。</p>
<p>在这项工作中，我们在PagedAttention的基础上构建了一个高吞吐量的分布式LLM服务引擎vLLM，它在KV高速缓存中实现了接近零的浪费。vLLM使用与PagedAttention共同设计的块级内存管理和抢占式请求调度。vLLM支持GPT[5]、OPT[62]、LLaMA[52]等流行的llm，支持不同大小的llm，包括超过单个GPU内存容量的llm。我们对各种模型和工作负载的评估表明，与最先进的系统相比，vLLM将LLM服务吞吐量提高了2-4倍，而完全不影响模型的准确性。对于更长的序列、更大的模型和更复杂的解码算法，改进更加明显。</p>
<p>综上所述，我们做出了以下贡献:</p>
<ul>
<li>我们确定了在服务llm时内存分配方面的挑战，并量化了它们对服务性能的影响。</li>
<li>受操作系统中虚拟内存和分页的启发，提出了一种基于非连续分页内存中KV cache 的注意力算法PagedAttention。</li>
<li>我们设计并实现了vLLM，一个基于PagedAttention的分布式LLM服务引擎。</li>
<li>我们在各种情况下评估了vLLM，并证明它大大优于以前的核心解决方案，如FasterTransformer[31]和Orca[60]。</li>
</ul>
<h4 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h4><p>在本节中，我们描述了典型LLM的生成和服务过程，以及LLM服务中使用的iteration 调度。</p>
<h5 id="2-1-Transformer-Based-Large-Language-Models"><a href="#2-1-Transformer-Based-Large-Language-Models" class="headerlink" title="2.1 Transformer-Based Large Language Models"></a>2.1 Transformer-Based Large Language Models</h5><p>语言建模的任务是对标记列表 $(x_1, \dots, x_n)$ 的概率进行建模。由于语言具有自然的顺序排序，因此通常将整个序列的联合概率分解为条件概率的乘积(也称为自回归分解[3]):</p>
<script type="math/tex; mode=display">
P(x)=P(x_1) \cdot P(x_2|x_1)\cdot P(x_n|x_1,\cdots,x_{n-1})</script><p>Transformer[53]已经成为在大范围内对上述概率进行建模的事实上的标准架构。基于transformer的语言模型最重要的组件是它的自注意力层。对于输入隐藏层状态 $(x_1,\cdots,x_n) \in \mathbb{R}^{n\times d}$ ，自注意力层首先对每个位置向量进行线性变换，得到Query, key 和 value 向量:</p>
<script type="math/tex; mode=display">
q_i=W_qx_i,k_i=W_kx_i,v_i=W_vx_i</script><p>然后，自注意层通过将某一位置的 Query 向量与其之前的所有 key 向量相乘来计算关注分数 $a_{ij}$，并计算输出 $o_{i}$ 作为 value 向量的加权平均值:</p>
<script type="math/tex; mode=display">
a_{ij}=\frac{exp(q_i^Tk_j/\sqrt{d})}{\sum_{t=1}^i exp(q_i^Tk_t/\sqrt{d})}, o_i=\sum\limits_{j=1}^{i}a_{ij}v_{j}.</script><p>除Eq. 4中的计算外，Transformer模型中的所有其他组成部分，包括嵌入层、前馈层、层归一化[2]、剩余连接[22]、输出logit计算以及Eq. 2中的Query, key, value 值转换，都是按照位置独立应用的，形式为: $y_i=f(x_i)$。</p>
<h5 id="2-2-LLM-Service-amp-Autoregressive-Generation"><a href="#2-2-LLM-Service-amp-Autoregressive-Generation" class="headerlink" title="2.2 LLM Service &amp; Autoregressive Generation"></a>2.2 LLM Service &amp; Autoregressive Generation</h5><p>经过训练后，llm通常被部署为生成服务(例如，生成API[34]或聊天机器人[19,35])。对LLM服务的请求提供了一个输入提示token序列$(x_1,\cdots,x_n)$，LLM服务根据式(1)生成一个输出令牌列表$(x_{n+1},\cdots,x{n+T})$。我们将提示符列表和输出列表的连接称为序列。</p>
<p>LLM只能逐个采样并生成新的令牌，并且每个新token的生成过程取决于该序列中所有先前的tonken，特别是它们的key和value。在这个顺序生成过程中，通常缓存现有token的key和value向量，以生成未来的token，称为KV cache。注意，一个token的KV cache依赖于它之前的所有token。这意味着同一token在序列中出现在不同位置的KV cache 将是不同的。</p>
<p>给定一个请求prompt，LLM服务中的生成计算可以分解为两个阶段:</p>
<p><strong>The prompt phase: </strong> 将用户的整个prompt  $(x_1,\cdots,x_n)$ 作为输入计算第一个token的概率$P(x_{n+1}|x_1,\cdots,x_n)$ ，在此过程中，还生成了key向量$k_1,\cdots,k_n$ 和 value 向量$v_1,\cdots,v_n$，由于token $(x_1,\cdots,x_n)$ 都是已知的，提示阶段的计算可以使用矩阵乘法运算并行化。因此，这一阶段可以有效地利用gpu固有的并行性。</p>
<p><strong>The autoregressive generation phase: </strong> 依次生成剩余的新token，在第 t 个迭代，模型需要token $x_{n+t}$ 作为输入并使用key向量 $k_1,\cdots,k_{n+t}$ 和value 向量 $v_1,\cdots,v_{n+t}$ 计算概率 $P(x_{n+t+1}|x_1,\cdots,x_{n+t})$ ,注意，$1\to n + t-1$ 的key和value向量在之前的迭代已经缓存了，这个迭代值生成新的token的key 和 value 。当序列达到最大长度(由用户或者llm限制)或发出序列结束($<eos>$)token时，自回归阶段完成。由于数据依赖性，不同迭代的计算不能并行化，通常采用矩阵-向量乘法，效率较低。因此，这一阶段严重未充分利用GPU计算并成为内存限制，带来单个请求的大部分延迟。</eos></p>
<h5 id="2-3-Batching-Techniques-for-LLMs"><a href="#2-3-Batching-Techniques-for-LLMs" class="headerlink" title="2.3 Batching Techniques for LLMs"></a>2.3 Batching Techniques for LLMs</h5><p>通过批量处理多个请求，可以提高llm服务的计算利用率。由于请求共享相同的模型权重，因此移动权重的开销在批处理请求中平摊，并且当批处理大小足够大时，可能会被计算开销所抵消(相比变得很小)。但是，由于两个原因，将请求批处理到LLM服务是非常重要的。首先，请求可能在不同的时间到达。直接的批处理策略要么让较早的请求等待较晚的请求，要么将之后的请求延迟到较早的请求完成，从而导致严重的排队延迟。其次，请求可能具有不同的输入和输出长度(图11)。直接的批处理技术填充请求的输入和输出，以平衡它们的长度，但会浪费GPU计算和内存。</p>
<p>为了解决这个问题，人们提出了细粒度的批处理机制，如cellular batching[16]和iteration-level scheduling[60]。与在请求级别工作的传统方法不同，这些技术在迭代级别操作。在每次迭代之后，完成的请求将从批处理中删除，并添加新的请求。因此，可以在等待单个迭代后处理新请求，而不是等待整个批处理完成。</p>
<p>此外，使用特殊的GPU内核，这些技术消除了填充输入和输出的需要。通过减少排队延迟和填充带来的低效率，细粒度批处理机制显著提高了LLM服务的吞吐量。</p>
<h4 id="3-Memory-Callenge-in-LLM-Serving"><a href="#3-Memory-Callenge-in-LLM-Serving" class="headerlink" title="3. Memory Callenge in LLM Serving"></a>3. Memory Callenge in LLM Serving</h4><p>尽管细粒度批处理减少了计算浪费，并使请求能够以更灵活的方式进行批处理，但可以批处理的请求数量仍然受到GPU内存容量的限制，特别是分配给存储KV cache 的空间。换句话说，服务系统的吞吐量受内存限制。克服这种内存限制需要解决内存管理中的以下挑战:</p>
<p><strong>Large KV cache</strong> 。KV缓存大小随着请求数量的增加而快速增长。例如，对于13B参数的OPT模型[62]，单个令牌的KV缓存需要800 KB的空间，计算为2(key 和 value)× 5120(hidden state size)× 40(number of layers)× 2(bytes per FP16)。由于OPT可以生成多达2048个token的序列，因此存储一个请求的KV cache所需的内存可能高达1.6 GB。并发GPU的内存容量为几十GB。即使将所有可用内存分配给KV cache，也只能容纳几十个请求。此外，低效的内存管理会进一步减小批处理大小，如图2所示。此外，从目前的趋势来看，GPU的计算速度的增长速度超过了内存容量的增长速度。例如，从NVIDIA A100到H100, FLOPS提高了2倍以上，但GPU内存最大保持在80GB。因此，我们相信内存将成为越来越重要的瓶颈。</p>
<p><strong>Complex decoding algorithms.</strong> LLM服务提供了一系列解码算法供用户选择，每种算法对内存管理复杂性的影响各不相同。例如，当用户从单个输入提示请求多个随机样本(程序建议中的典型用例)时，提示部分的KV cache 可以共享，在我们的实验中(§6.3)，它占总KV缓存的12%，以最小化内存使用。另一方面，在自回归生成阶段，由于不同的样本结果及其对环境和位置的依赖，KV cache 应该保持不共享。<strong>KV cache 共享的程度取决于所采用的具体解码算法。</strong> 在beam search [49]等更复杂的算法中，不同的请求可以共享其KV cache 的更大部分(高达55%的内存节省，参见§6.3)，并且共享模式随着解码过程的推进而变化。</p>
<p><strong>Scheduling for unknown input &amp; output lengths.</strong> LLM服务的请求在其输入和输出长度方面表现出可变性。这就要求内存管理系统能够适应各种提示长度。此外，随着解码时请求的输出长度增加，其KV cache 所需的内存也会扩展，并且可能耗尽用于传入请求或正在生成的现有提示的内存。系统需要做出调度决策，例如从GPU内存中删除或交换某些请求的KV缓存。</p>
<h5 id="3-1-Memory-Management-in-Existing-Systems"><a href="#3-1-Memory-Management-in-Existing-Systems" class="headerlink" title="3.1 Memory Management in Existing Systems"></a>3.1 Memory Management in Existing Systems</h5><p>由于当前深度学习框架中的大多数运算符要求将张量存储在连续内存中，以前的LLM服务系统也将一个请求的KV缓存存储为跨不同位置的连续张量。由于LLM的输出长度不可预测，因此它们根据请求的最大可能序列长度静态地为请求分配一块内存，而不考虑请求的实际输入或最终输出长度。</p>
<p> <img src="/2025/01/28/llm_1/c86039995d1b47249a4346281689a445.png" alt="在这里插入图片描述"></p>
<p>图3显示了两个请求:请求A的最大可能序列长度为2048，请求B的最大可能序列长度为512。现有系统中的块预分配方案有三个主要的内存浪费来源:为未来token 保留的内存、由于过度供应潜在的最大序列长度而导致的内部碎片，以及来自内存分配器(如buddy分配器)的外部碎片。外部碎片永远不会用于生成的令牌，这在服务请求之前是已知的。内部碎片也未被使用，但这只有在请求完成采样后才会实现。它们都是纯粹的内存浪费。虽然保留的内存最终会被使用，但是在整个请求期间保留这个空间，特别是当保留的空间很大时，会占用本来可以用来处理其他请求的空间。我们在图2中可视化了我们的实验中内存浪费的平均百分比，揭示了以前系统中的实际有效内存可以低至20.4%。</p>
<p>虽然compaction[54]已经被提出作为一种潜在的碎片解决方案，但由于大量KV缓存，在性能敏感的LLM服务系统中执行压缩是不切实际的。即使使用了压缩，为每个请求预先分配的块空间也会阻止现有内存管理系统中特定于解码算法的内存共享。</p>
<h4 id="4-Method"><a href="#4-Method" class="headerlink" title="4. Method"></a>4. Method</h4><p> <img src="/2025/01/28/llm_1/062049b20dcc430d999bc89bf80af483.png" alt="在这里插入图片描述"></p>
<p>在这项工作中，我们开发了一种新的注意力算法PagedAttention，并构建了一个LLM服务引擎vLLM，以解决§3中概述的挑战。vLLM的架构如图4所示。vLLM采用集中式调度程序来协调分布式GPU工作线程的执行。KV cache 管理器以分页方式有效地管理KV cache，由PagedAttention启用。具体来说，KV cache 管理器通过集中调度程序发送的指令来管理GPU工作线程上的物理KV cache。</p>
<p>接下来，我们在§4.1中描述PagedAttention算法。我们分别在§4.2中展示KV cache 管理器的设计以及它如何促进§4.3中的PagedAttention。然后，我们展示了这种设计如何促进各种解码方法(§4.4)的有效内存管理，并处理可变长度的输入和输出序列(§4.5)。最后，我们展示了vLLM的系统设计如何在分布式设置中工作(第4.6节)。</p>
<h5 id="4-1-PageAttention"><a href="#4-1-PageAttention" class="headerlink" title="4.1 PageAttention"></a>4.1 PageAttention</h5><p>为了解决§3中的内存挑战，我们引入了PagedAttention，这是一种受操作系统中分页的经典思想启发的注意力算法[25]。与传统的注意力算法不同，PagedAttention允许在非连续的内存空间中存储连续的键和值。具体来说，PagedAttention将每个序列的KV cache 划分为KV block。每个块包含固定数量的token的key和value向量，我们将其记为KV block size。记key block为 $K_j=(k_{(j-1)B+1},\cdots,k_{jB})$ ，记value block为$V_j=(v_{(j-1)B+1},\cdots,v_{jB})$ ，注意力可以逐块计算为:</p>
<script type="math/tex; mode=display">
A_{ij}=\frac{exp(q_i^TK_j/\sqrt{d})}{\sum_{t=1}^{\lceil /B\rceil }exp(q_i^TK_t/\sqrt{d})}, o_i = \sum\limits_{j=1}^{\lceil i / B\rceil} V_j A_{ij}^T</script><p>其中$A_{ij}=(a_{i,(j-1)B+1},\cdots,a_{i,j}B)$表示第j个KV block中的行向量。在注意力计算过程中，PagedAttention内核分别识别和提取不同的KV块，我们在图5中展示了一个PagedAttention的例子:键和值向量分布在三个块上，并且这三个块在物理内存上不是连续的。每次，内核将查询tokend 的Query向量和block中的key向量$K_j$ (例如，block 0中的key向量”Four score and seven”)相乘，计算出注意力分数 $A_{ij}$， 然后将变量 $A_{ij}$ 和block中的value向量$V_j$相乘得到最终的注意力分数。 </p>
<p> <img src="/2025/01/28/llm_1/d64723720b434d668211104cb6a749cc.png" alt="在这里插入图片描述"></p>
<p>总之，PagedAttention算法允许将KV块存储在非连续的物理内存中，从而在vLLM中实现更灵活的分页内存管理。</p>
<h5 id="4-2-KV-Cache-Manager"><a href="#4-2-KV-Cache-Manager" class="headerlink" title="4.2 KV Cache Manager"></a>4.2 KV Cache Manager</h5><p>vLLM内存管理器背后的关键思想类似于操作系统中的虚拟内存[25]。操作系统将内存划分为固定大小的页面，并将用户程序的逻辑页面映射到物理页面。连续的逻辑页可以对应于非连续的物理内存页，允许用户程序访问内存，就好像它是连续的一样。此外，物理内存空间不需要提前完全预留，使操作系统可以根据需要动态分配物理页面。vLLM使用虚拟内存背后的思想来管理LLM服务中的KV缓存。通过PagedAttention，我们将KV缓存组织为固定大小的KV块，就像虚拟内存中的页面一样。</p>
<p>请求的KV cache 表示为一系列逻辑KV块，从左到右填充为生成的新token及其KV cache。最后一个KV区块的未填充位置保留给未来生成的token使用。在GPU节点上，块引擎分配一个连续的GPU DRAM块，并将其划分为物理KV块(这也在CPU RAM上完成，用于交换 ,§4.5)。KV块管理器还维护块表——每个请求的逻辑和物理KV块之间的映射。每个块表项记录一个逻辑块对应的物理块和填充位置的数量。分离逻辑和物理KV块允许vLLM动态增长KV缓存，而无需提前为所有位置保留它，这消除了现有系统中的大部分内存浪费，如图2所示。</p>
<h5 id="4-3-Decoding-with-PagedAttention-and-vLLM"><a href="#4-3-Decoding-with-PagedAttention-and-vLLM" class="headerlink" title="4.3 Decoding with PagedAttention and vLLM"></a>4.3 Decoding with PagedAttention and vLLM</h5><p> <img src="/2025/01/28/llm_1/973ddad656ae4831b5623c6cf729b0da.png" alt="在这里插入图片描述"></p>
<p>接下来，我们通过一个示例，如图6所示，来演示vLLM如何在单个输入序列的解码过程中执行PagedAttention并管理内存:</p>
<p>①就像在OS的虚拟内存中一样，vLLM不需要为最初可能生成的最大序列长度保留内存。相反，它只保留必要的KV块，以容纳在提示计算期间生成的KV缓存。在本例中，提示符有7个令牌，因此vLLM将前2个逻辑KV块(0和1)映射到2个物理KV块(分别为7和1)。在预填充步骤中，vLLM使用传统的自关注算法(例如[13])生成提示符和第一个输出令牌的KV缓存。然后，vLLM将前4个令牌的KV缓存存储在逻辑块0中，并将随后的3个令牌存储在逻辑块1中。剩余的槽保留给后续的自回归生成阶段。</p>
<p>②在第一个自回归解码步骤中，vLLM使用物理块7和1上的PagedAttention算法生成新的令牌。由于在最后一个逻辑块中仍然有一个槽可用，因此新生成的KV缓存存储在那里，并且块表的#filled记录被更新。</p>
<p>③在第二步解码时，由于最后一个逻辑块已满，vLLM将新生成的KV缓存存储在新的逻辑块中;vLLM为它分配一个新的物理块(物理块3)，并将这个映射存储在块表中。</p>
<p>全局而言，对于每次解码迭代，vLLM首先选择一组候选序列进行批处理(参见§4.5)，并为新需要的逻辑块分配物理块。然后,vLLM连接所有当前迭代的输入token作为一个序列，并将其输入LLM。在LLM的计算过程中，vLLM使用PagedAttention内核访问之前以逻辑KV块形式存储的KV缓存，并将新生成的KV缓存保存到物理KV块中。在KV块中存储多个令牌(块大小&gt; 1)使PagedAttention内核能够跨多个位置并行处理KV缓存，从而增加硬件利用率并减少延迟。然而，更大的块大小也会增加内存碎片。我们在§7.2中研究了块大小的影响。</p>
<p>同样，当生成更多令牌及其KV缓存时，vLLM会动态地将新的物理块分配给逻辑块。由于所有的块都是从左到右填充的，并且只有在之前的所有块都已满时才分配新的物理块，因此vLLM将请求的所有内存浪费限制在一个块内，因此它可以有效地利用所有内存，如图2所示。这允许将更多请求放入内存中进行批处理，从而提高吞吐量。一旦一个请求完成了它的生成，它的KV块可以被释放来存储其他请求的KV缓存。在图7中，我们展示了一个vLLM管理两个序列的内存的示例。两个序列的逻辑块映射到GPU worker中块引擎保留的空间内的不同物理块。两个序列的相邻逻辑块不需要在物理GPU内存中连续，两个序列可以有效地利用物理块的空间。</p>
<p> <img src="/2025/01/28/llm_1/e507ff7cc78044088cd9ae2ce3fff6a6.png" alt="在这里插入图片描述"></p>
<h5 id="4-4-Application-to-Other-Decoding-Scenarios"><a href="#4-4-Application-to-Other-Decoding-Scenarios" class="headerlink" title="4.4 Application to Other Decoding Scenarios"></a>4.4 Application to Other Decoding Scenarios</h5><p>§4.3展示了PagedAttention和vLLM如何处理基本的解码算法，例如贪婪解码和采样，将一个用户提示作为输入并生成单个输出序列。在许多成功的LLM应用程序中[18,34]，LLM服务必须提供更复杂的解码场景，表现出复杂的访问模式和更多的内存共享机会。在本节中，我们将展示vLLM对它们的一般适用性。</p>
<p><strong>Parallel sampling.</strong> 并行采样。在基于LLM的程序助手中[6,18]，LLM为单个输入提示生成多个采样输出;用户可以从各种候选输出中选择自己喜欢的输出。到目前为止，我们已经隐含地假设了一个请求生成单个序列。在本文的其余部分中，我们假设一个请求生成多个序列的更一般的情况。在并行采样中，一个请求包含多个共享相同输入提示的样本，从而允许共享提示的KV缓存。通过它的PagedAttention和分页内存管理，vLLM可以很容易地实现这种共享并节省内存。在本文的其余部分中，我们假设一个请求生成多个序列的更一般的情况。在并行采样中，一个请求包含多个共享相同输入提示的样本，从而允许共享提示的KV缓存。通过它的PagedAttention和分页内存管理，vLLM可以很容易地实现这种共享并节省内存。</p>
<p> <img src="/2025/01/28/llm_1/136cfecb0ff74f36abe81a6ce50bd312.png" alt="在这里插入图片描述"></p>
<p>图8示出用于两个输出的并行解码的示例。由于两个输出共享相同的提示符，我们在提示阶段只为提示符状态的一个副本保留空间;两个序列的提示符的逻辑块映射到相同的物理块:两个序列的逻辑块0和1分别映射到物理块7和1。由于单个物理块可以映射到多个逻辑块，因此我们为每个物理块引入一个引用计数。在这种情况下，物理块7和1的引用计数都是2。在生成阶段，两个输出采样不同的输出令牌，需要单独存储KV缓存。对于需要通过多个序列修改的物理块，vLLM在块粒度上实现了一种写时复制机制，类似于操作系统虚拟内存中的写时复制技术(例如，当fork一个进程时)。具体来说，在图8中，当示例A1需要写入它的最后一个逻辑块(逻辑块1)时，vLLM识别到对应的物理块(物理块1)的引用计数大于1;它分配一个新的物理块(物理块3)，指示块引擎从物理块1复制信息，并将引用计数减少到1。接下来，当示例A2写入物理块1时，引用计数已经减少到1;因此A2直接将其新生成的KV缓存写入物理块1。</p>
<p>总之，<strong>vLLM支持跨多个输出样本共享用于存储提示的KV缓存的大部分空间，但最后的逻辑块除外，该逻辑块由写时复制机制管理。</strong>通过跨多个示例共享物理块，可以大大减少内存使用，特别是对于长输入提示。</p>
<p><strong>Beam search.</strong> 在机器翻译等LLM任务中[59]，用户期望LLM输出的top-𝑘最合适的翻译。Beam search 被广泛用于解码LLM最可能的输出序列，因为它降低了完全遍历样本空间的计算复杂度。该算法依赖于波束宽度参数𝑘，该参数决定了每一步保留的最佳候选数。在解码过程中，波束搜索通过考虑所有可能的标记来扩展波束中的每个候选序列，使用LLM计算它们各自的概率，并在候选序列(长度为$k\cdot|V|$)中保留最可能的𝑘个序列，其中 $|V|$ 是词汇表大小。</p>
<p> <img src="/2025/01/28/llm_1/5a259c8fbe4647e1aea8be7242a5d386.png" alt="在这里插入图片描述"></p>
<p>与并行解码不同，波束搜索工具不仅共享初始提示块，还共享不同候选块，并且共享模式随着解码过程的推进而动态变化，类似于复合分叉在操作系统中创建的进程树。图9显示了对于𝑘= 4的波束搜索示例，vLLM如何管理KV块。在用虚线表示的迭代之前，每个候选序列已经使用了4个完整的逻辑块。所有candidate共享第一个块0(即提示符)。candidate 3从第二部分开始离题。candidate0-2共用前3个block，并在第四个block分开。在随后的迭代中，前4个可能的候选项都来自候选项1和2。由于原来的候选0和3不再是最优候选，它们的逻辑块被释放，相应的物理块的引用计数被减少。vLLM释放所有引用计数达到0的物理块(block 2,4,5,8)，然后，vLLM分配新的物理块(block 9-12)来存储来自新候选对象的新KV缓存。现在，所有candidate共享0、1、3块;candidate0和1共享区块6，candidate2和3进一步共享block7。</p>
<p>以前的LLM服务系统需要在candidate上频繁地复制KV缓存。例如，在图9所示的情况下，在虚线之后，候选3将需要复制candidate2的KV缓存的大部分以继续生成。vLLM的物理块共享大大减少了这种频繁的内存复制开销。在vLLM中，不同candidate的大部分块可以共享。只有当新生成的令牌位于旧的共享块中时，才应用写时复制机制，就像并行解码一样。这只涉及复制一个数据块。</p>
<p> <img src="/2025/01/28/llm_1/cebed53b3256400aa2b1871479e9a69e.png" alt="在这里插入图片描述"></p>
<p><strong>Shared prefix.</strong> 通常，LLM用户提供任务的(长)描述，包括指令和示例输入输出，也称为系统提示符[36]。描述与实际任务输入连接起来，形成请求提示。LLM生成的输出基于完整的提示符。图10显示了一个示例。此外，可以通过提示工程进一步调整共享前缀，以提高下游任务的准确性。</p>
<p>对于这种类型的应用程序，许多用户提示共享一个前缀，因此LLM服务提供商可以提前存储前缀的KV缓存，以减少在前缀上花费的冗余计算。在vLLM中，可以通过LLM服务提供者为一组预定义的共享前缀保留一组物理块来方便地实现这一点，正如操作系统如何跨进程处理共享库一样。带有共享前缀的用户输入提示符可以简单地将其逻辑块映射到缓存的物理块(最后一个块标记为写时复制)。提示阶段的计算只需要在用户的任务输入上执行。</p>
<p><strong>Mixed decoding methods.</strong> 前面讨论的解码方法表现出不同的内存共享和访问模式。尽管如此，vLLM促进了具有不同解码偏好的请求的同时处理，这是现有系统无法有效做到的。这是因为vLLM通过将逻辑块转换为物理块的公共映射层隐藏了不同序列之间的复杂内存共享。LLM及其执行内核只看到每个序列的物理块id列表，不需要处理跨序列的共享模式。与现有系统相比，这种方法扩大了具有不同采样要求的请求的批处理机会，最终提高了系统的总体吞吐量。</p>
<h5 id="4-5-Scheduling-and-Preemption"><a href="#4-5-Scheduling-and-Preemption" class="headerlink" title="4.5 Scheduling and Preemption"></a>4.5 Scheduling and Preemption</h5><p>当请求流量超过系统容量时，vLLM必须优先考虑请求一个子集。在vLLM中，我们对所有请求采用先到先服务(FCFS)调度策略，确保公平性并防止饥饿。当vLLM需要抢占请求时，它确保首先服务最早到达的请求，并首先抢占最新的请求。</p>
<p>LLM服务面临着一个独特的挑战:LLM的输入提示的长度可能会有很大的不同，并且结果的输出长度是未知的，这取决于输入提示和模型。随着请求数量及其输出的增长，vLLM可能会耗尽GPU的物理块来存储新生成的KV缓存。在这种情况下，vLLM需要回答两个经典问题:(1)应该驱逐哪些街区? (2)如果再次需要，如何恢复被驱逐的块?通常，驱逐策略使用启发式方法来预测哪个块将在未来被访问得最远，并驱逐该块。在我们的例子中，我们知道序列的所有块都是一起被访问的，所以我们实现了一个全有或全无的驱逐策略，即，要么驱逐序列的所有块，要么不驱逐。此外，一个请求中的多个序列(例如，一个波束搜索请求中的波束候选序列)作为一个序列组进行组调度。一个序列组中的序列总是被抢占或重新调度在一起，因为这些序列之间可能存在内存共享。为了回答第二个问题，即如何恢复被驱逐的块，我们考虑两种技术:</p>
<p><strong>Swapping.</strong> 这是大多数虚拟内存实现使用的经典技术，它将被驱逐的页面复制到磁盘上的交换空间。在本例中，我们将被驱逐的块复制到CPU内存中。如图4所示，除了GPU块分配器之外，vLLM还包含一个CPU块分配器，用于管理交换到CPU RAM的物理块。当vLLM为新令牌耗尽空闲物理块时，它会选择一组序列来驱逐并将它们的KV缓存传输到CPU。一旦它抢占了一个序列并驱逐了它的块，vLLM就会停止接受新的请求，直到所有被抢占的序列都被完成。一旦请求完成，它的块就从内存中释放出来，并将被抢占序列的块带回来继续处理该序列。请注意，在这种设计中，交换到CPU RAM的块数量永远不会超过GPU RAM中的总物理块数量，因此CPU RAM上的交换空间受到分配给KV缓存的GPU内存的限制。</p>
<p><strong>Recomputation.</strong> 在这种情况下，当被抢占的序列被重新调度时，我们只需重新计算KV缓存。请注意，重新计算延迟可以显著低于原始延迟，因为在解码时生成的token可以与原始用户提示连接为一个新的token——它们在所有位置的KV缓存可以在一个提示阶段迭代中生成。</p>
<p>交换和重计算的性能取决于CPU RAM和GPU内存之间的带宽以及GPU的计算能力。我们将在§7.3中测试交换和重计算的速度。</p>
<h5 id="4-6-Distributed-Execution"><a href="#4-6-Distributed-Execution" class="headerlink" title="4.6 Distributed Execution"></a>4.6 Distributed Execution</h5><p>许多llm的参数大小超过了单个GPU的容量[5,9]。因此，有必要将它们划分到分布式gpu上，并以模型并行的方式执行[28,63]。这需要能够处理分布式内存的内存管理器。vLLM通过支持 transformer 上广泛使用的Megatron-LM风格张量模型并行策略，在分布式设置中是有效的。</p>
<p>该策略遵循SPMD(单程序多数据)调度，其中线性层是分区的，以执行逐块矩阵乘法，gpu通过allreduce操作不断同步中间结果。具体来说，注意力 算子在注意头维度上被分割，每个SPMD过程在多头注意中负责注意头的一个子集。我们观察到，即使模型并行执行，每个模型分片仍然处理相同的一组输入令牌，因此需要KV缓存用于相同的位置。因此，vLLM在集中式调度器中具有单个KV缓存管理器，如图4所示。不同的GPU工作者共享管理器，以及从逻辑块到物理块的映射。这种公共映射允许GPU worker使用调度程序为每个输入请求提供的物理块来执行模型。虽然每个GPU worker都有相同的物理块id，但一个工作线程只为其相应的注意头存储一部分KV缓存。</p>
<p>在每个步骤中，调度器首先为批处理中的每个请求准备带有输入token id的消息，并为每个请求准备块表。接下来，调度器将这个控制消息广播给GPU worker。然后，GPU worker 开始使用输入token id执行模型。在注意层，GPU工作人员根据控制消息中的块表读取KV缓存。在执行过程中，GPU worker 中间结果与all-reduce通信原语同步，而不需要调度程序的协调。最后，GPU工作器将这次迭代的采样token发送回调度器。总之，GPU工作人员不需要同步内存管理，因为他们只需要在每次解码迭代开始时接收所有内存管理信息以及step输入。</p>
<h4 id="5-Implementation"><a href="#5-Implementation" class="headerlink" title="5. Implementation"></a>5. Implementation</h4><p>vLLM是一个端到端服务系统，采用FastAPI[15]前端和基于gpu的推理引擎。前端扩展了OpenAI API[34]接口，允许用户为每个请求定制采样参数，如最大序列长度和beam width 𝑘。vLLM引擎是用8.5k行Python和2K行c++ /CUDA代码编写的。我们在Python中开发控制相关组件，包括调度器和块管理器，同时为关键操作(如PagedAttention)开发自定义CUDA内核。对于模型执行器，我们使用PyTorch和Transformer实现流行的llm，如GPT[5]、OPT[62]和LLaMA [52]我们使用NCCL[32]在分布式GPU worker之间进行张量通信。</p>
<h5 id="5-1-Kernel-level-Optimization"><a href="#5-1-Kernel-level-Optimization" class="headerlink" title="5.1 Kernel-level Optimization"></a>5.1 Kernel-level Optimization</h5><p>由于PagedAttention引入了现有系统无法有效支持的内存访问模式，我们开发了几个GPU内核来优化它。</p>
<p>(1)Fused reshape and block write. 在每个Transformer层中，新的KV缓存被分割成块，重新塑造为块读取优化的内存布局，然后保存在块表指定的位置。为了最小化内核启动开销，我们将它们融合到一个内核中。</p>
<p>(2)Fusing block read and attention.我们采用FasterTransformer[31]中的注意力内核，根据块表读取KV缓存，并动态执行注意力操作。为了确保合并内存访问，我们分配了一个GPU warp来读取每个块。此外，我们还增加了对请求批处理中可变序列长度的支持。</p>
<p>(3)Fused block copy.由写时拷贝机制发出的块拷贝操作可以在不连续的块上操作。如果我们使用cudamempyasync API，这可能导致大量的小数据移动调用。为了减少开销，我们实现了一个内核，它将不同块的复制操作批处理到单个内核启动中。</p>
<h5 id="5-2-Supporting-Various-Decoding-Algrithms"><a href="#5-2-Supporting-Various-Decoding-Algrithms" class="headerlink" title="5.2 Supporting Various Decoding Algrithms"></a>5.2 Supporting Various Decoding Algrithms</h5><p>vLLM使用三个关键方法实现各种解码算法:fork、append和free。fork方法从一个现有序列创建一个新序列。append方法向序列追加一个新标记。最后，free方法删除序列。例如，在并行采样中，vLLM使用fork方法从单个输入序列创建多个输出序列。然后在每次迭代中使用append向这些序列添加新的标记，并使用free删除满足停止条件的序列。vLLM在波束搜索和前缀共享中也采用了相同的策略。我们相信结合这些方法也可以支持未来的解码算法。</p>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
        <category>大语言模型</category>
      </categories>
      <tags>
        <tag>paper notes</tag>
        <tag>llvm</tag>
      </tags>
  </entry>
  <entry>
    <title>使用 torch.profiler记录模型训练轨迹</title>
    <url>/2025/02/03/torchProfile/</url>
    <content><![CDATA[<p>使用 torch.profiler记录模型训练轨迹，并使用Tensorboard进行可视化分析，首先导入需要的库，准备模型和数据集，设置记录器，生成json格式的文件，最后通过Tensorboard可视化。</p>
<span id="more"></span>
<h4 id="Steps"><a href="#Steps" class="headerlink" title="Steps"></a>Steps</h4><ol>
<li>Prepare the data and model</li>
<li>Use profiler to record execution events</li>
<li>Run the profiler</li>
<li>Use TensorBoard to view results and analyze model performance</li>
<li>Improve performance with the help of profiler</li>
<li>Analyze performance with other advanced features</li>
<li>Additional Practices: Profiling PyTorch on AMD GPUs</li>
</ol>
<h4 id="1-Prepare-the-data-and-model"><a href="#1-Prepare-the-data-and-model" class="headerlink" title="1. Prepare the data and model"></a>1. Prepare the data and model</h4><p><strong>导入需要的库:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn</span><br><span class="line"><span class="keyword">import</span> torch.optim</span><br><span class="line"><span class="keyword">import</span> torch.profiler</span><br><span class="line"><span class="keyword">import</span> torch.utils.data</span><br><span class="line"><span class="keyword">import</span> torchvision.datasets</span><br><span class="line"><span class="keyword">import</span> torchvision.models</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> T</span><br></pre></td></tr></table></figure>
<p><strong>准备数据集</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">transform = T.Compose(</span><br><span class="line">    [T.Resize(<span class="number">224</span>),</span><br><span class="line">     T.ToTensor(),</span><br><span class="line">     T.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])</span><br><span class="line">train_set = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">train_loader = torch.utils.data.DataLoader(train_set, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p><strong>模型定义</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span>)</span><br><span class="line">model = torchvision.models.resnet18(weights=<span class="string">&#x27;IMAGENET1K_V1&#x27;</span>).cuda(device)</span><br><span class="line">criterion = torch.nn.CrossEntropyLoss().cuda(device)</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">model.train()</span><br></pre></td></tr></table></figure>
<p><strong>模型训练</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">data</span>):</span><br><span class="line">    inputs, labels = data[<span class="number">0</span>].to(device=device), data[<span class="number">1</span>].to(device=device)</span><br><span class="line">    outputs = model(inputs)</span><br><span class="line">    loss = criterion(outputs, labels)</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>
<h4 id="2-使用Profiler记录轨迹"><a href="#2-使用Profiler记录轨迹" class="headerlink" title="2. 使用Profiler记录轨迹"></a>2. 使用Profiler记录轨迹</h4><p>some useful parameters are as follow:</p>
<p><code>schedule</code>: 参数例如<code>wait=1,warmup=1,active=3,repeat=1</code>(profiler 会跳过第一个step/iteration，在第二个iter热身，记录三个iter。). In total, the cycle repeats once. Each cycle is called a “span” in TensorBoard plugin.</p>
<p>在<code>wait</code>阶段，profiler 不生效，在<code>warmup</code> 阶段，proliler 开始工作但不记录结果，是为了减少开销，proliling 的开始开销很大，会影响结果。</p>
<p><code>on_trace_ready</code> :  在每个cylce结束时调用，例如使用<code>torch.profiler.tensorboard_trace_handler</code>来时生成Tensorboard使用的结果文件，在Profiling后，结果文件存储在<code>./log/resnet18</code>中。</p>
<p><code>record_shapes</code>：是否记录输入张亮的形状</p>
<p><code>profile_memory</code>: 追踪张量空间申请和释放。</p>
<p><code>with_stack</code>：记录算子的代码信息，如果在vscode中集成TensorBoard, 单击可以跳转到特定行。</p>
<p><a href="https://code.visualstudio.com/docs/datascience/pytorch-support#_tensorboard-integration">https://code.visualstudio.com/docs/datascience/pytorch-support#_tensorboard-integration</a></p>
<p><strong>以上下文管理器启动/停止：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.profiler.profile(</span><br><span class="line">        schedule=torch.profiler.schedule(wait=<span class="number">1</span>, warmup=<span class="number">1</span>, active=<span class="number">3</span>, repeat=<span class="number">1</span>),</span><br><span class="line">        on_trace_ready=torch.profiler.tensorboard_trace_handler(<span class="string">&#x27;./log/resnet18&#x27;</span>),</span><br><span class="line">        record_shapes=<span class="literal">True</span>,</span><br><span class="line">        profile_memory=<span class="literal">True</span>,</span><br><span class="line">        with_stack=<span class="literal">True</span></span><br><span class="line">) <span class="keyword">as</span> prof:</span><br><span class="line">    <span class="keyword">for</span> step, batch_data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        prof.step()  <span class="comment"># Need to call this at each step to notify profiler of steps&#x27; boundary.</span></span><br><span class="line">        <span class="keyword">if</span> step &gt;= <span class="number">1</span> + <span class="number">1</span> + <span class="number">3</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        train(batch_data)</span><br></pre></td></tr></table></figure>
<p><strong>也可以以非上下文管理器启动/停止：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">prof = torch.profiler.profile(</span><br><span class="line">        schedule=torch.profiler.schedule(wait=<span class="number">1</span>, warmup=<span class="number">1</span>, active=<span class="number">3</span>, repeat=<span class="number">1</span>),</span><br><span class="line">        on_trace_ready=torch.profiler.tensorboard_trace_handler(<span class="string">&#x27;./log/resnet18&#x27;</span>),</span><br><span class="line">        record_shapes=<span class="literal">True</span>,</span><br><span class="line">        with_stack=<span class="literal">True</span>)</span><br><span class="line">prof.start()</span><br><span class="line"><span class="keyword">for</span> step, batch_data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">    prof.step()</span><br><span class="line">    <span class="keyword">if</span> step &gt;= <span class="number">1</span> + <span class="number">1</span> + <span class="number">3</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    train(batch_data)</span><br><span class="line">prof.stop()</span><br></pre></td></tr></table></figure>
<h4 id="3-运行profiler"><a href="#3-运行profiler" class="headerlink" title="3. 运行profiler"></a>3. 运行profiler</h4><h4 id="4-使用Tensorboard展示结果"><a href="#4-使用Tensorboard展示结果" class="headerlink" title="4. 使用Tensorboard展示结果"></a>4. 使用Tensorboard展示结果</h4><p>安装Pytorch Profiler TensorBoard Plugin</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pip install torch_tb_profiler</span><br></pre></td></tr></table></figure>
<p>登录TensorBoard</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tensorboard --logdir=./log</span><br></pre></td></tr></table></figure>
<p>打开TensorBoard</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">http://localhost:6006/#pytorch_profiler</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>配置</category>
      </categories>
      <tags>
        <tag>configure</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文阅读】Characterization of Large Language Model Development in the Datacenter</title>
    <url>/2025/02/03/scheduler_1/</url>
    <content><![CDATA[<p>大语言模型（LLMs）在许多任务中表现出色。然而，要高效利用大规模集群资源开发LLM并非易事，常常伴随着频繁的硬件故障、复杂的并行化策略和资源利用不平衡等诸多挑战。为此，我们针对Acme GPU数据中心在为期六个月的LLM开发工作负载中所累积的跟踪数据，进行了一次深入的特征分析研究。我们特别探讨了LLM与以往深度学习（DL）工作负载之间的差异，研究了资源利用模式，分析了各种任务失败的影响，总结了所遇到的难题，并揭示了优化LLM系统的潜在机会。</p>
<span id="more"></span>
<h3 id="【论文阅读】Characterization-of-Large-Language-Model-Development-in-the-Datacenter"><a href="#【论文阅读】Characterization-of-Large-Language-Model-Development-in-the-Datacenter" class="headerlink" title="【论文阅读】Characterization of Large Language Model Development in the Datacenter"></a>【论文阅读】Characterization of Large Language Model Development in the Datacenter</h3><ul>
<li>出处: NSDI-2024                                             <a href="https://www.usenix.org/system/files/nsdi24-hu.pdf">数据中心中大型语言模型开发的表征</a></li>
<li><a href="https://github.com/InternLM/AcmeTrace">InternLM/AcmeTrace (github.com)</a></li>
</ul>
<h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p>大语言模型（LLMs）在许多任务中表现出色。然而，要高效利用大规模集群资源开发LLM并非易事，常常伴随着频繁的硬件故障、复杂的并行化策略和资源利用不平衡等诸多挑战。为此，我们针对Acme GPU数据中心在为期六个月的LLM开发工作负载中所累积的跟踪数据，进行了一次深入的特征分析研究。</p>
<p>我们特别探讨了LLM与以往深度学习（DL）工作负载之间的差异，研究了资源利用模式，分析了各种任务失败的影响，总结了所遇到的难题，并揭示了优化LLM系统的潜在机会。此外，我们介绍了改进措施：（1）预训练容错，通过LLM参与的故障诊断和自动恢复，增强了容错能力。(2)针对评估任务负载的解耦调度，通过任务分解和调度优化，实现即时的性能反馈。</p>
<h4 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h4><p>近年来，大语言模型（LLMs）引起了学术界和工业界的广泛关注，例如ChatGPT和GitHub Copilot。然而，由于这些模型规模庞大且对数据需求量巨大，训练这些模型需要庞大的计算基础设施，通常需要成千上万的加速器。因此，科技公司和云服务提供商通常会构建大规模的GPU集群来促进LLM的开发，特别是在ChatGPT流行之后。然而，在如此高成本的基础设施上进行高效的LLM开发并非易事。<strong>开发人员常常面临诸多问题和挑战，包括频繁的硬件故障、复杂的并行化策略、不稳定的训练进度、长时间的排队延迟等。</strong></p>
<p>LLM的开发在各个方面都与GPU集群的支持密不可分。<strong>对集群工作负载的全面分析对于理解挑战和发现为LLM量身定制系统设计的机会至关重要</strong>。然而，许多在LLM兴起之前提出的深度学习工作负载分析工作的结论和启示并不适用于LLM开发。这主要是由于LLM具有不同的特性和需求： </p>
<p>1.<strong>范式转变</strong>：深度学习（DL）工作负载通常遵循特定任务的范式，在特定领域的数据上训练模型以解决特定任务（例如翻译）。相比之下，大语言模型（LLMs）采用一种新兴的范式，通过自监督训练在广泛的数据上生成基础模型，然后适应各种下游任务。<strong>这一转变标志着模型开发流程（如预训练、对齐）和工作负载特征与之前的深度学习工作负载存在显著差异。</strong></p>
<p>2.<strong>定制软件栈</strong> ：为了适应LLMs庞大的模型规模，一系列系统实现了先进的技术来优化LLMs的执行。例如，Deepspeed、Megatron和Alpa通过混合并行或状态分片优化器加速训练。而在模型服务方面，Orca和vLLM通过迭代调度或内存管理来提高吞吐量。</p>
<p>3.<strong>统一架构</strong> ：之前的深度学习工作负载通常采用各种模型架构（例如CNN、RNN）来解决不同的任务。相反，LLMs普遍采用Transformer架构，如BERT、GPT-3、LLaMA和PaLM。这种架构上的一致性表明LLM开发流程高度统一，不同数据中心之间具有很高的相似性。</p>
<p><strong>为了解决这一差距，我们对上海人工智能实验室的数据中心Acme的运营经验进行了深入研究。</strong> 该中心拥有两个专门用于LLM开发的集群，Seren和Kalos，<strong>总共配备了4,704个A100 GPU</strong>。我们的分析基于从2023年3月到8月期间收集的日志数据，<strong>包括调度器日志、基础设施监控数据、故障日志和细粒度的性能分析数据</strong> 。我们的主要发现和识别的挑战总结如下：</p>
<p><strong>1.更短的作业时长和不公平的排队延迟</strong> ：与传统的认知相反，<strong>我们数据中心的工作负载平均作业时长比以往的DL工作负载缩短了2.7到12.8倍</strong> 。这主要是由于大量短期任务（如评估任务）的存在。在作业排队延迟方面，我们的发现也与以往的DL工作负载不同，往的DL工作负载中规模较大的作业通常会有更长的等待时间。我们观察到，尽管评估任务是短期且小规模的，但它们却有最长的排队延迟。这一差异源于<strong>为了减少预训练作业的排队延迟，系统保留了大部分资源用于预训练作业，而评估作业则被安排在优先级较低的位置，使用有限的备用资源。</strong></p>
<p><strong>2.资源使用不均衡</strong>：这种不平衡体现在两个方面。首先，在工作负载分布方面，<strong>预训练作业仅占总作业量的3.2%，但消耗了Kalos集群94.0%的计算资源</strong>（即GPU时间）。相反，<strong>评估作业虽然占所有作业的92.9%，却仅使用了0.8%的资源。</strong>其次，从基础设施利用率来看，我们发现包括CPU、主机内存和网络在内的相关资源经常处于未充分利用状态。相比之下，作为主要资源的GPU显示出很高的利用率。Kalos集群（LLMs负载）中<strong>GPU内存和GPU利用率的中位数分别高达75%（60GB）和99%</strong>，而在PAI集群（传统DL负载）中，这两个数值分别仅为10%和4%。<strong>这些观察结果证实了LLMs在计算和内存方面的高需求，也表明基于GPU共享的技术可能不适用于LLMs。</strong></p>
<p><strong>3.评估任务负载GPU比较空闲</strong>：我们的<strong>评估任务负载分析显示，各个阶段的GPU资源利用率严重不足</strong>。例如，HumanEval评估作业有29.5%的时间用于模型加载和数据预处理，另有19.0%的时间用于合成程序正确性评估。因此，<strong>只有一半的时间用于GPU推理</strong>，导致评估试验的排队延迟时间较长。</p>
<p><strong>4.频繁的作业失败</strong>：我们发现，<strong>LLM工作负载的各种错误主要发生在作业开始阶段，导致快速终止</strong>。然而，基础设施故障在长期预训练作业中很常见，严重影响训练效率。因此，及时诊断和恢复这些故障对于提高训练效率至关重要。</p>
<p>根据我们的特征研究，我们发现了LLM开发过程中遇到的几个挑战，如<strong>训练进度不稳定、远程存储瓶颈和模型性能反馈延迟</strong>。为了解决这些问题，我们结合运营经验，构建了两个集成到LLM框架中的系统，以提高开发的稳健性和效率。</p>
<p><strong>首先，为了缓解频繁的故障问题，我们建立了一个预训练容错系统</strong>。它包含三个关键设计：(1) 通过异步检查点实现频繁的模型保存，(2) 结合启发式规则和LLM识别各种故障的根本原因，(3) 使用综合检测工具包定位故障节点，并从适当保存的检查点自动重新开始训练。该系统使检查点速度提高了3.6至58.7倍，并显著减少了人工干预。</p>
<p><strong>其次，我们开发了一个用于评估任务的解耦调度系统</strong>，为开发人员提供及时的模型质量反馈。它不仅通过解耦模型检索解决了远程模型加载争用问题，还通过解耦指标计算过程最小化了GPU空闲时间。该系统还利用数据集的先验知识和灵活性在所有GPU上平衡工作负载。实验表明，它可以将评估时间缩短最多达1.8倍。</p>
<h4 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h4><h4 id="2-1-LLM-Development-Pipeline"><a href="#2-1-LLM-Development-Pipeline" class="headerlink" title="2.1 LLM Development Pipeline"></a>2.1 LLM Development Pipeline</h4><p>与传统的深度学习模型不同，大语言模型（LLM）采用一种新兴的自监督学习范式，在广泛的数据上进行训练，并进一步后训练适应各种下游任务。LLM的开发由于其庞大的<strong>模型规模（包含数十亿参数）和大量的训练数据</strong>，通常需要庞大的计算基础设施。图1展示了LLM开发的完整流程，包括从零开始到服务上线的五个不同阶段（蓝色方块），蓝色箭头表示流程顺序。灰色的循环箭头表示预训练阶段可以进行定期的对齐和评估，以评估中间模型并及时调整配置。各个阶段具体如下：</p>
<p> <img src="/2025/02/03/scheduler_1/4f4dd76580e7490ab18120e1cba78cdb.png" alt="请添加图片描述"></p>
<p><strong>数据准备</strong>：初始阶段包括数据的收集和预处理，可分为两个部分：（1）预训练数据，由从公共或私人来源获取的大量未标注语料组成，通过净化和去重等过程进行筛选；（2）对齐数据，由用于将模型对齐到特定任务的小规模高质量标注语料组成，这些数据通常通过昂贵的人力注释或标注获取。此外，所有数据必须进行分词处理，以确保与模型输入的兼容性。</p>
<p><strong>预训练</strong>：这一阶段涉及在大规模精选数据上进行自监督训练，占据了整个开发流程中大部分资源。要在大规模上高效训练LLMs，需要采用多种系统创新技术，如状态分片优化器、数据并行、流水线并行和张量并行等精细的模型放置方法。</p>
<p><strong>对齐</strong>：这一阶段旨在使LLMs能够适应用户意图，以应对各种下游任务。常用的两种对齐范式是：（1）提示工程，通过指定提示（即输入）而不修改模型参数。例如，在文本摘要中，在输入文章后添加提示“TL;DR”可以提高模型性能；（2）微调，通过在特定任务的数据集上更新模型参数来提高在特定领域的性能。此外，从人类反馈中进行的强化学习（RLHF）进一步增强了对齐效果，并且像LoRA这样的参数高效技术被提出以降低微调的成本。</p>
<p><strong>评估</strong>：鉴于LLMs的广泛应用场景，仅依靠训练损失等单一指标来评估模型质量可能不准确。需要考虑的因素有很多，如准确性、公平性和毒性。因此，有必要采用多样的标准并在多个任务上衡量性能。此外，在预训练阶段进行定期评估，以提供关于模型质量的及时反馈也是至关重要的</p>
<p><strong>部署</strong>：为了满足LLM应用的严格成本和延迟要求，已经开发了多种先进技术来实现高效的模型服务，包括量化、蒸馏、CUDA内核优化、模型并行和内存管理等。</p>
<h5 id="2-2-Acme-Overview"><a href="#2-2-Acme-Overview" class="headerlink" title="2.2 Acme Overview"></a>2.2 Acme Overview</h5><p>Acme 是我们的私人 GPU 数据中心，支持研究人员和工程师在多个领域开发深度学习模型。<strong>在本文中，我们专注于分析两个专门用于开发大语言模型（LLM）的集群：Seren 和 Kalos。</strong>我们收集并分析了这两个集群中的所有作业。需要注意的是，Acme 中还有其他集群用于不同领域，如自动驾驶和科学研究中的人工智能，但这些集群因与本文无关而被排除在外。<br><img src="/2025/02/03/scheduler_1/aca92d57dd464d749030dfa64f2cca73.png" alt="在这里插入图片描述"></p>
<p><strong>集群架构：</strong> 表1总结了这两个同质 LLM 集群的配置。Seren 和 Kalos 分别有 2,288 和 2,416 个 GPU。每个节点配备 8× NVIDIA A100-SXM 80GB GPU 和 2× Intel Xeon Platinum 8358P CPU（共128个线程）。GPU 通过 NVLink 和 NVSwitch 互连，节点间通信通过 NVIDIA Mellanox 200Gbps HDR InfiniBand 实现。相比 Seren，Kalos 是一个相对较新的集群，网络配置有所改进。Kalos 每个节点具有更大的主机内存（2TB），配备了四个专用于应用通信的 InfiniBand HCA 以及一个专用于存储的额外 HCA。</p>
<p>此外，分布式存储系统对工作负载性能也至关重要。Acme 采用了全 NVMe 共享并行文件系统以实现快速数据访问和存储。随着时间的推移，我们的资源调度系统已经演变为支持多种集群环境。具体来说，Seren 和 Kalos 上的调度器分别基于 Slurm 和 Kubernetes 构建。为了为大规模预训练作业提供资源保证，我们的调度器实现了资源隔离和配额预留。此外，它还结合了尽力而为的作业机制以提高利用率。</p>
<p><strong>LLM 工作负载：</strong> 我们开发了一系列 LLM，参数规模从 70 亿到超过 1230 亿不等。这些模型都采用基于 Transformer 的解码器架构，类似于 GPT 和 LLaMA 系列。Acme 涵盖了前述的一般 LLM 开发流程中的任务（见 §2.1）。需要注意的是，Acme 不涉及任何服务任务，因为我们的 LLM 被部署在专门用于服务的独立集群上。</p>
<p><strong>软件栈：</strong> 为了支持在数千个 GPU 上训练亿级参数的模型，我们构建了一个名为 InternEvo 的系统，集成了多种系统优化技术，如 FlashAttention、3D 并行、零冗余优化、混合精度训练、选择性激活重计算以及细粒度通信重叠。此外，该系统还支持模型微调和评估等附加任务。</p>
<h5 id="2-3-Traces-from-Acme"><a href="#2-3-Traces-from-Acme" class="headerlink" title="2.3 Traces from Acme"></a>2.3 Traces from Acme</h5><p>表2比较了Acme的规格和跟踪信息与由Microsoft、SenseTime和Alibaba进行的先前跟踪分析工作。与Acme专注于LLM开发不同，这些数据中心包含来自各个领域的通用深度学习工作负载。例如，Helios [38] 包含4个专用于计算机视觉和强化学习模型训练的集群，而PAI [97] 则包括用于训练和服务作业的多种服务器。</p>
<p> <img src="/2025/02/03/scheduler_1/b62d91ae4ef045a8a11d4e55e5db2511.png" alt="在这里插入图片描述"></p>
<p><strong>跟踪来源：</strong> 我们的特征化研究基于从Acme的两个LLM集群收集的跟踪数据。这些跟踪数据跨越2023年3月至8月的6个月时间。Seren集群包含了368K个CPU作业和664K个GPU作业，而Kalos集群的作业跟踪数据包含了42K个CPU作业和20K个GPU作业。此外，我们总结了我们研究中使用的跟踪数据来源：</p>
<ol>
<li><strong>作业日志</strong>：我们从调度数据库中收集作业日志，包括每个作业的执行时间（提交、开始和结束时间）、最终状态（完成、取消、失败）、请求的资源（CPU、GPU、内存）、工作目录以及其他相关数据。</li>
<li><strong>硬件监控数据</strong>：这些数据来源广泛，来自多个维度长期采集。我们从Prometheus数据库获取CPU、内存和网络使用数据，从NVIDIA DCGM获取与GPU相关的指标，以及从IPMI获取与电源相关的数据。这些数据的采样间隔设置为15秒。</li>
<li><strong>运行时日志</strong>：为了进行精确的作业失败分析，我们捕获LLM框架在作业执行期间的标准输出和标准错误日志。</li>
<li><strong>性能分析数据</strong>：针对一部分具有代表性的作业，我们使用DCGM等工具进行深入的性能分析。这些跟踪维度的协同作用使我们能够全面了解数据中心中LLM作业的特性。</li>
</ol>
<h4 id="3-Datacenter-Characterization"><a href="#3-Datacenter-Characterization" class="headerlink" title="3. Datacenter Characterization"></a>3. Datacenter Characterization</h4><p>本节中，我们对Acme进行了深入分析，包括比较LLM与之前DL工作负载之间的工作负载分布（§3.1），调查不同LLM工作负载类型（§3.2），探索资源利用模式（§3.3），以及评估环境影响（§3.4）。</p>
<h5 id="3-1-LLM与先前DL工作负载的对比"><a href="#3-1-LLM与先前DL工作负载的对比" class="headerlink" title="3.1 LLM与先前DL工作负载的对比"></a>3.1 LLM与先前DL工作负载的对比</h5><p>  <img src="/2025/02/03/scheduler_1/31cf84e94ceb421ea1100eef22dffe13.png" alt="在这里插入图片描述"></p>
<p><strong>更短的作业持续时间：</strong> 如图2(a)所示，与普遍认为的LLM相关作业通常长时间运行的刻板印象相反，我们发现我们集群中的工作负载（蓝色和橙色线条）展示出较短的GPU作业持续时间（即作业运行时间，不包括排队延迟），与先前作业追踪中观察到的DL工作负载（虚线）相比。具体来说，Seren和Kalos的中位数作业持续时间为2分钟，比其他集群的中位数作业持续时间短了1.7∼7.2倍。此外，显然最近的追踪显示了较短的作业持续时间分布。特别是，在考虑到Philly集群的平均作业持续时间（2017年收集）时，它比Helios（2020年）和PAI（2020年）长了2.7∼3.8倍，比Acme（2023年）长了12.8倍。<strong>为了解释这一观察结果，我们概述了三个潜在因素：</strong>（1）硬件升级。GPU和网络的迭代带来了显著的效率提升。（2）资源丰富。用户通常请求更多资源（如表2所示），在Seren平均5.7个GPU和Kalos平均26.8个GPU。这可以显著加速训练过程。（3）广泛的关联工作负载：LLM开发流水线涉及许多小规模的关联作业，例如评估。我们将在§3.2中深入探讨这一点。（4）高未完成率：大约40%的作业失败，完成的作业只消耗GPU资源的20∼30%。这突显了需要一个容错系统的紧迫性。有关更多详细信息，请参见图17和附录A.1。</p>
<p><strong>两极化的GPU利用率：</strong>图2(b)展示了各个集群中整体的GPU利用率分布。显然，我们两个集群中的GPU利用表现出两极化的模式，主要集中在两个明显的状态：0%和100%。这种两极化主要源于我们集群中的<strong>工作负载具有相似的模型架构</strong>，即基于transformer的LLMs。相比之下，Philly和PAI则涵盖了更广泛的利用率范围。此外，当比较中位数GPU利用率时，Seren和Kalos分别表现出97%和99%的显著高值，而Philly和PAI的观察值分别为48%和4%。这一观察结果符合LLMs计算密集的普遍理解。它还表示，基于GPU共享的调度技术[40,98,99,106]可能不适合LLM的开发。需要注意的是，“GPU利用率”有时可能是一个较弱的利用率指标[8, 94]。我们将在§3.3中提供更精确的利用率分析。</p>
<p> <img src="/2025/02/03/scheduler_1/9ea3fdddc4574ec998dda10c73f6acd4.png" alt="在这里插入图片描述"></p>
<p><strong>高度倾斜的工作负载分布：</strong> 我们进一步研究了与作业数量相关的GPU需求的累积分布函数（图3(a)）和GPU时间（图3(b)）。在作业数量方面，所有集群都显示出类似的模式，即大多数作业为单GPU作业，少于7%的作业请求超过8个GPU。然而，当考虑GPU时间时，单GPU作业仅占我们两个集群不到2%的资源，而在PAI中却占据了超过68%的GPU时间。与之形成鲜明对比的是，Kalos集群中大规模作业（≥256个GPU）主导了GPU时间，占据了超过96%的资源。</p>
<p>这种更为陡峭的分布给集群调度器的设计带来了重大挑战。<strong>大部分资源被分配给少数预训练作业，可能导致头部阻塞问题，从而造成严重的排队延迟。</strong>现有的DL集群调度器通常依赖于抢占机制，但是考虑到恢复的巨大开销，这些机制并不适用于LLM工作负载。这凸显了迫切需要为LLM集群量身定制调度系统的重要性，需要考虑整个流程的工作负载特征。</p>
<h5 id="3-2-Workload-Categories"><a href="#3-2-Workload-Categories" class="headerlink" title="3.2 Workload Categories"></a>3.2 Workload Categories</h5><p>为了深入了解LLM开发流程（§2.1）中不同工作负载的特征，我们根据它们的生产分工和元数据（如作业名称），进一步将作业分类为特定类型。</p>
<p> <img src="/2025/02/03/scheduler_1/c3861282fe424f81a3c1194420dc8ddd.png" alt="在这里插入图片描述"></p>
<p><strong>作业数和资源利用的不相关性：</strong> 图4展示了各种工作负载类型的作业数量和GPU时间分布，其中只有Seren包含SFT和MLLM工作负载。此外，MLLM作业包含自己的开发流程（如预训练），并采用较小规模的模型进行探索。我们的分析主要集中在LLM作业上。显而易见，评估作业在总作业计数中占据了大多数，然而它们在资源消耗方面相对较小（Kalos中为0.8%）。相反，预训练作业仅占总作业计数的0.9%和3.2%，但在Seren和Kalos中分别消耗了69.5%和94.0%的总GPU时间</p>
<p>  <img src="/2025/02/03/scheduler_1/ed4dfdb7f82c48c7882fee37ab2bcc67.png" alt="在这里插入图片描述"></p>
<p><strong>作业类型与GPU需求相关性：</strong> 我们进一步在图5中描述了各种工作负载类型的GPU需求分布。每个箱子由第一四分位数和第三四分位数框定，箱内黑线表示中位数值。两个“须”在1.5倍四分位距（IQR）处定义。与通常需要少于4个GPU的评估作业相比，预训练作业通常需要超过100个GPU。这一观察部分解释了为什么图4(d)中Kalos中的评估作业消耗的资源很少。此外，我们注意到调试作业的GPU请求范围很广，这与评估作业通常需要各种类型任务的事实相一致。<br><img src="/2025/02/03/scheduler_1/28d5061e5b9b4bdba07e2f2938329257.png" alt="在这里插入图片描述"></p>
<p><strong>相同的时间分布：</strong> 图6展示了不同工作负载的作业持续时间和排队延迟分布。就作业持续时间而言，尽管预训练作业持续时间最长，但它们在中位数上超过其他工作负载一个数量级，并且在两个集群中不到5%的作业持续时间超过1天。这可以归因于预训练过程中频繁的失败，这将在§5中进一步探讨。至于作业排队延迟，与之前的报告[38, 45, 97]建议的大规模作业经历更长等待时间不同，我们观察到<strong>尽管评估作业的GPU需求最低，作业持续时间最短，但它们的排队延迟最长。</strong>这种差异是因为大多数资源都被保留用于预训练作业，以最小化它们的排队延迟。评估作业通常作为低优先级的批量同时提交，利用有限的备用资源。</p>
<h5 id="3-3-Infrastructure"><a href="#3-3-Infrastructure" class="headerlink" title="3.3 Infrastructure"></a>3.3 Infrastructure</h5><p>除了负载特性分析外，我们进一步对基础设施利用情况进行了全面分析。</p>
<p> <img src="/2025/02/03/scheduler_1/bcc0520bd99a486ab69f4f8e4c0e4d51.png" alt="在这里插入图片描述"></p>
<p><strong>更高的GPU利用率：</strong> 如图7(a, b)所示，考虑到GPU在LLM开发中的关键作用，我们从DCGM [7]中收集了细粒度的性能计数器指标，包括SM活动（PROF_SM_ACTIVE）张量核心活动（PROF_PIPE_TENSOR_ACTIVE）和GPU内存占用（DEV_FB_USED）。与PAI [97]中大部分GPU内存未充分利用（内存利用率低于25%）不同，我们在Kalos中的观察表明，<strong>50%的GPU占用了超过75%的GPU内存（60 GB）</strong>。此外，我们观察到两个集群的中位数SM活动约为40%，是PAI报告的20%的两倍。这些发现与LLM的内存密集型和计算密集型特性一致。</p>
<p><strong>未充分利用的关联资源：</strong>我们还深入研究了与LLM开发密切相关的CPU、主机内存和网络方面。在图7(b)中，我们比较了主机端和GPU端的内存占用情况。显而易见，CPU内存利用率保持在50%以下。需要注意的是，Kalos相比Seren拥有两倍的内存容量（2TB）（见表1）。这显示了<strong>CPU内存的显著未充分利用</strong>。附录A.2提供了更详细的分析。尽管GPU内存卸载技术[80,81]提高了CPU内存利用率并缓解了GPU内存限制，但由于有限的PCIe带宽，它也会影响训练吞吐量。因此，我们不采用卸载机制。此外，由于高CPU与GPU比例（每GPU 16个CPU），CPU通常处于低利用状态，如图7(c)所示。此外，在图7(d)中，我们测量了Seren中IB的网络发送和接收带宽。两条线重叠良好，IB在LLM执行期间用于对称通信。我们观察到NIC在超过60%的时间内保持空闲状态，并且活动带宽很少超过IB提供的最大带宽的25%。</p>
<h5 id="3-4-Environemntal-Impact"><a href="#3-4-Environemntal-Impact" class="headerlink" title="3.4 Environemntal Impact"></a>3.4 Environemntal Impact</h5><p> <img src="/2025/02/03/scheduler_1/1de8768282ca4e0ea101fcf1f1c921c1.png" alt="在这里插入图片描述"></p>
<p>图8(a)展示了GPU能耗的分布情况。我们观察到约30%的GPU处于空闲状态，但仍需消耗60W的电力。此外，由于计算需求强烈，我们发现在Seren和Kalos中，有22.1%和12.5%的GPU消耗超过400W（TDP），甚至有些达到600W，可能引发一些亚稳态问题[41]。图8(b)呈现了Seren中所有GPU服务器以及额外6台仅CPU服务器的能耗分布情况。我们发现<strong>GPU服务器的平均能耗是CPU服务器的5倍</strong>。</p>
<h4 id="4-Workload-Profilling"><a href="#4-Workload-Profilling" class="headerlink" title="4.Workload Profilling"></a>4.Workload Profilling</h4><p>在本节中，我们对代表性任务的资源利用进行了细粒度分析。我们关注预训练和评估任务，因为它们是最消耗资源或数量密集的工作负载。</p>
<h5 id="4-1-Pretraining-Workload"><a href="#4-1-Pretraining-Workload" class="headerlink" title="4.1 Pretraining Workload"></a>4.1 Pretraining Workload</h5><p> <img src="/2025/02/03/scheduler_1/26f8299ab7a34a24a99eed47ae1be892.png" alt="在这里插入图片描述"></p>
<p>如前所述，预训练LLMs需要大量的计算资源。为了提高训练效率，我们的预训练框架InternEvo [25] 在系统设计上进行了持续的优化和迭代。如图10所示，InternEvo的初始版本（早期作业采用的版本）分别如下：(a) 主要采用类似于MegatronLM [68] 的3D并行，(b) 使用分层的ZeRO机制 [25]，实现模型状态的选择性冗余分片。举例说明，我们对一个拥有1230亿参数的LLM在2048个GPU上的性能进行了详细的分析。我们也在附录A.4中提供了1024个GPU的分析结果。(a) 3D并行方法，我们采用了管道并行ism配置= 4，张量并行ism= 8。我们对第一个管道秩的第一个GPU进行了性能分析。(b) 分层ZeRO方法，我们将参数分片限制为每组64个GPU，并启用了重新计算。我们以1毫秒间隔采集了像DCGM指标这样的GPU性能计数器。</p>
<p><strong>GPU SM利用率</strong>：图10展示了相同LLM在不同训练策略下的GPU SM利用率。这两个版本保持了相同的全局批处理大小，并根据各自的配置进行了优化。显然，相比于InternEvo V1，InternEvo V2呈现出更优秀的峰值SM利用率，并减少了空闲时期，实现了约16%的加速。<strong>3D并行的相对低利用率主要是由于混合并行引入的通信对关键路径的影响，例如管道并行中的气泡。</strong>需要注意的是，不同的节点内和节点间通信硬件设置可能导致不同的最佳配置。</p>
<p> <img src="/2025/02/03/scheduler_1/a64d3c829d184d0caa34b4f89d64ba7d.png" alt="在这里插入图片描述"></p>
<p><strong>GPU内存占用：</strong> 对于一个包含Ψ个参数的模型，在使用Adam [48]优化器进行主流混合精度训练时，参数、梯度和优化器状态的内存占用分别为2Ψ、2Ψ和12Ψ。为了降低内存成本，ZeRO [79]有效地将这些元素的冗余内存分片到全局GPU工作进程中。图11展示了使用Pytorch内存快照工具 [11]捕获的GPU随时间的实际内存使用情况。上部动态部分代表激活和梯度，而下部静态部分代表参数和优化器状态所占用的内存。需要注意的是，图中仅展示已分配的内存，而保留的内存未呈现。<strong>我们的分析显示，与分层ZeRO相比，3D并行中激活内存的需求显著更高。这一观察强调了高效的激活内存管理作为提升3D并行批处理大小和吞吐量的关键因素</strong>。</p>
<p> <img src="/2025/02/03/scheduler_1/225aed6035df48e7adfff6e131e7b4f3.png" alt="在这里插入图片描述"></p>
<p><strong>激活大小的不平衡：</strong> 在使用管道并行时，每个等级需要持有不同数量的激活，因为在各个管道等级上，待进行反向计算的微批次数量各不相同。图12展示了这种不平衡问题在不同管道等级上的情况。<strong>这表明我们应该采用专门的分区机制来解决管道并行中不同等级之间的内存使用不平衡问题，以实现更高的效率，比如重新计算激活。</strong></p>
<h5 id="4-2-Evilatopm-Workload"><a href="#4-2-Evilatopm-Workload" class="headerlink" title="4.2 Evilatopm Workload"></a>4.2 Evilatopm Workload</h5><p><strong>在LLM预训练过程中，定期评估生成的检查点对指导LLM预训练的演进至关重要</strong> 。因此，LLM评估作业占据了大多数作业，每个作业在不同的LLM基准数据集上执行指标计算。我们分析整个评估工作流程，并结合细粒度的资源使用信息定量地展示两个即将到来的资源利用问题。我们还将在第6.2节讨论相应的解决方案</p>
<p> <img src="/2025/02/03/scheduler_1/e744bd43d4324fc19ac1422361be39bd.png" alt="在这里插入图片描述"></p>
<p><strong>模型加载和数据预处理高开销：</strong> 在评估作业的初始化阶段，为每个任务加载模型检查点至关重要。此外，数据预处理阶段，特别是标记化，构成了显著的时间开销。这些因素导致了分配的GPU资源在相对较长时间内的低利用率。如图13所示，在实际GPU推理之前，评估任务消耗超过1分钟，占评估持续时间的29.5%。这种开销可能会随着模型或数据集规模的增大而增加。为了解决预处理开销，一个有效的策略是缓存标记化数据。此外，评估作业具有灵活性，可以将多个评估任务（数据集）合并为单个作业。这种合并可以有效减少评估过程中模型加载阶段的相对时间开销。</p>
<p><strong>指标计算高开销：</strong>评估过程经常涉及复杂且耗时的指标计算。例如，需要在编码数据集（如HumanEval [24]和MBPP [17]）上执行合成程序正确性测试。此外，还会调用OpenAI GPT-4 API评估模型对话的表现（例如Chatbot Arena [112]）。这些过程可能需要长达30分钟的时间，在此期间GPU保持空闲状态。因此，我们可以观察到GPU使用的不同阶段，包括需要GPU进行推理和生成的阶段，以及不需要GPU进行指标计算和验证的阶段。以HumanEval基准测试为例，如图13所示，GPU在最后42秒处于空闲状态，浪费了总GPU时间的约19.0%。</p>
<h4 id="5-Failure-Analysis"><a href="#5-Failure-Analysis" class="headerlink" title="5 Failure Analysis"></a>5 Failure Analysis</h4><p>本节我们将基于我们两个集群的运行日志和硬件监控数据，对作业失败进行全面分析。在Kalos集群中，我们收集了32,500个任务的日志，其中包括31,293个（96.3%）推理任务，647个（2.0%）预训练任务和调试任务（1.7%）。在Seren集群中，我们仅收集了675个预训练任务的日志。此外，针对预训练任务，我们提取了所有相关信息和元数据，包括实际训练步骤、冷启动开销、恢复时间戳等。我们希望通过我们的分析为未来LLM开发中的容错研究提供深刻见解。</p>
<h5 id="5-1-故障分类"><a href="#5-1-故障分类" class="headerlink" title="5.1 故障分类"></a>5.1 故障分类</h5><p>我们采用了一种故障诊断系统，结合基于规则和LLM技术的方法，从运行时日志中提取错误信息。我们在第6.1节详细解释了这个系统。此外，为确保准确识别故障类型和根本原因，进行了手动检查和修正。表3总结了Acme中常见故障的发生频率和重启时间。基本上，它们可以分为以下三类。请注意，这些分类可能有重叠，对特定错误类型分类的主要标准是其最频繁发生的情况</p>
<p> <img src="/2025/02/03/scheduler_1/1f41f9b81f1347aaa27ec152d5cab774.png" alt="在这里插入图片描述"></p>
<ul>
<li><strong>基础设施：</strong> 基础设施相关的故障源于底层计算平台或远程存储的问题。这些故障主要发生在作业执行过程的中途阶段，尤其是在预训练任务中。它们由于复杂且耗时的恢复过程严重影响训练进度。</li>
<li><strong>框架：</strong> 多种类型的运行时错误，如RuntimeError、ValueError和AttributeError，可能与张量操作、形状、数据类型或意外行为有关。它们通常出现在作业的初始阶段，并且通常通过修复配置来解决。</li>
<li><strong>脚本：</strong> 脚本错误通常源于编程错误或用户疏忽。它们占据了大多数的故障，并且通常通过修订代码来解决.</li>
</ul>
<h5 id="5-2-Failure-Characterization"><a href="#5-2-Failure-Characterization" class="headerlink" title="5.2 Failure Characterization"></a>5.2 Failure Characterization</h5><p>我们从分析中得出了几个关键观察：<strong>基础设施故障造成的影响最为严重</strong>。如表3所示，由于基础设施问题导致的作业失败往往使用大量GPU资源（GPU需求），并且重新启动需要相当大的工作量（重启时间）。它们占用了超过82%的GPU时间（GPU时间），但失败作业数量仅占11%（数量）。这些作业大多是长期的预训练任务，可能多次遭遇硬件故障，如GPU问题（例如CUDAError、ECCError）、NVLink问题（NVLinkError）以及网络系统问题（NCCLRemoteError、S3StorageError）。需要注意的是，NodeFailure表示由于不明确的硬件问题而导致的未分类错误。解决这些基础设施故障需要精细的诊断工作，以准确定位问题的根源，通常需要对有缺陷的硬件进行维护或更换，这导致了显著的重启成本。</p>
<p><strong>高温引起的故障：</strong> 另一个显著的观察是，在Kalos训练7B模型往往会导致GPU过热，可能引发NVLinkError或ECCError。这种现象主要是由于高度优化的通信成本，导致GPU空闲率异常低。我们观察到，在训练这些模型时，整个集群服务器室内的温度大约上升了5°C。此外，我们发现大多数这类故障发生在2023年7月，这是有记录以来最炎热的月份[63]。这种异常的气候可能是这些故障的潜在原因，这与微软最近的研究结果相一致[100]。我们在附录A.5中提供了关于GPU温度的更详细数据。随后，我们的团队增强了集群的冷却能力，显著减少了这类故障的发生频率。</p>
<p><strong>许多故障由辅助服务引起：</strong> 在我们的预训练框架中，我们连接到外部组件或服务进行指标报告、日志记录、监控和警报。这些辅助服务容易受到网络不稳定性的影响，可能导致超时或故障，从而减缓或中断训练过程。大量的ConnectionError和NetworkError事件源自这些辅助服务。</p>
<p><strong>评估作业很少遇到错误：</strong> 在Kalos中，仅有6.7%的评估任务遇到错误，特别地，并未记录GPU或NVLink的故障情况。低错误率可能归因于评估任务持续时间较短，对GPU和NVLink连接的压力较小。因此，这降低了硬件和操作故障在评估作业中更频繁发生的可能性<br><img src="/2025/02/03/scheduler_1/f138dab1049d4e2f912c929c509fa958.png" alt="在这里插入图片描述"></p>
<h5 id="5-3-Failure-Characterization"><a href="#5-3-Failure-Characterization" class="headerlink" title="5.3 Failure Characterization"></a>5.3 Failure Characterization</h5><p>我们在以下三种情况下会重新启动作业：(1) 当作业内部发生错误时，(2) 当训练指标异常，如损失急剧上升时，以及(3) 当训练过程陷入停滞时。所谓的“损失急剧上升”指的是先前正常下降的损失突然增加，并且在一定时间内无法恢复。重新启动时，作业会回到上一个检查点，导致训练进度的损失。由于现有的LLM框架缺乏自动恢复支持，开发人员通常需要手动重新启动中断的训练作业。开发人员经常需要轮流值班，以确保预训练模型及时完成。</p>
<p>如图14所示，在我们手动处理所有故障的早期阶段（3月至4月），我们选择了两个预训练作业。我们从两个集群大规模模型训练过程的日志中提取信息，包括每次提交的运行持续时间、开始和结束时间，以及训练的初始和最终迭代次数。104B模型是在框架还在开发初期时的早期尝试。因此，加载之前的模型检查点导致了整体训练过程中的重大损失。相比之下，一个月后123B模型的训练中，我们改进了框架并采用了更小的检查点保存间隔。此外，我们增加了一个优雅终止作业的功能，在结束作业之前允许保存当前的训练结果。显然，123B模型的训练过程更为稳定，由于回滚而造成的损失更少。然而，这一进展是有代价的，因为不同时间中断的作业必须迅速重新启动。</p>
<h4 id="6-Deployed-LLM-Systems"><a href="#6-Deployed-LLM-Systems" class="headerlink" title="6. Deployed LLM Systems"></a>6. Deployed LLM Systems</h4><p>正如前文所强调的，LLM的开发过程面临重重障碍，但也揭示了克服这些问题的可行策略。本节将分两个阶段介绍我们的工作：(1) 预训练阶段：通过LLM相关的故障诊断和自动恢复来增强容错能力。(2) 评估阶段：通过任务分解实现及时的性能响应。</p>
<h5 id="6-1-Fault-tolerant-Pretraining"><a href="#6-1-Fault-tolerant-Pretraining" class="headerlink" title="6.1 Fault-tolerant Pretraining"></a>6.1 Fault-tolerant Pretraining</h5><p><strong>动机：</strong> 在LLM预训练过程中，由于涉及大量GPU和训练过程的长时间持续性，故障不可避免且经常发生 [15, 44, 88, 96]。这些故障会严重阻碍训练进度，导致资源利用效率低下（§5）。因此，为了最小化基础设施的停机时间，通常采用轮值值班的方式手动处理故障。这给工程师和研究人员带来了重大负担，正如Meta OPT [110]和BigScience BLOOM [1]团队所抱怨的那样。我们的团队也面临这个问题。为了减轻这一负担并提升硬件效率，我们开发了一个系统，自动检测故障根本原因并促进恢复。</p>
<p><strong>系统设计：</strong> 我们的容错系统无缝集成到LLM预训练框架中，包括三个关键模块：(1) 检查点：增加频繁的模型保存，以最小化训练进度的损失；(2) 诊断：使用启发式规则和LLM结合，准确识别不同故障的根本原因；(3) 恢复：采用全面的检测工具包，确定故障节点，并从适当保存的检查点自动重新启动训练。我们将详细讨论这些模块。</p>
<p><strong>1.异步检查点</strong>：频繁的检查点有效减少了由意外故障引起的浪费时间 [32]。然而，由于LLM可以生成TB级别的模型状态（即跨所有GPU的总模型状态），保存检查点本身可能会引入显著的开销，导致训练时间减慢高达43% [60]。为了解决这个问题，我们采用了异步检查点策略 [64, 69]，有效地将检查点过程与训练过程分离。我们的观察表明，CPU内存（参见图7 (b)）能够容纳多个检查点。通过利用这一点，我们可以将模型状态存储在内存中，并利用单独的线程定期将这些状态保存到远程持久存储。这种简单的策略显著减少了检查点的开销。</p>
<p> <img src="/2025/02/03/scheduler_1/1638a6ca0b3e4e91b107242e268430dc.png" alt="在这里插入图片描述"></p>
<p><strong>2.故障诊断.：</strong> 正如我们在§5讨论的那样，故障可能源自多种复杂因素，包括用户脚本或框架的错误，以及处于高压条件下的硬件问题。确定故障是否可恢复对于自动恢复至关重要。一种常见的方法是利用启发式规则的组合，对故障作业的日志进行过滤和正则表达式匹配 [23, 45, 52, 53, 66]。然而，由于错误日志的广泛多样性和复杂性，这种方法经常表现不准确。许多情况下可能没有特定的错误声明，而是多种错误同时存在。例如，一个作业可能因包含NCCLTimeoutError、CUDAError和多种RuntimeError的消息而失败，而根本原因可能是CUDAError。尝试用特定规则集匹配每种错误场景可能变得不切实际。</p>
<p><strong>➤Real-time Log Compression</strong>：由预训练作业生成的大量日志文件主要包含训练指标记录，其大小可达数百MB。为了加速诊断并满足LLM的上下文长度限制，首先进行日志压缩。系统持续更新一组正则表达式集合，称为过滤规则。这些规则高效地移除常规的日志输出，如初始化信息、训练指标记录、框架输出和调试信息。系统的关键组成部分是基于LLM的日志代理，负责分析实时生成的日志片段，并识别符合固定模式的行。通过这种方式，基于LLM的日志代理动态地编写正则表达式，更新过滤规则，有效地减小日志文件的大小。此外，日志代理将识别的错误消息转发给后续的诊断模块。</p>
<p>此外，我们采用自一致性方法来确保日志代理结果的稳健性和这些结果的格式化 [95]。这包括对每个日志片段进行多次处理，并让另一个LLM对日志代理的多个结果进行投票，通过正则表达式确保匹配的准确性。随着时间的推移，过滤规则对当前任务变得更加全面，使日志过滤过程更加高效。此外，系统可以利用任务的元数据识别重复或相似的任务，直接应用现有的过滤规则进行日志过滤，从而避免冗余工作。在大型模型集群环境中，这一特性尤其有益，因为较少的租户和任务重新提交是常见的。</p>
<p>➤ <strong>LLM-assisted Automated Diagnosis</strong>：日志代理高效压缩运行时日志，隔离像CUDA错误或运行时异常等关键错误日志。尽管日志在抵达该模块时已经压缩，但错误日志可能仍然很长。我们采用两步方法来解决这个问题。首先，将错误日志与在过去多失败作业错误的诊断定义的规则进行比较。如果预定义的规则无法诊断问题，压缩日志将通过嵌入模型进行向量化，并存储在向量存储库中，作为检索库。然后，故障代理介入。它利用查询引擎 [55] 搜索向量存储库。通过这种搜索，故障代理可以识别反映作业中断根本原因的日志行，提取错误类型，并指示错误是否源自用户错误或基础设施故障，为恢复过程提供线索。此外，它还为用户或运维团队生成缓解建议。</p>
<p>故障代理还对故障诊断系统的持续学习做出贡献。对于每一个新的故障，一旦诊断并解决完成，故障代理会编写相应的正则表达式，并将其添加到基于规则的诊断模块中。这个过程是迭代的，确保故障诊断系统不断进化，能够更加熟练地诊断并提出故障的缓解方法。为了实现更加稳健的性能，目前我们使用GPT-4 [2] 进行诊断，计划逐步过渡到我们的LLM模型。</p>
<p><strong>3. 快速故障检测与恢复：</strong> 根据故障诊断结果，如果属于某种基础设施故障，我们会进行相应的检测测试以识别问题节点。例如，为了迅速解决频繁出现的NVLinkError问题，我们采用了两轮NCCL测试[5]方法。首先，我们将所有节点分成多个两节点组，并在每对节点间执行allgather任务。如果服务器总数为奇数，我们将一个组设为三节点。如果某个组中的allgather任务失败，则该组中的节点可能是故障节点。在第二轮中，我们将可能的故障节点与正常节点配对，形成新的组。每组中的节点继续执行allgather任务，从而识别出故障节点并隔离它们。另一方面，如果故障归因于损失的突然增加（即‘loss spike’[27, 110]），这会自动由我们的预训练框架触发，我们选择恢复到较早的健康检查点并跳过后续的数据批次。此方法有效维护了模型质量。</p>
<p><strong>系统性能：</strong> 我们的异步检查点策略大幅减少了检查点开销，因为检查点过程不会阻塞训练过程。7B和123B模型大小的检查点时间和开销比例分别减少了3.6到58.7倍（间隔=30分钟）。请注意，持久化存储所需的时间不包含在异步检查点测量中。此外，我们的故障诊断系统显著减少了约90%的人工干预，从而减轻了开发人员的负担。请注意，由于系统的某些组件仍在改进中，这不是严格的评估。</p>
<p><strong>动机：</strong> 仅凭单一指标（如训练损失）来评估LLM的质量可能无法提供准确的评估[58]。因此，必须结合多种标准并在一系列任务中评估性能[22]。我们的LLM框架在预训练阶段的每个检查点都会进行定期评估。这使开发人员能够跟踪模型训练的进展，并识别出最佳的模型检查点。我们旨在提供快速反馈，以便及时调整。然而，如图6所示，由于资源有限和大量试验的同时提交，评估任务经历了最长的排队延迟。尽管面临这些挑战，我们还是发现了几种加速评估过程的机会。</p>
<p><strong>系统设计：</strong> 我们开发了一个试验协调器，以协调集群调度器和LLM框架的操作。该设计包括以下三种关键技术，旨在提高评估过程的效率。</p>
<p> <img src="/2025/02/03/scheduler_1/4789cf2ae27f4ad1a4ef2d28d7f165d7.png" alt="在这里插入图片描述"></p>
<p><strong>1. 解耦远程模型加载</strong>：由于LLM的规模庞大，从远程存储检索和加载它们可能是一个漫长的过程。此外，同时执行大量评估任务（约60个数据集）会因争用增加而使加载过程更加复杂。如图16（左）所示，在Seren中并行评估试验时，单个节点上单GPU试验的数量从1增加到8时，模型加载速度显著下降，这是由于存储NIC的带宽限制（25Gb/s）。另一方面，当试验数量在8到256 GPU之间时，加载速度趋于稳定。这个观察启发了我们采用一种战略方法。我们将模型加载过程与评估过程分离，而不是将每个评估数据集作为一个单独的试验提交，如图16（右）所示。具体来说，试验协调器首先从集群调度器获取可用节点列表，然后为每个节点生成一系列前置任务。这些任务将模型从远程存储加载到本地共享内存。接着，协调器将评估任务提交给调度器，评估任务通过高带宽PCIe加载模型。该方法有效地利用了空闲的主机内存。在评估完成后，协调器清理文件。</p>
<p><strong>2. 解耦指标计算</strong>：如图13所示，评估过程通常涉及复杂且耗时的指标计算。例如，在编码数据集如HumanEval [24] 和MBPP [17]上，必须执行合成程序正确性测试。为了解决这一问题，我们将指标计算过程与评估试验分离开来。当模型推理在GPU上完成后，其输出会迅速保存到文件中，从而终止推理任务。由于输出通常是基于文本的，体积较小，这个文件转储过程非常迅速。然后，我们生成CPU任务来进行指标计算。这种方法有效地减少了GPU的空闲时间，加快了评估过程。</p>
<p><strong>3. 基于优先级的弹性调度：</strong> 除了解耦方法外，我们注意到，对于每个评估数据集的近似试验运行时间，我们已有相当可靠的先验知识。此外，这些数据集是灵活的，我们可以将多个数据集批量合并到一个试验中，以规避模型加载。我们还可以拆分大型数据集并解耦指标计算。因此，试验协调器能够通过分解最大化GPU的占用率，利用先验信息平衡每个GPU的工作负载，并在排序的作业队列中采用轮循分配策略。此外，我们优先处理作业队列中CPU指标计算时间较长的评估试验，以更好地重叠其计算过程。此方法不仅增强了工作负载平衡，还最小化了试验切换的开销。</p>
<p><strong>系统性能：</strong> 我们对试验协调器进行了代表性测试，使用了一个典型的7B规模LLM评估任务，涉及在63个数据集上的工作负载评估。我们分别在两种不同条件下测量了完成所有评估试验所需的总时间：单节点（代表资源有限）和四节点（代表资源相对充足）。结果显示，试验协调器能够分别将总时间减少1.3倍和1.8倍。</p>
<p><strong>7 Discussion</strong></p>
<p><strong>范围限制：</strong> 尽管我们尽最大努力分析了Acme的工作负载，但我们无法覆盖所有类型的工作负载。具体限制包括：</p>
<ol>
<li>我们的分析集中在模型服务之前的开发过程，而Acme不包含任何服务作业（即部署阶段的工作负载）。</li>
<li>我们的分析主要集中在GPU作业，对CPU作业的关注较少。</li>
<li>我们主要描述了基于Transformer的、仅包含解码器架构的模型（如GPT-3和LLaMA 2）。对于新型的模型架构，我们在附录A.6中简单描述了专家混合（MoE）模型。其他模型架构，如多模态LLM，不在我们的分析范围内。</li>
</ol>
<p><strong>持续系统改进：</strong>随着大模型的快速发展，本文所述的系统可能无法满足未来工作负载的需求。对此，我们正在积极优化我们的系统，以适应更先进的训练工作负载，包括长序列预训练、MoE预训练和高效的RLHF。此外，我们正在升级基础设施，特别关注网络接口控制器（NIC），并扩展计算集群，以促进更大规模的预训练。我们还在探索一些有前景的方向，例如通过使用Hydro进行超参数优化来提高LLM的质量，以及为新兴模型架构（如Diffusion和Mamba）提供高效的系统支持。</p>
<h4 id="8-Conclusion"><a href="#8-Conclusion" class="headerlink" title="8.Conclusion"></a>8.Conclusion</h4><p>总之，我们分析了数据中心Acme中LLM的工作负载和资源利用情况，揭示了LLM开发的独特特点和挑战，例如资源低效和故障影响。我们还发现了一些为LLM系统优化的潜在机会，并介绍了我们在预训练和评估工作负载方面的努力。我们相信，这些经验和见解具有广泛的适用性，能够为后续研究带来显著益处。</p>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
        <category>调度</category>
      </categories>
      <tags>
        <tag>scheduler</tag>
      </tags>
  </entry>
  <entry>
    <title>transformer学习笔记</title>
    <url>/2025/02/03/transformer-not/</url>
    <content><![CDATA[<p>介绍Transformer的两个阶段，以及如何控制推理过程中的KV-Cach缓存大小来优化推理速度，最后分析Transformer的性能瓶颈和优化策略。</p>
<span id="more"></span>
<p>使用 Transformer 解码器（Transformer decoder）生成 tokens 需要以下两个步骤。这两个步骤分别是处理提示语步骤和多个自回归步骤。两个步骤在硬件利用上有着截然不同的特征。</p>
<p><a href="https://segmentfault.com/a/1190000044790264">程序员 - LLM 推理优化探微 (4) ：模型性能瓶颈分类及优化策略 - IDP技术干货 - SegmentFault 思否</a></p>
<h4 id="1-LLM-做出回答的两个阶段"><a href="#1-LLM-做出回答的两个阶段" class="headerlink" title="1.LLM 做出回答的两个阶段"></a>1.LLM 做出回答的两个阶段</h4><p>文本生成的两个阶段包括:启动阶段和生成阶段。</p>
<p>Transformer 解码器的模型轮廓图:<br><img src="/2025/02/03/transformer-not/bed5e37508e74fa390f4ed8949a91dd0.png" alt="在这里插入图片描述"></p>
<p>解码器本身并不输出 tokens，而是输出 logits（数量与词汇表的大小相同）（译者注：logits 是一个数值向量，其维度等于词汇表的大小，表示每个 token 的可能性分数。）在生成文本时，通过 logits 提取 tokens 的过程是通过一种被称为搜索策略（search strategy）、生成策略（generation strategy）或解码策略（decoding strategy）的启发式方法完成的。</p>
<p>基于 Transformer 的解码器从输入文本序列（通常称为提示语（prompt））生成文本（通常也被称为对输入文本的扩展或补充）基本上包括以下步骤：</p>
<ol>
<li>将模型权重加载到GPU</li>
<li>在CPU上对提示词(prompt)进行分词(tokenizing)，并将token张量传输到GPU。</li>
</ol>
<p>分词步骤示意图:</p>
<p><img src="/2025/02/03/transformer-not/0525b0974fe6495098901a06816ec47d.png" alt="在这里插入图片描述"></p>
<ol>
<li>将分词完成后的提示语输入神经网络，生成扩展的第一个token</li>
</ol>
<p><strong>这一阶段通常被称为启动阶段（initiation phase）。</strong> 在下一篇文章中，我们将看到它也经常被称为预填充阶段（pre-fill phase）。</p>
<ol>
<li>将生成token附加到输入的token序列中，并将其用作生成扩展文本中第二个token的新输入。然后，重复此过程，直到生成了停止序列或达到所配置的最大序列长度。</li>
</ol>
<p><strong>这个由多个步骤组成的阶段通常被称为生成阶段（generation phase）、解码阶段（decoding phase）、自回归阶段（auto-regressive phase），甚至是增量阶段（incremental phase）。</strong></p>
<ol>
<li>将完成的 tokens 从 GPU 获取到 CPU ，并对它们进行 detokenize（译者注：”detokenize“指的是将模型生成的 tokens 序列转换回原始文本或句子的过程。可能包括去除 tokens 之间的空格、添加标点符号、还原缩写等操作，以还原生成文本的自然语言形式。），以获取生成的文本（图5）。</li>
</ol>
<p>论在硬件上如何进行计算，两个阶段之间确实没有区别，因此两个阶段在这方面都没有什么特别之处。这种设置涉及大量冗余计算，因此在许多情况下效率低下。缓解这种情况的一种重要方式是缓存我们不想重新计算的内容。这种优化被称为 KV 缓存，并引入了我一直在暗示的这两个阶段之间的关键差异。</p>
<p><strong>单头注意力</strong> :</p>
<p>假设只处理长度为 t 的单个输入序列,则会有 t 个查询向量、t 个键向量和 t 个值向量。对于每个查询向量，都会生成一个输出向量，输出向量是输入序列中所有值向量的线性组合，每个值向量在线性组合中的权重由对应的注意力分数决定。换句话说，对于每个查询向量，生成的输出向量是通过对输入序列中的值向量进行加权求和而得到的，其中权重由注意力分数确定。对于给定的查询向量，都会与所有的键向量进行点积运算。点积运算的结果表示了查询向量与每个键向量之间的关联度，即它们的<strong>相似性</strong>。这些点积的结果经过适当的处理后，成为了<strong>注意力分数</strong>，用于权衡对应值向量在输出向量中的贡献。这样，我们就能为序列中的每个 token 生成一个包含其他 token 信息的向量表征，也就是说，我们为每个 token 创建了一个上下文表征（contextual representation）。</p>
<p>然而，在自回归解码（auto-regressive decoding）的情境中，我们不能使用所有可能的值向量来构建给定查询向量的输出表征。实际上，在计算与特定 token 相关的查询向量的输出时，我们不能使用序列中后面出现的 token 的值向量。<strong>这种限制是通过一种称为 masking 的技术实现的，实质上是将被禁止的值向量（即被禁止的 token）的注意力分数设置为零。</strong></p>
<h4 id="2-masking-技术的使用导致生成阶段出现冗余计算"><a href="#2-masking-技术的使用导致生成阶段出现冗余计算" class="headerlink" title="2. masking 技术的使用导致生成阶段出现冗余计算"></a>2. masking 技术的使用导致生成阶段出现冗余计算</h4><p>由于 masking 技术的使用，在生成当前 tokens 的输出表征时，仅使用之前已生成 tokens 的信息，而不使用之后生成的 tokens 的信息。<strong>因为之前的 tokens 在各次迭代中都是相同的，所以对于该特定 tokens 的输出表征在随后的所有迭代中也都是相同的，这就意味着存在冗余计算。</strong></p>
<p>在自回归解码步骤的新一次迭代中，使用了“What color is the sky? The sky is ”作为输入序列，在之前的步骤中唯一尚未计算的是输入序列中的最后一个token “is”的表征。</p>
<p>需要的量有:</p>
<ol>
<li>“is”的查询向量。</li>
<li>用于计算注意力分数的“What”，“ color”，“ is”，“ the”，“ sky”，“?”，“The ”，“sky ”和“is ” 的键向量。</li>
<li>用于计算输出的“What”，“ color”，“ is”，“ the”，“ sky”，“?”，“The ”，“sky ”和“is ” 的值向量。</li>
</ol>
<p>至于键（key）和值（value）向量，除了 ”is “之外，它们已经在之前的迭代中为所有 tokens 计算过了。因此，我们可以保存（即缓存）并重复使用先前迭代中的键和值向量（译者注：原文是“query vectors”，可能是作者笔误，此处译者修改为“值向量”）。这种优化简单地被称为 KV 缓存。为“is ”计算输出表征将会变得非常简单：</p>
<ol>
<li>计算“is ”的查询向量、键向量和值向量。</li>
<li>从缓存中获取“What”，“ color”，“ is”，“ the”，“ sky”，“?”，“The ” 和 “sky ”的键和值向量，并将它们与刚刚为“is ”计算的键向量和值向量连接起来。</li>
<li>使用“is ”查询向量和所有键向量计算注意力分数。</li>
<li>使用注意力分数和所有值向量计算“is ”的输出向量。</li>
</ol>
<p><strong>当我们使用 KV 缓存时，模型的实际输入是最后生成的 tokens （而非整个序列）和 KV 缓存。</strong></p>
<p><strong>KV cacge</strong>可以节省多少运算量?假设有一批输入序列，数量为b个，每个序列由N个生成的tokens和t个输入的tokens(总长度为 N+t)组成。对于这些序列的前 t+N-1 个 tokens，计算 KV 值是冗余的，也就是说，在生成步骤的第 N 步，我们可以为每个序列节省 t+N-1 次 KV 计算。如果不重新计算，那么在前 N 个生成步骤中，每个序列总共可以节省 N.t+N.(N-1)/2 次 KV 计算。</p>
<p>通过 KV 缓存节省的运算数量与生成的 tokens 数量的平方成正比。（换句话说，如果生成的 tokens 数量翻倍，通过KV缓存所节省的运算数量将变为原来的四倍。）</p>
<p>KV 缓存是一种妥协，因此并不是免费的午餐：<strong>其实是使用更多的内存消耗和数据传输来换取更少的计算量。</strong></p>
<p>与启动阶段所需的输入相比，强制执行这一缓存策略改变了注意力层在生成阶段的输入。在启动阶段，注意力层会一次性处理整个输入序列，而启用 KV 缓存的生成阶段只需要最后生成的 token 和 KV 缓存作为输入。这种启动阶段和生成阶段之间的新差异不仅仅是概念上的。例如，<strong>与在两个阶段使用相同的 GPU 内核相比，在每个阶段使用特定的 GPU 内核能带来更好的性能。</strong></p>
<h4 id="3-有效控制KV-cache的内存占用，优化推理速度"><a href="#3-有效控制KV-cache的内存占用，优化推理速度" class="headerlink" title="3. 有效控制KV cache的内存占用，优化推理速度"></a>3. 有效控制KV cache的内存占用，优化推理速度</h4><p>多头注意力（MHA）模型的 KV 缓存确实会<strong>消耗大量 GPU 内存</strong>，并且很容易增长到比模型权重还大的规模， KV 缓存大小的控制对于优化大模型的推理至关重要。</p>
<p><strong>KV 缓存阻碍了我们处理或生成超长序列（即长上下文窗口带来的挑战或障碍）和/或处理大 batches ，因此无法最大限度地提高硬件效率。</strong></p>
<p>从这个角度来看，最大化模型处理能力意味着为 KV 缓存留出尽可能多的内存空间，可以通过以下方式实现：</p>
<ul>
<li>减少模型权重的内存占用(权重量化)</li>
<li>较少KV cache 的内存占用，滑动窗口，缓存丢弃，</li>
<li>将模型分片到多个GPU上，以牺牲网络通信为代价(模型并行)或私用其他类型的存储，如CPU内存或者磁盘，从而将多个设备的内存池化。</li>
</ul>
<p>PagedAttention 没有涉及到的另一个可能的优化措施是跨请求重用键值缓存（reusing the key-value cache across requests）。当提示词（prompts）共享某一类共同的前缀时，这种优化就会适用，这种情况通常出现在聊天界面和Agent等多轮用例或使用提示词模板时（图 4）。</p>
<p><img src="/2025/02/03/transformer-not/760c80e82311464dad6e90e345f747e1.png" alt="在这里插入图片描述"></p>
<p>RadixAttention 算法在完成内容生成请求后，并非直接丢弃 KV 缓存，而是将其保留在 GPU 内存中，并向专用数据结构（radix tree）添加一个新条目，将 tokens 序列映射到其 KV 缓存张量。当新请求到达时，调度程序会使用 radix tree 进行前缀匹配。如果有缓存命中，调度程序就会重新使用缓存的 KV 张量来满足请求。</p>
<h4 id="4-模型性能瓶颈分类及优化策略"><a href="#4-模型性能瓶颈分类及优化策略" class="headerlink" title="4. 模型性能瓶颈分类及优化策略"></a>4. 模型性能瓶颈分类及优化策略</h4><p><strong>模型性能瓶颈的4种主要类型:</strong></p>
<ul>
<li>计算能力受限情况。在该情况下，大部分时间（即延迟）都耗费在执行计算操作上。</li>
<li>内存带宽受限情况。在这种情况下，大部分时间都用于在嵌入于处理器芯片上的内存和处理器之间移动数据。</li>
<li>通信受限情况。仅适用于计算和数据分布在多个芯片上的情况。大部分任务处理时间被芯片间的网络数据传输占用。</li>
<li>开销受限情况。是软件导致的瓶颈。</li>
</ul>
<p>通常情况下，不可能所有内核都运行在同一种机制下。因此，<strong>关键在于识别出大部分时间都耗费在哪一种机制上。</strong> 然后，我们应该<strong>优先优化这一主要的性能瓶颈，再找出下一个影响最大的性能瓶颈，如此循环往复。</strong></p>
]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>notes</tag>
      </tags>
  </entry>
  <entry>
    <title>ubuntu_git</title>
    <url>/2025/02/03/ubuntu-git/</url>
    <content><![CDATA[<p>ubuntu 下git常用指令，后续遇到新的指令会持续更新</p>
<span id="more"></span>
<h4 id="ubuntu-下git常用指令【持续更新中】"><a href="#ubuntu-下git常用指令【持续更新中】" class="headerlink" title="ubuntu 下git常用指令【持续更新中】"></a><center>ubuntu 下git常用指令【持续更新中】</center></h4><h5 id="1-下载"><a href="#1-下载" class="headerlink" title="1.下载"></a>1.下载</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo apt install git		</span><br></pre></td></tr></table></figure>
<h5 id="2-查看版本"><a href="#2-查看版本" class="headerlink" title="2. 查看版本"></a>2. 查看版本</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git --version</span><br></pre></td></tr></table></figure>
<h5 id="3-登录git账号"><a href="#3-登录git账号" class="headerlink" title="3. 登录git账号"></a>3. 登录git账号</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git config --global user.email &quot;you@example.com&quot;</span><br><span class="line">git config --global user.name &quot;Your Name&quot;</span><br></pre></td></tr></table></figure>
<h5 id="4-生成密钥对"><a href="#4-生成密钥对" class="headerlink" title="4.生成密钥对"></a>4.生成密钥对</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa -C &quot;your_email@youremail.com&quot;</span><br></pre></td></tr></table></figure>
<p>复制公钥</p>
<h5 id="5-复制公钥到github"><a href="#5-复制公钥到github" class="headerlink" title="5. 复制公钥到github"></a>5. 复制公钥到github</h5><p><img src="/2025/02/03/ubuntu-git/91800e522e0020909564e868640d31a5.png" alt="请添加图片描述"></p>
]]></content>
      <categories>
        <category>配置</category>
      </categories>
      <tags>
        <tag>configure</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>ubuntu创建新用户，添加用户权限，删除用户</title>
    <url>/2025/02/03/ubuntu-opts/</url>
    <content><![CDATA[<p>ubuntu创建新用户，添加用户权限，删除用户</p>
<span id="more"></span>
<p><strong>进入root用户</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">su root</span><br></pre></td></tr></table></figure>
<p><strong>创建新用户</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo useradd -r -m -s /bin/bash abcd #abcd是用户名称</span><br></pre></td></tr></table></figure>
<p><strong>设置密码</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo passwd abcd</span><br></pre></td></tr></table></figure>
<p>来设置新用户的密码。</p>
<p>其中参数的意义如下：<br>-r：建立系统账号<br>-m：自动建立用户的登入目录<br>-s：指定用户登入后所使用的shell</p>
<p><strong>修改用户权限</strong></p>
<p>修改 /etc/sudoers 文件，该文件为只读文件，所以先修改为w权限，再改写，再改回r权限。</p>
<p>修改文件权限</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo chmod +w /etc/sudoers</span><br></pre></td></tr></table></figure>
<p>改写文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim /etc/sudoers</span><br></pre></td></tr></table></figure>
<p>在文件最后一行加入下列语句<br>有关 vim  的操作可参考 ：<a href="https://blog.csdn.net/weixin_46091520/article/details/138215969">ubuntu 下 vim 的使用-CSDN博客</a></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">用户名(这里是abcd)  ALL=(ALL:ALL) ALL</span><br></pre></td></tr></table></figure>
<p>改回文件权限：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo chmod -w /etc/sudoers</span><br></pre></td></tr></table></figure>
<p><strong>删除用户</strong></p>
<p>分三步：</p>
<ol>
<li>删除用户</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo userdel 用户名(这里是abcd)</span><br></pre></td></tr></table></figure>
<ol>
<li>删除用户目录</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo rm -rf /home/用户名(这里是abcd)</span><br></pre></td></tr></table></figure>
<ol>
<li>删除用户权限相关配置：删除或者注释掉/etc/sudoers中关于要删除用户的配置，否则无法再次创建同名用户</li>
</ol>
]]></content>
      <categories>
        <category>配置</category>
      </categories>
      <tags>
        <tag>configure</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>vscode 使用code runner 运行代码输出乱码原因及解决办法</title>
    <url>/2025/02/03/vscode-codeRunner/</url>
    <content><![CDATA[<p>解决vscode使用code runner 乱码的问题，主要保持代码的编码方式和终端编码方式一致。可以通过设置终端默认编码方式或代码文件默认编码方式解决。</p>
<span id="more"></span>
<h3 id="vscode-使用code-runner-运行代码输出乱码"><a href="#vscode-使用code-runner-运行代码输出乱码" class="headerlink" title="vscode 使用code runner 运行代码输出乱码"></a><center>vscode 使用code runner 运行代码输出乱码</center></h3><h4 id="问题所在："><a href="#问题所在：" class="headerlink" title="问题所在："></a>问题所在：</h4><p><strong>代码文件使用的编码格式</strong>和<strong>终端使用的编码格式不一致</strong>，查看代码文件右下角，会显示代码文件的编码格式。</p>
<p>测试代码如下：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    cout&lt;&lt;<span class="string">&quot;你好！&quot;</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>此时输出乱码：</p>
<p> <img src="/2025/02/03/vscode-codeRunner/484d36f4669b9f38c499083d5143ca98.png" alt="请添加图片描述"></p>
<p>查看右下角的代码文件编码格式，显示为 <code>utf-8</code><br><img src="/2025/02/03/vscode-codeRunner/cbe03b81b7f64a96dc12c1d89acaf026.png" alt="请添加图片描述"></p>
<p>终端输入<code>chcp</code>, 显示当前终端编码为 <code>936</code>，即为<code>gbk</code>。<code>utf-8</code>为<code>65001</code>。<br><img src="/2025/02/03/vscode-codeRunner/94714c7fe347e014f04d4f257293dcf5.png" alt="请添加图片描述"></p>
<h4 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h4><p>可以点击文件右下角，通过编码保存，选择<code>gbk</code>保存；也可以在终端输入<code>chcp 65001</code>将终端编码改为<code>utf-8</code>。这里选择第二种，输入<code>chcp 65001</code> 则代码和终端编码都为<code>utf-8</code>。</p>
<p>重新运行，正常输出：<br> <img src="/2025/02/03/vscode-codeRunner/2241e55ddc5800fb6022e892c3eb9553.png" alt="请添加图片描述"></p>
<p>但这种方式只是暂时的，如果终端重新打开，则又得重新改。可以在<code>settings.json</code>中配置终端信息。</p>
<p>点击扩张-&gt;设置-&gt;在settings.json中设置。</p>
<p><img src="/2025/02/03/vscode-codeRunner/217f6db2953a80c63dbacb8a046b4f54.png" alt="请添加图片描述"></p>
<p>在<code>&#123;&#125;</code>中输入以下代码</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 设置终端的参数，编码格式等</span></span><br><span class="line"><span class="string">&quot;terminal.integrated.profiles.windows&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;Command Prompt&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;path&quot;</span>: <span class="string">&quot;C:\\Windows\\System32\\cmd.exe&quot;</span>,</span><br><span class="line">        <span class="string">&quot;args&quot;</span>: [<span class="string">&quot;-NoExit&quot;</span>, <span class="string">&quot;/K&quot;</span>, <span class="string">&quot;chcp 65001&quot;</span>]</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">&quot;PowerShell&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;source&quot;</span>: <span class="string">&quot;PowerShell&quot;</span>,</span><br><span class="line">        <span class="string">&quot;args&quot;</span>: [<span class="string">&quot;-NoExit&quot;</span>, <span class="string">&quot;/C&quot;</span>, <span class="string">&quot;chcp 65001&quot;</span>]</span><br><span class="line">    &#125;</span><br><span class="line">&#125;,</span><br><span class="line"><span class="string">&quot;terminal.integrated.defaultProfile.windows&quot;</span>: <span class="string">&quot;PowerShell&quot;</span>,   <span class="comment">//使用PowerShell 或者Command Prompt作为终端</span></span><br></pre></td></tr></table></figure>
<p><img src="/2025/02/03/vscode-codeRunner/2f59fab9df71d38d711c86059862222c.png" alt="请添加图片描述"></p>
<p>注意前后的<code>,</code>否则会影响其他设置。现在，每次打开终端都会设置编码为UTF-8</p>
<p><img src="/2025/02/03/vscode-codeRunner/4af924d125c0be037bd0137122579235.png" alt="请添加图片描述"></p>
]]></content>
  </entry>
  <entry>
    <title>【论文阅读】Predictive Scheduling for GPU Based Cloud Servers Using Machine Learning</title>
    <url>/2025/02/03/scheduler-4/</url>
    <content><![CDATA[<p>提出一个干扰感知调度器Mystic，用于在基于GPU 的集群和云服务器上高效地协同执行应用程序。Mystic识别新应用程序和正在执行的应用程序之间的相似之处，并指导调度器将干扰最小化并提高系统吞吐量。</p>
<span id="more"></span>
<h4 id="7-Mystic-Predictive-Scheduling-for-GPU-Based-Cloud-Servers-Using-Machine-Learning"><a href="#7-Mystic-Predictive-Scheduling-for-GPU-Based-Cloud-Servers-Using-Machine-Learning" class="headerlink" title="7.Mystic: Predictive Scheduling for GPU Based Cloud Servers Using Machine Learning"></a>7.Mystic: Predictive Scheduling for GPU Based Cloud Servers Using Machine Learning</h4><ul>
<li><p>出处：2016IEEE IPDPS  <a href="https://ieeexplore.ieee.org/document/7516031">使用机器学习的基于GPU的云服务器的预测调度</a></p>
</li>
<li><p>背景：</p>
<ul>
<li>在同一GPU上共同执行的应用程序之间的资源争用会产生干扰，从而导致性能下降，影响应用程序的QoS要求，降低系统整体吞吐率。现有解决方案要么是为了CPU集群开发的，要么是使用静态分析方法，这可能是计算密集型的，并且不能很好地扩展。</li>
</ul>
</li>
<li><p>主要工作：提出一个干扰感知调度器Mystic，用于在基于GPU 的集群和云服务器上高效地协同执行应用程序。Mystic识别新应用程序和正在执行的应用程序之间的相似之处，并指导调度器将干扰最小化并提高系统吞吐量。</p>
<ul>
<li>使用<strong>协同过滤</strong>来识别新任务和其他正在执行的任务的相似性。协同过滤广泛应用于推荐系统，通过分析用户的购买历史，并根据用户的兴趣提供个性化的推荐。协同过滤中有两种常用方法——基于邻域的方法和潜在因素模型。</li>
<li>争用的影响：多上下文GPU上的资源争用可能导致多个协同执行的应用程序之间的干扰，如流多处理器SMS、内存资源、纹理缓存、全局DRAM和互联网络。</li>
</ul>
</li>
<li><p>模型：</p>
<ul>
<li><p>Stage I: 初始化器和配置文件生成器。</p>
<ul>
<li>初始化程序：查询集群状态表MAST，获取每个计算节点的IP地址、CPU核数、GPU核数和系统内存量，同时为每个传入的应用程序创建状态项。</li>
<li>Mysitc为每个传入的应用程序启动连个简短的分析，已获得两个随机选择的COI(干扰的原因：流多处理器SMS、内存资源、纹理缓存、全局DRAM和互联网络。)，并存在PIT(概要信息表)中。PIT用PID(进程ID)索引。</li>
<li>TRM(评级矩阵)：维护离线分析阶段的几个应用程序的完整配置文件。</li>
</ul>
</li>
<li><p>Stage II: 基于协同过滤(CF)的预测</p>
<ul>
<li>CF将PIT和TRM作为输入。首先根据PID返回一个稀疏的PIT向量v(不包含所有COI信息)并附加到TRM上，然后执行基于svd的协同过滤填充TRM中所有缺失的值(只有一行缺少值),补全后将该行添加到PRT(预测表)中。知道调度。</li>
</ul>
</li>
<li><p>Stage III: 干扰感知的调度程序</p>
<p>以MAST和PRT作为输入，生成应用程序对之间的相似度，决定传入的程序A0是否不会与同一GPU上正在执行的应用程序A1并发执行。首先检查是否有空闲GPU，如果有，则分配到最靠近头节点的GPU，如果没有，获取正在执行的PRT条目，干扰分数并分配到干扰分数最低的GPU。</p>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
        <category>调度</category>
      </categories>
      <tags>
        <tag>scheduler</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文阅读】MICCO:An Enhanced Multi-GPU Scheduling Framework for Many-Body Correlation Functions</title>
    <url>/2025/02/03/scheduler-3/</url>
    <content><![CDATA[<p>首先对数据重用和负载平衡的相互作用进行了全面的研究，并提出了局部重用模式和重用边界两个新概念，研究两者之间实现最佳权衡的机会。在此基础上，MICCO提出了一种启发式调度算法和一种基于机器学习的回归模型来生成重用边界的最优设置。</p>
<span id="more"></span>
<h4 id="MICCO-An-Enhanced-Multi-GPU-Scheduling-Framework-for-Many-Body-Correlation-Functions"><a href="#MICCO-An-Enhanced-Multi-GPU-Scheduling-Framework-for-Many-Body-Correlation-Functions" class="headerlink" title="MICCO: An Enhanced Multi-GPU Scheduling Framework for Many-Body Correlation Functions"></a>MICCO: An Enhanced Multi-GPU Scheduling Framework for Many-Body Correlation Functions</h4><ul>
<li><p>出处：2022IEEE    <a href="https://ieeexplore.ieee.org/document/9820666">MICCO:多体关联函数的增强型多GPU调度框架</a></p>
</li>
<li><p>主要工作：首先对数据重用和负载平衡的相互作用进行了全面的研究，并提出了局部重用模式和重用边界两个新概念，研究两者之间实现最佳权衡的机会。在此基础上，MICCO提出了一种启发式调度算法和一种基于机器学习的回归模型来生成重用边界的最优设置。</p>
</li>
<li><p>动机：</p>
<ul>
<li>多体相关函数是计算和内存密集型，现有的基于图的多GPU调度器无法捕获这些以数据为中心的特征，从而导致多体相关函数计算性能不佳。</li>
<li>数据重用和负载均衡形成了一种权衡关系。</li>
</ul>
</li>
<li><p>影响负载均衡和数据重用均衡的因素：多体相关函数计算的执行主要包括三个部分：内核计算、内存分配和数据通信，后两者称为内存操作。数据重用主要降低内存操作成本。</p>
<ul>
<li>局部重用的影响：如果调度程序能够捕获所有的数据重用，并以最佳的数据重用-负载均衡组合为目标进行穷举搜索，则可以找到最优的任务调度方案。但是实际存在两个问题：许多情况下可能全局信息不可用(特别是动态生成收缩图时)；搜索空间太大，NP难问题。本文提出<strong>利用局部重用模式信息动态搜索局部最优调度方案</strong>。</li>
<li>重用边界：数据重用时允许的负载不均衡的级别。</li>
</ul>
</li>
<li><p>多GPU调度框架</p>
<p>MICCO的设计侧重于这些方面：</p>
<ul>
<li>探索重复张量的数据重用机会。</li>
<li>改善负载均衡以保持GPU繁忙。</li>
<li>在考虑内存调出的情况下实现最佳的数据重用-负载均衡权衡。</li>
</ul>
<p>主要由两部分组成</p>
<p> <img src="/2025/02/03/scheduler-3/7e5dfcc212b47c4b6a4ea85225cc1abf.png" alt="在这里插入图片描述"></p>
</li>
</ul>
<p>  MICCO将每个向量的数据特征提取到回归模型中(①)。回归模型生成最优复用边界(②)。启发式算法对张量对进行分类(③)，共同管理三条策略</p>
<ul>
<li>启发式调度算法</li>
<li>预训练的轻量级回归模型</li>
</ul>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
        <category>调度</category>
      </categories>
      <tags>
        <tag>scheduler</tag>
      </tags>
  </entry>
</search>
