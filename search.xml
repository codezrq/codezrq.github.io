<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Docker安装以及Docker常用命令</title>
    <url>/2025/02/03/Docker-install/</url>
    <content><![CDATA[<p>Linux下的Docker安装过程，包括Docker的安装，卸载，更换软件源，配置用户组，已经Docker使用过程中的常见命令。</p>
<span id="more"></span>
<h3 id="Ubuntu-下-Docker-安装以及常见操作"><a href="#Ubuntu-下-Docker-安装以及常见操作" class="headerlink" title="Ubuntu 下 Docker 安装以及常见操作"></a><center>Ubuntu 下 Docker 安装以及常见操作</center></h3><h4 id="一-Docker-安装"><a href="#一-Docker-安装" class="headerlink" title="一. Docker 安装"></a>一. Docker 安装</h4><h5 id="1-卸载旧Docker"><a href="#1-卸载旧Docker" class="headerlink" title="1. 卸载旧Docker"></a>1. 卸载旧Docker</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo apt-get remove docker docker-engine docker.io containerd runc</span><br></pre></td></tr></table></figure>
<h5 id="2-安装docker依赖"><a href="#2-安装docker依赖" class="headerlink" title="2. 安装docker依赖"></a>2. 安装docker依赖</h5><p>Docker在Ubuntu上依赖一些软件包。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo apt update sudo apt upgrade</span><br><span class="line">sudo apt-get install ca-certificates curl gnupg lsb-release</span><br></pre></td></tr></table></figure>
<h5 id="3-添加秘钥"><a href="#3-添加秘钥" class="headerlink" title="3. 添加秘钥"></a>3. 添加秘钥</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">curl -fsSL http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -</span><br></pre></td></tr></table></figure>
<h5 id="4-添加软件源"><a href="#4-添加软件源" class="headerlink" title="4. 添加软件源"></a>4. 添加软件源</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository &quot;deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable&quot;</span><br></pre></td></tr></table></figure>
<h5 id="5-安装Docker"><a href="#5-安装Docker" class="headerlink" title="5. 安装Docker"></a>5. 安装Docker</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">apt-get install docker-ce docker-ce-cli containerd.io</span><br></pre></td></tr></table></figure>
<h4 id="二-配置用户组"><a href="#二-配置用户组" class="headerlink" title="二. 配置用户组"></a>二. 配置用户组</h4><p>默认情况下，只有root用户和docker组的用户才能运行Docker命令。我们可以将当前用户添加到docker组，以避免每次使用Docker时都需要使用sudo。</p>
<p>如果出现“启动“docker.service”需要认证。Multiple identities can be used for authentication:”的报错，说明是没有将当前用户加入到docker用户组中。</p>
<p><strong>创建docker组</strong>(如果已经有，则不用创建)</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo groupadd docker</span><br></pre></td></tr></table></figure>
<p><strong>将用户加入用户组</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo usermod -aG docker $USER</span><br></pre></td></tr></table></figure>
<p><strong>重新登陆</strong></p>
<p><strong>刷新用户组</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">newgrp docker</span><br></pre></td></tr></table></figure>
<p>可以通过一下命令查看存在的用户组</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">newgrp docker</span><br></pre></td></tr></table></figure>
<p><strong>测试能否使用docker:</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker run hello-world</span><br></pre></td></tr></table></figure>
<h4 id="三-常用命令"><a href="#三-常用命令" class="headerlink" title="三.常用命令"></a>三.常用命令</h4><h5 id="1-基础命令"><a href="#1-基础命令" class="headerlink" title="1. 基础命令"></a>1. 基础命令</h5><p><strong>查看docker版本信息</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker version</span><br><span class="line">docker info</span><br></pre></td></tr></table></figure>
<p><strong>启动 docker</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl start docker</span><br></pre></td></tr></table></figure>
<p><strong>关闭 docker</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl stop docker</span><br></pre></td></tr></table></figure>
<p><strong>重启 docker</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl restart docker</span><br></pre></td></tr></table></figure>
<p><strong>设置docker随服务启动而启动</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl enable docker</span><br></pre></td></tr></table></figure>
<p><strong>查看docker运行状态</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl status docker</span><br></pre></td></tr></table></figure>
<p>如果在运行中，输入命令后会看到绿色的 active(running)</p>
<h5 id="2-镜像命令"><a href="#2-镜像命令" class="headerlink" title="2. 镜像命令"></a>2. 镜像命令</h5><p><strong>查看镜像列表</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker images</span><br></pre></td></tr></table></figure>
<p><strong>搜索镜像</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker search 镜像名</span><br><span class="line">docker search --filter=STARS=9000 mysql 搜索 STARS &gt;9000的 mysql 镜像</span><br></pre></td></tr></table></figure>
<p><strong>拉取镜像</strong></p>
<p><strong>拉取镜像</strong> 不加tag(版本号) 即拉取docker仓库中该镜像的最新版本latest，加:tag则是拉取指定版本</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker pull 镜像名 </span><br><span class="line">docker pull 镜像名:tag</span><br></pre></td></tr></table></figure>
<p><strong>运行镜像</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker run 镜像名</span><br><span class="line">docker run 镜像名:Tag</span><br></pre></td></tr></table></figure>
<h5 id="3-容器命令"><a href="#3-容器命令" class="headerlink" title="3. 容器命令"></a>3. 容器命令</h5><p><strong>查看运行中的容器</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker ps</span><br></pre></td></tr></table></figure>
<p><strong>查看所有容器</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker ps -a</span><br></pre></td></tr></table></figure>
<p><strong>启动容器</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker start 容器id或容器名</span><br></pre></td></tr></table></figure>
<p><strong>停止容器</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker stop 容器id或容器名</span><br></pre></td></tr></table></figure>
<p><strong>查看容器的所有信息</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker inspect 容器id</span><br></pre></td></tr></table></figure>
<p><strong>查看容器日志</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">doker container logs 容器id</span><br></pre></td></tr></table></figure>
<p><strong>查看容器里的进程</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker top 容器id</span><br></pre></td></tr></table></figure>
<p><strong>退出容器</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">exit</span><br></pre></td></tr></table></figure>
<p><strong>删除已停止的容器</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker rm 容器id或name</span><br></pre></td></tr></table></figure>
<p><strong>删除正在运行的容器</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker rm -f 容器id</span><br></pre></td></tr></table></figure>
<p><strong>进入容器</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker exec -it 容器ID sh</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>配置</category>
      </categories>
      <tags>
        <tag>configure</tag>
      </tags>
  </entry>
  <entry>
    <title>GridWord</title>
    <url>/2025/02/03/GridWord/</url>
    <content><![CDATA[<p>使用Q-Learning 和Sara算法解决GridWorld炸弹环境，分为两个类：gridWorld.py和Agent.py:<br>环境类：继承gym.Wrapper，主要实现了render（显示每次的地图）。step（和环境交互，计算奖励值）<br>Agent类：包括两种算法，主要实现了learn（学习方法，每次更新Q-table）predict（根据输入的观察值，预测输出的动作）。sample(根据输入的观察值，采样输入的动作)<br>整体步骤为，首先根据grdiWordl创建出环境，每次机器人根据环境选择动作并更新。</p>
<span id="more"></span>
<h3 id="使用Q-Learning-和-Sara-解决GridWorld-炸弹环境"><a href="#使用Q-Learning-和-Sara-解决GridWorld-炸弹环境" class="headerlink" title="使用Q-Learning 和 Sara 解决GridWorld 炸弹环境"></a><center>使用Q-Learning 和 Sara 解决GridWorld 炸弹环境</center></h3><h4 id="代码链接"><a href="#代码链接" class="headerlink" title="代码链接"></a>代码链接</h4><p><a href="https://download.csdn.net/download/weixin_46091520/88788389">代码连接</a></p>
<h4 id="一-实验原理"><a href="#一-实验原理" class="headerlink" title="一.实验原理"></a>一.实验原理</h4><h5 id="1-1-Q-learning-和-Sara-的异同"><a href="#1-1-Q-learning-和-Sara-的异同" class="headerlink" title="1.1 Q-learning 和 Sara 的异同"></a>1.1 Q-learning 和 Sara 的异同</h5><h6 id="1-1-1-相似之处"><a href="#1-1-1-相似之处" class="headerlink" title="1.1.1 相似之处"></a>1.1.1 相似之处</h6><ol>
<li>两种算法本质都是通过策略迭代得到最优策略。</li>
<li>两种算法都是基于时序差分法进行更新，可以看作蒙特卡洛仿真和动态规划的结合。</li>
<li>在选择策略时，都使用 $\epsilon - greedy$ 算法，即以$\epsilon$ 的概率选择使得动作-值函数最大的动作，以$1-\epsilon$的概率随机选择。</li>
</ol>
<h6 id="1-1-2-不同之处"><a href="#1-1-2-不同之处" class="headerlink" title="1.1.2 不同之处"></a>1.1.2 不同之处</h6><p>Q-Learning是强化学习算法中value-based的算法，Q即为Q（s,a）就是在某一时刻的 s 状态下(s∈S)，采取 动作a (a∈A)动作能够获得收益的期望，环境会根据agent的动作反馈相应的回报reward r，所以算法的主要思想就是将State与Action构建成一张Q-table来存储Q值，然后根据Q值来选取能够获得最大的收益的动作。<br>Q-learing 算法可用如下伪代码表示：</p>
<p><img src="/2025/02/03/GridWord/04e5b899e940ca476855b8887e213762.jpeg" alt="在这里插入图片描述"></p>
<pre><code>Sara和Q-Learning基本一致，可用如下伪代码表示：
</code></pre><p> <img src="/2025/02/03/GridWord/793e378b13782198478fda89df29b6c6.jpeg" alt="在这里插入图片描述"></p>
<p>从两个算法的伪代码可以看出，两者的最大区别在于Q-table的更新方式不同：</p>
<p>Q-Learning更新Q值的公式为：</p>
<p>$Q(S_t,A_t) \leftarrow Q(S_t, A_t) + \alpha[R_{t+1}+\gamma \underset{a}{max}Q(S_{t+1},a)-Q(S_t,A_t)]$</p>
<p>Sara更新Q值的公式为：</p>
<p>$Q(S_t,A_t)\leftarrow Q(S_t,A_t)+\alpha[R_{t+1}+\gamma Q(S_{t+1},A_{t+1})-Q(S_t,A_t)]$</p>
<ol>
<li><strong>Q-learning：</strong>在状态$S_t$下，根据 $\epsilon-greedy$策略选择动作$A_{t}$ 到达$S_{t+1}$后，利用状态$S_{t+1}$下的最佳Q值$Q(S_{t+1},a)$来更新$Q(S_t,A_{t})$，但并不真正采取动作$(S_{t+1},a)$ 。更新Q-table用到的值有$<S_t,A_t,reward,S_{t+1}>$</S_t,A_t,reward,S_{t+1}></li>
<li><strong>Sara:</strong> 在状态$S_t$下,根据 $\epsilon-greedy$ 策略选择动作$A_t$到达$S_{t+1}$之后，选择最大的$(S_{t+1},a)$并真正采取该动作。更新Q-table用到的值有$<S_t,A_t,reward,S_{t+1},A_{t+1}>$ </S_t,A_t,reward,S_{t+1},A_{t+1}></li>
<li>Q−learning选取动作和更新Q表值的方法不同，而Sarsa选取动作和更新Q表值的方法相同。Q-Learning算法，先假设下一步选取最大奖赏的动作，更新值函数。然后再通过ε-greedy策略选择动作。Sarsa算法，先通过ε-greedy策略执行动作，然后根据所执行的动作，更新值函数。</li>
<li>可以看出Q-Learning使用的更新方法更激进，即直接选择下一个状态下的最大值进行更新。而Sara算法更保守，基于现有的步骤进行更新，整体上来说Sara更偏向于避免陷阱。</li>
</ol>
<h5 id="1-2-算法图解"><a href="#1-2-算法图解" class="headerlink" title="1.2 算法图解"></a>1.2 算法图解</h5><p>两种算法的基本流程出了训练过程中更新参数的方法不同，其余流程相同。可用下图表示：</p>
<p><img src="/2025/02/03/GridWord/ba98fb145409026de633a655bc35b888.png" alt="在这里插入图片描述"></p>
<p>​                                                              </p>
<h4 id="二-算法实现"><a href="#二-算法实现" class="headerlink" title="二.算法实现"></a>二.算法实现</h4><p>整体分为环境类和代码类。</p>
<h5 id="2-1-环境"><a href="#2-1-环境" class="headerlink" title="2.1 环境"></a>2.1 环境</h5><p>定义类<code>FronzenLakeWapper(gym.Wrapper)</code>，主要实现以下接口：</p>
<p><code>draw_box</code>: 绘制一个坐标处的矩形框，并做以下填充：</p>
<ul>
<li><p>起点：红色</p>
</li>
<li><p>出口：黄色</p>
</li>
<li>炸弹：黑色</li>
<li>平地：白色</li>
</ul>
<p><code>move_player(self, x, y)</code>:将智能体移动到对应的坐标</p>
<p><code>render(self)</code>:渲染一帧图像</p>
<p><code>step(self,action)</code>:根据传入的动作，计算智能体的新坐标，以及对应的返回值。为了训练智能体避免炸弹并且尽量减少路径长度，将奖励值设置如下：</p>
<ul>
<li>起点或空地: <code>reward = -2</code></li>
<li>炸弹：<code>reward = -20</code></li>
<li>终点：<code>reward = 10</code></li>
</ul>
<h5 id="2-2-智能体"><a href="#2-2-智能体" class="headerlink" title="2.2 智能体"></a>2.2 智能体</h5><p>根据使用的算法不同，分别创建类<code>QLearningAgent(object)</code> 和 <code>SaraAgent(object)</code> 。</p>
<p>两个类有以下相同接口：</p>
<p><code>sample(self, obs)</code>：根据输入的观察值，使用$\epsilon-greedy$ 策略选择动作。</p>
<p><code>predict(self, obs)</code>: 根据输入的观察值，预测输出的动作值。</p>
<p><code>save(self, npy_file)</code>: 将Q表保存到文件中。</p>
<p><code>restore(self, npy_file)</code>: 从文件中读取Q表数据。</p>
<p>根据Q表更新公式的不同，实现不同的学习函数。</p>
<p><code>QLearningAgent.learn(self,obs,action,next_obs,reward,done)</code>:根据当前状态和动作以及下个状态更新Q表。</p>
<p><code>QLearningAgent.learn(self,obs,action,next_obs,next_action,reward,done)</code>:根据当前状态和动作以及下个状态和下个动作更新Q表。</p>
<h4 id="三-实验结果及分析"><a href="#三-实验结果及分析" class="headerlink" title="三.实验结果及分析"></a>三.实验结果及分析</h4><h5 id="3-1-输入"><a href="#3-1-输入" class="headerlink" title="3.1 输入"></a>3.1 输入</h5><p>使用文件输入，在<code>input.txt</code>中输入矩阵，例如下图，输入一个$7\times 17$ 的矩阵，其中S表示起点，F表示空地，H表示炸弹，G表示出口。</p>
<p>   <img src="/2025/02/03/GridWord/273454ab8ab09281e8b7c7f0bb4a075d.png" alt="在这里插入图片描述"></p>
<h5 id="3-2-可视化环境及移动轨迹"><a href="#3-2-可视化环境及移动轨迹" class="headerlink" title="3.2 可视化环境及移动轨迹"></a>3.2 可视化环境及移动轨迹</h5><p>运行结果，使用乌龟模拟机器人，从起点出发到达终点需要六步，分别如下：</p>
<p><img src="/2025/02/03/GridWord/0f6386cdc0acc2073eca6a7830036471.png" alt="在这里插入图片描述"></p>
<h5 id="3-3-学习参数对策略收敛的影响"><a href="#3-3-学习参数对策略收敛的影响" class="headerlink" title="3.3 学习参数对策略收敛的影响"></a>3.3 学习参数对策略收敛的影响</h5><p>训练过程中，当连续五局游戏都成功且总奖励值不变时，认为模型已经收敛</p>
<h6 id="3-3-1-Q-Learning-算法"><a href="#3-3-1-Q-Learning-算法" class="headerlink" title="3.3.1 Q-Learning 算法"></a>3.3.1 Q-Learning 算法</h6><p>模型的收敛速度随着回报衰减系数变化如下图：</p>
<p> <img src="/2025/02/03/GridWord/a9f0ce1ce2c8062bdf3f4fc4ce18a2c4.png" alt="在这里插入图片描述"></p>
<p>从图中可以看出，随着gamma值的增大，模型收敛速度越来越快，从Q-Learning的Q表更新公式可以看出，gamma值越大，更新程度越大，所需的训练次数也越小。</p>
<p>对各个gmma值下的收敛模型进行1000次测试，所得的成功率和平均步数如下：</p>
<p>从图中可以看出，当gamma=0.6时，模型的成功率最高并且平均步数最少。原因可能是：gamma值教小时，无法充分学习每步的未来收益，而gamma值过大时，模型采取的策略过于激进，可能出现过拟合。</p>
<h6 id="3-3-2-Sara-算法"><a href="#3-3-2-Sara-算法" class="headerlink" title="3.3.2 Sara 算法"></a>3.3.2 Sara 算法</h6><p>模型的收敛速度随着回报衰减系数变化如下图：</p>
<p> <img src="/2025/02/03/GridWord/47ec8bf94b00d507ca26c5e727c9bbc1.png" alt="在这里插入图片描述"></p>
<p>可以看出，整体来说，随着gamma值的变大，模型收敛所需的训练次数逐渐减少。但gamma从0.1变为0.2时，训练次数显著增加，可能是gamma=0.1时模型陷入局部最优。</p>
<p>对各个gmma值下的收敛模型进行1000次测试，所得的成功率和平均步数如下：</p>
<p> <img src="/2025/02/03/GridWord/104970e1031dc61755d4695e9ed002f9.png" alt="在这里插入图片描述"></p>
<p>从图中可以看出，当 gamma=0.7时，模型的成功率最高并且平均步数最少，原因可能是取一个适中的gamma值更能平衡当前收益和未来收益。同时可以看出，当gamma值由0.1变为0.2时，平均步数显著减少，可能是gamma=0.1时模型陷入局部最优。</p>
<h5 id="3-4-探究模型鲁棒性"><a href="#3-4-探究模型鲁棒性" class="headerlink" title="3.4 探究模型鲁棒性"></a>3.4 探究模型鲁棒性</h5><p>选取五个规模的地图，分别如下：</p>
<p><img src="/2025/02/03/GridWord/1b4a2076b2ecb3fa7d1d37c02d97f182.png" alt="在这里插入图片描述"></p>
<p>分别统计训练到收敛所需回合数以及成功率和平均步数(测试1000回合)：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>地图规模</th>
<th>收敛所需回合数</th>
<th>成功率</th>
<th>平均步数</th>
</tr>
</thead>
<tbody>
<tr>
<td>$2\times2$</td>
<td>44</td>
<td>97.2%</td>
<td>2.17</td>
</tr>
<tr>
<td>$4\times4$</td>
<td>48</td>
<td>91.5%</td>
<td>2.22</td>
</tr>
<tr>
<td>$6\times6$</td>
<td>403</td>
<td>93%</td>
<td>3.50</td>
</tr>
<tr>
<td>$8\times8$</td>
<td>115</td>
<td>96.4%</td>
<td>3.32</td>
</tr>
<tr>
<td>$10\times10$</td>
<td>998</td>
<td>91.6%</td>
<td>6.50</td>
</tr>
</tbody>
</table>
</div>
<p>从结果可以看出，随着地图的变大，均能保持较高的准确率，说明模型鲁棒性较高。</p>
<h5 id="3-5-可视化Q表"><a href="#3-5-可视化Q表" class="headerlink" title="3.5 可视化Q表"></a>3.5 可视化Q表</h5><p>从3.3可以看出，Q-learning算法效果较好，下面以Q-learning算法为例，当gamma=0.6时，每训练500个episode输出一次Q表中间结果：</p>
<p><img src="/2025/02/03/GridWord/b20f1709936a58223cc3b109b3028859.png" alt="在这里插入图片描述"></p>
<p>可以看出Q表中部分坐标的值始终为0（这些点包括炸弹，出口，以及距离出口较远的点），同时可以看出episode=1500时的对应Q表和episode=2000时对应的Q表几乎没有变化，这也和3.3.1中”当gamma=0.6时，模型训练1136个episode达到收敛“相符合。</p>
]]></content>
      <categories>
        <category>作业</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Mechine learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux下vim的使用</title>
    <url>/2025/02/03/Linux%E4%B8%8Bvim%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<p>ubuntu 下 vim 的使用。包括下载命令，打开文件命令，修改文件命令。</p>
<span id="more"></span>
<h3 id="ubuntu-下-vim-的使用"><a href="#ubuntu-下-vim-的使用" class="headerlink" title="ubuntu 下 vim 的使用"></a><center>ubuntu 下 vim 的使用</center></h3><h4 id="1-vim-下载"><a href="#1-vim-下载" class="headerlink" title="1. vim 下载"></a>1. vim 下载</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo apt-get install vim</span><br></pre></td></tr></table></figure>
<h4 id="2-vim-使用命令"><a href="#2-vim-使用命令" class="headerlink" title="2. vim 使用命令"></a>2. vim 使用命令</h4><p>打开文件<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo vim /etc/apt/sources.list</span><br><span class="line">//格式为sudo vim 文件位置（精确到文件名称）</span><br></pre></td></tr></table></figure></p>
<p>1.命令模式<br><code>i</code>   切换到输入模式，左下角出现–输入–<br><code>x</code>    删除当前光标所在处字符</p>
<p>2.输入模式<br>删除、换行、上下移动翻页、退格、输入等和平时输入一样。<br><code>esc</code>    退出输入模式</p>
<p>3.底线命令模式</p>
<p>修改完后，按 <code>esc</code> 后，输入<br><code>:wq</code>  退出程序并保存文件<br><code>:w</code> 保存文件<br><code>:q</code> 退出程序</p>
<p>如果加上 <code>!</code>，则表示强制命令，(<code>:!wq</code>，<code>:!w</code>，<code>:!q</code>)</p>
]]></content>
      <categories>
        <category>配置</category>
      </categories>
      <tags>
        <tag>configure</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Anaconda 常见命令</title>
    <url>/2025/02/03/anaconda-opts/</url>
    <content><![CDATA[<p>conda 常见命令，包括环境管理(创建，激活，退出，删除，导出配置)、包管理（查看、安装、更新、搜索、卸载）</p>
<span id="more"></span>
<h3 id="conda-常见命令"><a href="#conda-常见命令" class="headerlink" title="conda 常见命令"></a><center>conda 常见命令</center></h3><h4 id="一-环境管理"><a href="#一-环境管理" class="headerlink" title="一.环境管理"></a>一.环境管理</h4><h5 id="1-创建环境"><a href="#1-创建环境" class="headerlink" title="1. 创建环境"></a>1. 创建环境</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda create -n [your_env_name] python=x.x   #创建指定版本的python环境</span><br><span class="line">conda env create -n [your_env_name] -f environment.ymal # 从文件中创建环境</span><br><span class="line">conda create -n [your_env_name] --clone [source env] # 复制已有环境</span><br></pre></td></tr></table></figure>
<h5 id="2-查看有哪些环境"><a href="#2-查看有哪些环境" class="headerlink" title="2. 查看有哪些环境"></a>2. 查看有哪些环境</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda info --env</span><br></pre></td></tr></table></figure>
<h5 id="3-激活环境"><a href="#3-激活环境" class="headerlink" title="3. 激活环境"></a>3. 激活环境</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda activate [your_env_name]</span><br></pre></td></tr></table></figure>
<h5 id="4-退出环境"><a href="#4-退出环境" class="headerlink" title="4. 退出环境"></a>4. 退出环境</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda deactivate</span><br></pre></td></tr></table></figure>
<h5 id="5-删除环境"><a href="#5-删除环境" class="headerlink" title="5. 删除环境"></a>5. 删除环境</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda remove -n [your_env_name] --all</span><br></pre></td></tr></table></figure>
<h5 id="6-导出环境配置"><a href="#6-导出环境配置" class="headerlink" title="6. 导出环境配置"></a>6. 导出环境配置</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda env export &gt; environment.yaml</span><br></pre></td></tr></table></figure>
<h4 id="二-包管理"><a href="#二-包管理" class="headerlink" title="二.包管理"></a>二.包管理</h4><h5 id="1-查看安装了哪些包"><a href="#1-查看安装了哪些包" class="headerlink" title="1. 查看安装了哪些包"></a>1. 查看安装了哪些包</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda list</span><br></pre></td></tr></table></figure>
<h5 id="2-安装包"><a href="#2-安装包" class="headerlink" title="2. 安装包"></a>2. 安装包</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda install [package_name] # 安装最新版本</span><br><span class="line">conda install [package_name] # 安装特定版本</span><br></pre></td></tr></table></figure>
<h5 id="3-更新包"><a href="#3-更新包" class="headerlink" title="3. 更新包"></a>3. 更新包</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda remove [package_name] # 更新一个包</span><br><span class="line">conda update --all # 更新所有包</span><br></pre></td></tr></table></figure>
<h5 id="4-搜索包"><a href="#4-搜索包" class="headerlink" title="4. 搜索包"></a>4. 搜索包</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda search [pachage_name]</span><br></pre></td></tr></table></figure>
<h5 id="5-卸载包"><a href="#5-卸载包" class="headerlink" title="5. 卸载包"></a>5. 卸载包</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda remove [package_name]</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>配置</category>
      </categories>
      <tags>
        <tag>configure</tag>
      </tags>
  </entry>
  <entry>
    <title>使用hexo创建个人博客</title>
    <url>/2025/01/30/build-my-blog/</url>
    <content><![CDATA[<p>使用<code>github+hexo</code>创建个人博客</p>
<span id="more"></span>
<h4 id="1-下载工具"><a href="#1-下载工具" class="headerlink" title="1. 下载工具"></a>1. 下载工具</h4><p>自行下载node.js npm ,使用以下命令检查是否正确安装</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">node -v</span><br><span class="line">npm -v</span><br></pre></td></tr></table></figure>
<h4 id="2-在github上创建仓库"><a href="#2-在github上创建仓库" class="headerlink" title="2. 在github上创建仓库"></a>2. 在github上创建仓库</h4><p><img src="/2025/01/30/build-my-blog/image-20250128013605341.png" alt="image-20250128013605341"></p>
<h4 id="3-安装hexo"><a href="#3-安装hexo" class="headerlink" title="3.  安装hexo"></a>3.  安装hexo</h4><p>使用以下命令暗转hexo</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">npm install -g hexo -cli</span><br></pre></td></tr></table></figure>
<h4 id="4-创建Hexo文件夹"><a href="#4-创建Hexo文件夹" class="headerlink" title="4. 创建Hexo文件夹"></a>4. 创建Hexo文件夹</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hexo init &lt;folder&gt;</span><br><span class="line">cd &lt;folder&gt;</span><br><span class="line">npm install</span><br></pre></td></tr></table></figure>
<p>使用以下命令可以本地查看效果</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hexo server</span><br></pre></td></tr></table></figure>
<p>网页浏览: localhost:4000查看效果</p>
<p>初次登录会要求登录github, 现在github不支持密码登录了，只能通过token登录，去GitHub首页</p>
<p> <img src="/2025/01/30/build-my-blog/image-20250128014633141.png" alt="image-20250128014633141"></p>
<p>进入setting后拉到最下面的<code>Developer settings</code> 进行生成，期限选择永久。</p>
<p>修改<code>&lt;folder&gt;</code>下的配置文件<code>_config.yml</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repo: &lt;repository url&gt; # 库地址</span><br><span class="line">  branch: [branch]  # 分支，如main</span><br><span class="line">  message: [message] # 缺省为创建时间</span><br></pre></td></tr></table></figure>
<h4 id="5-发布文章"><a href="#5-发布文章" class="headerlink" title="5. 发布文章"></a>5. 发布文章</h4><p>进入site目录, 右键打开Git Bash, 创建博文:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hexo new &quot;My New Post&quot;</span><br></pre></td></tr></table></figure>
<p>然后source文件夹中会出现一个My New Post.md 文件，可以使用Markdown写文章。写完之后运行下面代码将文章渲染到GitHub Pages 上完成发布。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hexo g # 生成静态页面</span><br><span class="line">hexo d # 部署页面</span><br></pre></td></tr></table></figure>
<p>也可以不通过命令直接自己生成页面，但要在页面开始加入如下格式:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: Hello World # 标题</span><br><span class="line">date: 2019/3/26 hh:mm:ss # 时间</span><br><span class="line">categories: # 分类</span><br><span class="line">- Diary</span><br><span class="line">tags: # 标签</span><br><span class="line">- PS3</span><br><span class="line">- Games</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">摘要</span><br><span class="line">&lt;!--more--&gt;</span><br><span class="line">正文</span><br></pre></td></tr></table></figure>
<p>常用命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hexo new &quot;name&quot;       # 新建文章</span><br><span class="line">hexo new page &quot;name&quot;  # 新建页面</span><br><span class="line">hexo g                # 生成页面</span><br><span class="line">hexo d                # 部署</span><br><span class="line">hexo g -d             # 生成页面并部署</span><br><span class="line">hexo s                # 本地预览</span><br><span class="line">hexo clean            # 清除缓存和已生成的静态文件</span><br><span class="line">hexo help             # 帮助</span><br></pre></td></tr></table></figure>
<h4 id="6-访问"><a href="#6-访问" class="headerlink" title="6. 访问"></a>6. 访问</h4><p><a href="https://codezrq.github.io/">Hexo</a></p>
<h4 id="8-更换主题"><a href="#8-更换主题" class="headerlink" title="8. 更换主题"></a>8. 更换主题</h4><p>在<a href="https://hexo.io/themes/">Themes | Hexo</a>上选择主题，进入网站目录下载主题、</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git clone https://github.com/iissnan/hexo-theme-next themes/next</span><br></pre></td></tr></table></figure>
<p>然后修改 _config.yml 中的 theme 为新主题名称 next，发布。（有的主题需要将 _config.yml 替换为主题自带的，参考主题说明。）</p>
<h4 id="9-更改设置"><a href="#9-更改设置" class="headerlink" title="9. 更改设置"></a>9. 更改设置</h4><p>所有设置都在<code>_config.yml</code>目录下，参考官方文档<a href="https://hexo.io/zh-cn/docs/configuration">配置 | Hexo</a></p>
<h4 id="可能遇到的问题"><a href="#可能遇到的问题" class="headerlink" title="可能遇到的问题"></a>可能遇到的问题</h4><h5 id="1-在执行-hexo-d-时报错"><a href="#1-在执行-hexo-d-时报错" class="headerlink" title="1. 在执行 hexo d 时报错"></a>1. 在执行 hexo d 时报错</h5><p>fatal: 无法访问 ‘<a href="https://github.com/chixinn/chixinn.github.io.git/">https://github.com/chixinn/chixinn.github.io.git/</a>‘</p>
<p>解决办法： 其实时git push 的问题。先执行以下命令，再执行<code>hexo d</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git config --global --unset http.proxy</span><br></pre></td></tr></table></figure>
<h5 id="2-在执行-hexo-d-时报错"><a href="#2-在执行-hexo-d-时报错" class="headerlink" title="2.在执行 hexo d 时报错"></a>2.在执行 hexo d 时报错</h5><p>ERROR Deployer not found: git</p>
<p>解决办法: 没有安装<code>hexo-deployer-git</code></p>
<p>使用以下命令安装</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure>
<h5 id="3-修改并部署后没有效果"><a href="#3-修改并部署后没有效果" class="headerlink" title="3. 修改并部署后没有效果"></a>3. 修改并部署后没有效果</h5><p>使用 <code>hexo clean</code>清理后重新部署</p>
]]></content>
      <categories>
        <category>配置</category>
      </categories>
      <tags>
        <tag>configure</tag>
      </tags>
  </entry>
  <entry>
    <title>决策树原理详解</title>
    <url>/2025/02/09/decisionTree/</url>
    <content><![CDATA[<p>决策树原理，举例演算及其代码实现</p>
<span id="more"></span>
<h4 id="一-使用的例子："><a href="#一-使用的例子：" class="headerlink" title="一 .使用的例子："></a>一 .使用的例子：</h4><div class="table-container">
<table>
<thead>
<tr>
<th>人员</th>
<th>眼睛颜色</th>
<th>头发颜色</th>
<th>所属人种</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>黑色</td>
<td>金色</td>
<td>白种人</td>
</tr>
<tr>
<td>2</td>
<td>蓝色</td>
<td>黑色</td>
<td>黄种人</td>
</tr>
<tr>
<td>3</td>
<td>灰色</td>
<td>金色</td>
<td>白种人</td>
</tr>
<tr>
<td>4</td>
<td>蓝色</td>
<td>金色</td>
<td>白种人</td>
</tr>
<tr>
<td>5</td>
<td>灰色</td>
<td>金色</td>
<td>白种人</td>
</tr>
<tr>
<td>6</td>
<td>黑色</td>
<td>黑色</td>
<td>黄种人</td>
</tr>
<tr>
<td>7</td>
<td>灰色</td>
<td>黑色</td>
<td>黄种人</td>
</tr>
<tr>
<td>8</td>
<td>蓝色</td>
<td>黑色</td>
<td>黄种人</td>
</tr>
</tbody>
</table>
</div>
<p>决策树的基本思想是从一颗空的决策树开始，选择某一属性作为测试属性，该测试属性对应决策树中的决策顶点，再在剩下的数据集上递归选择另一属性建立决策顶点。</p>
<p><strong>一。决策树学习算法大体上可以分为两个阶段：建模阶段和预测阶段</strong></p>
<p><strong>建模阶段</strong>可通过以下步骤实现：</p>
<ul>
<li>首先，所有训练样本都处于根节点位置</li>
<li>基于一定的策略选择属性</li>
<li>根据选择的属性，递归地划分样本</li>
</ul>
<p>我们主要要解决两个问题：</p>
<ul>
<li><p>递归结束的条件：递归的情况有以下几种</p>
<ul>
<li>没有可用的数据：以父节点的标签作为标签</li>
<li>被分到的样本都属于同一个类别：以该类别作为标签</li>
<li>所有属性都已用于之前的划分，没有可用的属性可以继续划分：采用多数投票的方式选出标签。</li>
</ul>
</li>
<li><p>通过什么策略选择节点的属性</p>
</li>
</ul>
<p>使用上面的例子，如果我们先选眼睛颜色作为属性，可以建出如下的树<br><img src="/2025/02/09/decisionTree/c12497b3880b065611e22c3af4681b1c.png" alt="在这里插入图片描述"></p>
<p>如果先选择头发颜色作为属性，可以建出如下的树：</p>
<p><img src="/2025/02/09/decisionTree/2d3b1ce475a59658fcf6c79e95f1765b.png" alt="在这里插入图片描述"></p>
<p>从上面两棵树来看，选择不同属性作为节点对建树的影响很大，所以我们要选择合适的策略算法，常用的有<code>ID3,C4.5,CART</code>，这三种之后会逐个解释。</p>
<p>通过上述步骤，就可以建立一棵决策树，接下来就是<strong>预测阶段</strong>：</p>
<p>预测阶段就是根据带预测数据各属性的取值，逐步遍历树，直到叶子节点，叶子节点对应的标签就说我们的预测结果，比如我们要预测<strong>蓝眼睛黄头发</strong>的属于哪个人种，以第一棵树为例</p>
<p>从根节点出发，先看眼睛颜色，为蓝色，所以选择根节点的第二个节点作为下个节点，然后看头发颜色，为黄色，选择第一个节点，此时已到叶子节点，对应的标签为白种人，所以我们预测结果为白种人。</p>
<p><strong>二。决策函数</strong></p>
<p>上面已经解释了建立一颗树的大致步骤，但还有最重要的一个问题没有解决，就是选择什么策略：</p>
<p><strong>ID3</strong>使用信息增益度选择测试属性，要解释如何计算信息增益度，先要解释信息熵，信息熵是衡量一个信息不确定性大小的值，通常我们认为，如果某个信息让我们的判断更加有序，清晰，则它信息熵越小，反之越大。</p>
<p>$X$的信息熵计算公式如下：</p>
<p>$H(X) = \sum_{x\in X}(-p(x)\times log(p(x)))$</p>
<p>条件熵:$H(Y|X)$表示是在$X$确定下$Y$的熵，可以通过如下公式计算</p>
<p>$H(Y|X) = \sum_{x\in X}p(x)H(Y|X=x)=\sum_{x\in X}\sum_{y\in Y}p(x,y)log_2p(y|x)$</p>
<p>联合熵：$H(X,Y) = -\sum_{x\in X}\sum_{y\in Y}p(x,y)log_2p(x,y)=H(X)+H(Y|X)= H(Y)+H(Y|X))$</p>
<p>这表明两个变量的联合熵等于$X$的熵加上给定$X$,出现$Y$的条件熵。</p>
<p>接着根据联合熵的公式定义两个变量的互信息为$I(X,Y)=H(Y)-H(Y|X)$</p>
<p>互信息量表示两个变量之间的相关程度，互信息量越大，表示两个变量相关程度越大，$X$就越适合作为属性，比如$Y$表示人种，$X$表示头发颜色，如果$I(X,Y)$越大，表示两者之间关系很大，此时选择头发颜色作为属性是合适的。</p>
<p>例如，上例中$H(Y) = -0.5<em>log_2(0.5)-0.5</em>log_20.5=1$</p>
<p>$H(Y|X=”眼睛颜色”)=\frac{2}{8}<em>(-\frac{1}{2}</em>log_2\frac{1}{2}-\frac{1}{2}<em>log_2\frac{1}{2})+\frac{3}{8}</em>(-\frac{1}{3}<em>log_2\frac{1}{3}-\frac{2}{3}</em>log_2\frac{2}{3})+\frac{3}{8}<em>(-\frac{1}{3}</em>log_2\frac{1}{3}-\frac{2}{3}*log_2\frac{2}{3})$</p>
<p>$I(X=”眼睛颜色”，Y) = H(Y)-H(Y|X=”眼睛颜色”)$</p>
<p>类似的，可以算出$I(X=”头发颜色”，Y)$,然后取两者中的较大值。</p>
<p>但是采用信息增益进行数据分裂容易偏向取值较多的特征，可以理解为分支数越多，不确定性越大，熵就会越大。</p>
<p><strong>C4.5</strong>采用增益率克服了上述问题，通过引入一个变量增益率</p>
<p>$SplitInfo_A(D)=-\sum_{j}\frac{|D_j|}{|D|}\times log_2(\frac{|D_j|}{D})$</p>
<p>对信息增益进行归一化，一个特征的取值越多，变量增益率就越大，把它作为分母，就可以校正信息增益偏向取值较多的特征的问题，</p>
<p>即最终结果为$GainRatio_A(D)=\frac{Gain_A(D)}{SplitInfo_A(D)}$</p>
<p>取计算结果最大的特征作为属性。</p>
<p><strong>CART</strong>模型基于基尼指数选择合适的属性。</p>
<p>如果一个数据集包含来自n个类的样本，那么基尼指数$gini(D)=\sum_jP_j(1-p_j)=1-\sum_jp_j^2$</p>
<p>如果一个数据集$D$被分成两个子集$D_1$和$D_2$，大小分别为$N_1$和$N_2$，那么基尼指数$gini_{split}(D)=\frac{N_1}{N}gini(D_1)+\frac{N_2}{N}gini(D_2)$</p>
<p>取基尼指数最小的属性最为本节点的属性。</p>
<p><a href="https://download.csdn.net/download/weixin_46091520/52585766?spm=1001.2014.3001.5501">代码实现, 决策函数为ID3/C4.5/CART</a></p>
]]></content>
      <categories>
        <category>作业</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Mechine learning</tag>
      </tags>
  </entry>
  <entry>
    <title>大模型应用系列(一)LLM落地常见的技术方案</title>
    <url>/2025/03/03/llm-application-1/</url>
    <content><![CDATA[<p>简要介绍大模型落地的几种方案，包括：提示词工程，RAG，Agent，微调。</p>
<span id="more"></span>
<h4 id="一-LLM分类"><a href="#一-LLM分类" class="headerlink" title="一. LLM分类"></a>一. LLM分类</h4><p>大模型是指具有大规模参数和复杂计算结构的深度神经网络，ChatGPT对它的定义是: 大模型本质上是一个使用海量数据训练而成的深度神经网络模型，其巨大的数据和参数规模，实现了智能的涌现，展现出类似人类的智能。<br>大致可以分为两类: GPT(Generation Pre-trained Transformer)和BERT(Bidirectional Encoder Repersentations from Transformers)他们主要有以下不同点：</p>
<h5 id="1-训练方式"><a href="#1-训练方式" class="headerlink" title="1. 训练方式"></a>1. 训练方式</h5><p>BERT使用双向语言模型进行训练(在推理过程中会用到mask技术，类似填空)，GPT采用自回归语言模型进行训练。<br>可以简单理解为，BERT是双向的，同时考虑单词的左右上下文，BERT同时查看句子中的所有单词。而GPT每次只看到前面单词。</p>
<h5 id="2-任务目标"><a href="#2-任务目标" class="headerlink" title="2. 任务目标"></a>2. 任务目标</h5><p>BERT的目标是理解句子的是上下文，适合词语级别的任务，如命名实体识别，问答等<br>GPT的目标是生成连贯的文本，适用于生成式任务</p>
<h5 id="3-应用场景"><a href="#3-应用场景" class="headerlink" title="3. 应用场景"></a>3. 应用场景</h5><p>BERT主要用于自然语言理解任务，如文本分类，实体识别，语义关系判断等<br>GPU主要用于文本生成任务</p>
<p>目前比较火的是模型ChatGPT, Qwen, DeepSeek, 是GPT的，介绍BERT是觉得可能舆情那块没准也可以用BERT做分类。</p>
<h4 id="二-LLM训练到应用的基本步骤"><a href="#二-LLM训练到应用的基本步骤" class="headerlink" title="二.LLM训练到应用的基本步骤"></a>二.LLM训练到应用的基本步骤</h4><div class="table-container">
<table>
<thead>
<tr>
<th>阶段</th>
<th>效果</th>
<th>常用技术</th>
</tr>
</thead>
<tbody>
<tr>
<td>预训练</td>
<td>模型获得基本知识，(所有都懂，但都只懂一点点)</td>
<td>无监督训练，(消耗大部分资源), DS的主要创新点也在这里。</td>
</tr>
<tr>
<td>微调</td>
<td>通过来自某个领域的标注的高质量数据微调LLM，使LLM提高某方面能力</td>
<td>RLHF(rainforce learning from human feedback, 使用强化学习，人类对LLM的输出进行奖励，以指导LLM的学习。)，LoRA微调(将高维的参数空间转化到低秩空间以减少参数量，GPU不够时使用。(我使用的微调方式)</td>
</tr>
<tr>
<td>对齐</td>
<td>生成无害，符合人类预期的结果</td>
<td>RLHF</td>
</tr>
<tr>
<td>评估</td>
<td>在应用领域上评估模型效果</td>
<td>召回率，准确率等</td>
</tr>
<tr>
<td>部署</td>
<td>离线推理(每次都要加载模型参数)或者在线服务(API)</td>
<td>vllm, ollama。ollama适用于轻量型应用场景，企业级一般用vllm，我使用vllm部署。模型部署的环境，方式等会影响模型的性能(吞吐量，速度等)</td>
</tr>
<tr>
<td>应用开发</td>
<td>下游应用调用上游LLM</td>
<td>RAG，Agent等</td>
</tr>
</tbody>
</table>
</div>
<h4 id="三-应用大模型的几种方式"><a href="#三-应用大模型的几种方式" class="headerlink" title="三. 应用大模型的几种方式"></a>三. 应用大模型的几种方式</h4><h5 id="1-提示词工程"><a href="#1-提示词工程" class="headerlink" title="1. 提示词工程"></a>1. 提示词工程</h5><p>最常用的方式，不需要微调，基座模型都是一样的，使用预训练的基座模型即可，在prompt中描述场景，任务要求，输入，知识，输出格式等，LLM生成对应的输出。<br>优点:  省事，不需要微调，一个大模型可以像多个应用提供推理服务，只要针对不同应用写不同prompt。<br>缺点：LLM只看到prompt所展示的知识，并不是真的学会了，比如要让LLM回答一个法律知识，可能得在prompt里输入一整本法律书。目前的多轮回答上下文也只是在prompt中输入前几轮回答的问题和答案。</p>
<h5 id="2-提示词工程改进-RAG-Retrieval-Augmented-Generation-检索增强生成"><a href="#2-提示词工程改进-RAG-Retrieval-Augmented-Generation-检索增强生成" class="headerlink" title="2. 提示词工程改进:RAG(Retrieval-Augmented Generation 检索增强生成)"></a>2. 提示词工程改进:RAG(Retrieval-Augmented Generation 检索增强生成)</h5><p>RAG=检索+增强+生成<br>提示词工程可以再prompt中输入回答所需知识，以提高回答质量，但输入过多的知识会造成prompt过长，导致推理速度慢。<br>输入的知识时一个领域的知识，其中和当前问题相关的可能只有一些，真正起作用的其实只有这部分知识，RAG先构建一个知识库(如通过Nomic-Embed-Text模型等构建文本嵌入数据库)，在生成prompt前检索出最相关的知识，结合到prompt中<br>优点：缩短prompt长度，缓解LLM幻觉。<br>缺点：依赖检索的准确性</p>
<h5 id="3-微调"><a href="#3-微调" class="headerlink" title="3. 微调"></a>3. 微调</h5><p>使用LoRA(资源消耗少)，全参微调(资源消耗高)对模型进一步微调，使其更适合某个任务<br>优点：prompt可以更短，不需要输入太多知识，因为微调过程中已经学会了。<br>缺点：需要资源，微调后的模型一般只适用于一个场景，因为对每个应用都要部署一个模型。需要收集微调数据<br>LoRA原理: 在原始的PLM(Pre-trained Language Model)旁边增加一个旁路, 该旁路做一个先降维再升维的操作，只需要微调降维后的参数，以此来降低微调的参数量。</p>
<h5 id="4-Agent"><a href="#4-Agent" class="headerlink" title="4. Agent"></a>4. Agent</h5><p>Agent是能够模拟独立思考过程，灵活调用各类工具，逐步达成预设目标。大模型Agent由规划、记忆、工具与行动四大关键部分组成，分别负责任务拆解与策略评估、信息存储与回忆、环境感知与决策辅助、以及将思维转化为实际行动。<br><strong>1.规划</strong></p>
<ul>
<li><p>定义：规划是Agent的思维模型，负责拆解复杂任务为可执行的子任务，并评估执行策略。</p>
</li>
<li><p>实现方式：通过大模型提示工程（如ReAct、CoT推理模式）实现，使Agent能够精准拆解任务，分步解决</p>
</li>
</ul>
<p><strong>2.记忆</strong></p>
<ul>
<li><p>定义：记忆即信息存储与回忆，包括短期记忆和长期记忆。</p>
</li>
<li><p>实现方式：短期记忆用于存储会话上下文，支持多轮对话；长期记忆则存储用户特征、业务数据等，通常通过向量数据库等技术实现快速存取。</p>
</li>
</ul>
<p><strong>3.工具</strong></p>
<ul>
<li>定义：工具是Agent感知环境、执行决策的辅助手段，如API调用、插件扩展等。</li>
<li>实现方式：通过接入外部工具（如API、插件）扩展Agent的能力，如ChatPDF解析文档、Midjourney文生图等。<br>  <strong>4.行动</strong></li>
<li>定义：行动是Agent将规划与记忆转化为具体输出的过程，包括与外部环境的互动或工具调用。</li>
<li>实现方式：Agent根据规划与记忆执行具体行动，如智能客服回复、查询天气预报、AI机器人抓起物体等。</li>
</ul>
<h5 id="5-部署中的问题"><a href="#5-部署中的问题" class="headerlink" title="5. 部署中的问题"></a>5. 部署中的问题</h5><h6 id="a-部署过程中所需的显存-加载LLM参数的显存-kv-Cache显存"><a href="#a-部署过程中所需的显存-加载LLM参数的显存-kv-Cache显存" class="headerlink" title="a. 部署过程中所需的显存=加载LLM参数的显存+kv-Cache显存"></a>a. 部署过程中所需的显存=加载LLM参数的显存+kv-Cache显存</h6><p>kv-Cache: LLM推理时自左向右，意味着每次都要计算前面所有单词的注意力分数(每个token都有一个k值和一个v值)，LLM每次只预测一个token，预测时根据当前token的query值和之前序列的所有k值和v值进行预测。为了避免重复计算，LLM会缓存每个token的k值和v值，需要的时候直接读取，没有优化时kv-cache是连续的，并且需要提前预留。<br>正式应用中的部署一般使用vllm，vllm将大模型加载并常驻到显存中，当推理请求到来时，只需将输入加载到显存，因此可以加快推理速度。同时vllm使用了PageAttention优化(类似虚拟缓存中的分页管理)，平常的LLM推理会设置max-token，加载时需要预留显存，PageAttention技术可以动态管理kv-cache，并且不需要连续，减少内存碎片。(【论文阅读】Efficient Memory Management for Large Language Model Serving with PagedAttention | CodeRQ)</p>
<h6 id="b-性能"><a href="#b-性能" class="headerlink" title="b. 性能"></a>b. 性能</h6><p>部署模型的性能影响也是多方面的，比如上云时调度器将该任务调到哪个GPU上，多卡部署时还依赖于GPU之间的连结方式，这是基础设施带来的影响，我们很难决定(合理利用调度策略，欺骗调度器)<br>主要考虑的是选择的模型大小，max-token，一般模型越大，max-token越大，推理越慢。</p>
]]></content>
      <categories>
        <category>大模型应用</category>
      </categories>
  </entry>
  <entry>
    <title>大模型应用系列(二) Huggingface的安装和使用</title>
    <url>/2025/03/06/llm-application-2/</url>
    <content><![CDATA[<p>介绍如何从huggingface下载模型，如何使用API调用huggingface模型的在线服务，以及如何本地运行模型推理服务。</p>
<span id="more"></span>
<h4 id="一-使用API调用Huggingface-在线服务。"><a href="#一-使用API调用Huggingface-在线服务。" class="headerlink" title="一. 使用API调用Huggingface 在线服务。"></a>一. 使用API调用Huggingface 在线服务。</h4><p>通过post向huggingface发送请求, 代码如下:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="comment"># 通过post调用huggingface的在线模型</span></span><br><span class="line">API_URL = <span class="string">&quot;https://api-inference.huggingface.co/models/uer/gpt2-distil-chinese-cluecorpussmall&quot;</span></span><br><span class="line">API_TOKEN = <span class="string">&quot;hf_xxxxxxxxxxx&quot;</span> <span class="comment"># 从官网申请API_TOKEN</span></span><br><span class="line"></span><br><span class="line">headers = &#123;<span class="string">&quot;Authorization&quot;</span>: <span class="string">f&quot;Bearer <span class="subst">&#123;API_TOKEN&#125;</span>&quot;</span>&#125;</span><br><span class="line"><span class="comment"># 不使用token，匿名访问</span></span><br><span class="line">response = requests.post(API_URL, headers=headers, json = &#123;<span class="string">&quot;inputs&quot;</span>:<span class="string">&quot;你好,huggingface&quot;</span>&#125;)</span><br><span class="line"><span class="built_in">print</span>(response.json())</span><br></pre></td></tr></table></figure><br>其中API_TOKEN要从官网申请</p>
<p> <img src="/2025/03/06/llm-application-2/image-20250306224349015.png" alt="image-20250306224349015"></p>
<p>点击Access Token， 然后进入申请，里面的权限全选即可。</p>
<h4 id="二-从huggingface上拉取模型"><a href="#二-从huggingface上拉取模型" class="headerlink" title="二. 从huggingface上拉取模型"></a>二. 从huggingface上拉取模型</h4><p>首先创建python虚拟环境，如果没有安装Anaconda，可以参考<br><a href="https://blog.csdn.net/weixin_52677672/article/details/133632708">Windows下的Anaconda详细安装教程_windows安装anaconda-CSDN博客</a></p>
<p>pip 安装<code>huggingface,transformers</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pip install huggingface_hub</span><br><span class="line">pip install -U transformers     </span><br></pre></td></tr></table></figure>
<p>从<a href="https://huggingface.co/">https://huggingface.co/</a> 上查找对应的模型，然后用如下命令下载, 比如要下载模型<code>gpt2-chinese-cluecorpussmall</code>到当前目录下的 <code>./gpt2-chinese-cluecorpussmall</code>，则用如下命令:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">huggingface-cli download --resume-download Qwen/Qwen2.5-0.5B-Instruct --local-dir Qwen2.5-0.5B-Instruct</span><br></pre></td></tr></table></figure>
<p>下载数据集用如下命令:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">huggingface-cli download --repo-type dataset lavita/medical-qa-shared-task-v1-toy --local-dir edical-qa-shared-task-v1-toy</span><br></pre></td></tr></table></figure>
<p><strong>注意:</strong> 从huggingface上下载需要科学上网</p>
<p>如果没有科学上网，可以从huggingface的国内镜像下载(笔者常用，推荐)</p>
<p><a href="https://hf-mirror.com/">HF-MIRRO</a></p>
<p>下载前先设置环境变量</p>
<p>windows</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$</span><span class="language-bash"><span class="built_in">env</span>:HF_ENDPOINT = <span class="string">&quot;https://hf-mirror.com&quot;</span></span></span><br></pre></td></tr></table></figure>
<p>linux</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export HF_ENDPOINT=https://hf-mirror.com</span><br></pre></td></tr></table></figure>
<p>下载模型</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">huggingface-cli download --resume-download gpt2 --local-dir gpt2</span><br></pre></td></tr></table></figure>
<p>下载数据集</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">huggingface-cli download --repo-type dataset --resume-download wikitext --local-dir wikitext</span><br></pre></td></tr></table></figure>
<h4 id="三-本地运行模型"><a href="#三-本地运行模型" class="headerlink" title="三. 本地运行模型"></a>三. 本地运行模型</h4><p>下载好模型后，使用<code>transformers</code>运行模型, 目前大模型可以简单分为两类: Bert类和GPT类，Bert类常用于词嵌入，分类，情感识别等，GPT类用于生成。</p>
<h5 id="3-1-Bert类"><a href="#3-1-Bert类" class="headerlink" title="3.1 Bert类"></a>3.1 Bert类</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertForSequenceClassification, pipeline</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用bert做分类</span></span><br><span class="line">model_path = <span class="string">&quot;bert-base-chinese&quot;</span>  <span class="comment"># 模型文件夹所在目录，可以绝对路径或者相对路径</span></span><br><span class="line"></span><br><span class="line">model = BertForSequenceClassification.from_pretrained(model_path)</span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(model_path)</span><br><span class="line"></span><br><span class="line">classifier = pipeline(<span class="string">&quot;text-classification&quot;</span>, model = model, tokenizer = tokenizer, device = <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line">result = classifier(<span class="string">&quot;你好, 我是一款语言模型&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure>
<h5 id="3-2-GPT-类"><a href="#3-2-GPT-类" class="headerlink" title="3.2 GPT 类"></a>3.2 GPT 类</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM, AutoTokenizer, pipeline</span><br><span class="line"><span class="comment"># 本地调用GPT2进行语言生成</span></span><br><span class="line">model_name = <span class="string">&quot;uer/gpt2-distil-chinese-cluecorpussmall&quot;</span></span><br><span class="line">model_path = <span class="string">&quot;./gpt2-chinese-cluecorpussmall&quot;</span></span><br><span class="line"></span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(model_name)</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_name)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用GPT2创建生成文本的pipeline</span></span><br><span class="line">generator = pipeline(<span class="string">&quot;text-generation&quot;</span>, model = model, tokenizer = tokenizer, device=<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成文本</span></span><br><span class="line"></span><br><span class="line">output = generator(<span class="string">&quot;讲一个猫和老鼠的故事,&quot;</span>, </span><br><span class="line">                   max_length=<span class="number">50</span>, </span><br><span class="line">                   num_return_sequences=<span class="number">1</span>, <span class="comment"># 将输出划分为num_return_sequences</span></span><br><span class="line">                   truncation=<span class="literal">True</span>,  <span class="comment"># 输入超出max_token会截断</span></span><br><span class="line">                   temperature=<span class="number">0.1</span>, <span class="comment"># 温度参数, 越大越有创造性(随机性)</span></span><br><span class="line">                   top_k=<span class="number">50</span>, <span class="comment"># 每次生成时只会从概率最高的top_k中选择, 再根据temperature选择</span></span><br><span class="line">                   top_p=<span class="number">0.9</span>, <span class="comment"># 核采样, 会从生成的词汇中选择一组累计概率达到top_p的词汇中选择</span></span><br><span class="line">                   clean_up_tokenization_spaces=<span class="literal">True</span> <span class="comment"># 是否清除多余的空格</span></span><br><span class="line">                   )</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大模型应用</category>
        <category>微调与训练</category>
      </categories>
  </entry>
  <entry>
    <title>大模型应用系列(三) Bert微调-评论情感分析</title>
    <url>/2025/03/12/llm-application-3/</url>
    <content><![CDATA[<p>在二分类问题上微调Bert模型，介绍AI项目的开发流程，包括数据，模型，微调，评估，部署，以及介绍开发过程中细节，并给出各个步骤的代码。</p>
<span id="more"></span>
<h4 id="一-准备模型和数据"><a href="#一-准备模型和数据" class="headerlink" title="一. 准备模型和数据"></a>一. 准备模型和数据</h4><p>从huggingface上下载模型<code>bert-base-chinese</code>和数据集<code>data/gpt2-chinese-cluecorpussmall</code> (具体教程见上篇)</p>
<h4 id="二-分词器解读"><a href="#二-分词器解读" class="headerlink" title="二. 分词器解读"></a>二. 分词器解读</h4><p>使用<code>transformers</code>加载分词器, 打印分词器内容</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span>  BertTokenizer</span><br><span class="line"></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">&#x27;./model/bert-base-chinese&#x27;</span>) <span class="comment"># 模型相对路径</span></span><br><span class="line"><span class="built_in">print</span>(tokenizer)</span><br></pre></td></tr></table></figure>
<p>运行后打印如下:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">BertTokenizer(name_or_path=&#x27;./model/bert-base-chinese&#x27;, vocab_size=21128, model_max_length=512, is_fast=False, padding_side=&#x27;right&#x27;, truncation_side=&#x27;right&#x27;, special_tokens=&#123;&#x27;unk_token&#x27;: &#x27;[UNK]&#x27;, &#x27;sep_token&#x27;: &#x27;[SEP]&#x27;, &#x27;pad_token&#x27;: &#x27;[PAD]&#x27;, &#x27;cls_token&#x27;: &#x27;[CLS]&#x27;, &#x27;mask_token&#x27;: &#x27;[MASK]&#x27;&#125;, clean_up_tokenization_spaces=True, added_tokens_decoder=&#123;</span><br><span class="line">	0: AddedToken(&quot;[PAD]&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),</span><br><span class="line">	100: AddedToken(&quot;[UNK]&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),</span><br><span class="line">	101: AddedToken(&quot;[CLS]&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),</span><br><span class="line">	102: AddedToken(&quot;[SEP]&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),</span><br><span class="line">	103: AddedToken(&quot;[MASK]&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),</span><br><span class="line">&#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>一些比较重要的参数如下:</p>
<p><code>vocab_size</code>: 词典大小, 大模型没办法直接理解字符, 所以要先编码字符, 具体的编码方式是每个字符的编码是它在字符中的索引(从0开始)，可以从模型文件夹中的<code>vocab.json</code>看到。词典中每个字有两个，比如白，一个是<code>白</code>, 一个是<code>##白</code>, 第一个是以字为单位，第二个是以词为单位(含上下文)，所以词典包含的字符数其实是$\frac{vocab_size}{2}$ 目前大模型预训练一般以字为单位(训练难但扩展性好)。</p>
<p><code>special_toknes</code>: 一些特殊符号,包括,不在词典中的字符用<code>[UNK]</code>代替。<code>[PAD]</code>用于填充,</p>
<p>比如句子“白日依山尽，” 可以从<code>vocab.json</code>中查到他们对应的索引，编码为<code>[4635, 3189, 898, 2255, 2226, 8024]</code>(索引要从0开始), 使用如下代码测试(接上述代码)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 批量编码句子</span></span><br><span class="line">sents = [<span class="string">&#x27;白日依山尽，&#x27;</span>, <span class="string">&#x27;价格再找个地段属于适中，附近有早餐店，小饭店，比较方便，无早也无所&#x27;</span>]</span><br><span class="line"></span><br><span class="line">out = tokenizer.batch_encode_plus(</span><br><span class="line">    batch_text_or_text_pairs=[sents[<span class="number">0</span>], sents[<span class="number">1</span>]], <span class="comment"># 要编码的句子</span></span><br><span class="line">    add_special_tokens=<span class="literal">True</span>, <span class="comment"># 是否是用特殊字符</span></span><br><span class="line">    truncation=<span class="literal">True</span>, <span class="comment"># 截断超过max_length的部分</span></span><br><span class="line">    max_length=<span class="number">9</span>, <span class="comment"># 最大长度</span></span><br><span class="line">    padding=<span class="string">&quot;max_length&quot;</span>, <span class="comment"># 是否填充, 用0填充</span></span><br><span class="line">    return_tensors=<span class="literal">None</span>, <span class="comment"># 返回的数组是什么类型，tf(Tensorflow), pt(pytorch张量), np(numpy), None(list)</span></span><br><span class="line">    <span class="comment"># transformers模型需要输入以下三个数据</span></span><br><span class="line">    return_attention_mask=<span class="literal">True</span>,</span><br><span class="line">    return_special_token = <span class="literal">True</span>,</span><br><span class="line">    return_special_tokens_mask= <span class="literal">True</span>,</span><br><span class="line"></span><br><span class="line">    return_length = <span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k, v <span class="keyword">in</span> out.items():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;k&#125;</span> : <span class="subst">&#123;v&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重新解码</span></span><br><span class="line"><span class="built_in">print</span>(tokenizer.decode(out[<span class="string">&#x27;input_ids&#x27;</span>][<span class="number">0</span>]))</span><br><span class="line"><span class="built_in">print</span>(tokenizer.decode(out[<span class="string">&#x27;input_ids&#x27;</span>][<span class="number">1</span>]))</span><br></pre></td></tr></table></figure>
<p>结果如下:</p>
<p> <img src="/2025/03/12/llm-application-3/image-20250308095851125.png" alt="image-20250308095851125"></p>
<p>可以看出, 句子编码结果和推导一致，由于最大长度为9，所以第一个句子用<code>[PAD]</code>填充，第二个句子截断了。<br>input_ids: 编码结果<br>token_type_ids: 仅适用上下文编码, 第一个句子和特殊字符都是0, 第二个是1, 现在大多不适用了。<br>special_token_mask: 特殊字符的位置是1, 其他是0<br>length: 编码后的长度</p>
<h4 id="三-Bert-增量微调：情感分析"><a href="#三-Bert-增量微调：情感分析" class="headerlink" title="三. Bert 增量微调：情感分析"></a>三. Bert 增量微调：情感分析</h4><h5 id="3-0-AI项目开发流程"><a href="#3-0-AI项目开发流程" class="headerlink" title="3.0 AI项目开发流程"></a>3.0 AI项目开发流程</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">graph LR</span><br><span class="line">A[需求/数据]--&gt;B[模型选型/设计]--&gt;C[训练]--&gt;D[效果评估]--&gt;E[部署]</span><br></pre></td></tr></table></figure>
<h5 id="3-1-需求-数据"><a href="#3-1-需求-数据" class="headerlink" title="3.1 需求/数据"></a>3.1 需求/数据</h5><h6 id="3-1-1-获取数据"><a href="#3-1-1-获取数据" class="headerlink" title="3.1.1 获取数据"></a>3.1.1 获取数据</h6><p>使用如下代码从huggingface中下载数据集并保存到本地</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span>  load_dataset, load_from_disk</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从huggingface下载数据集并保存到本地 需要科学上网</span></span><br><span class="line">dataset = load_dataset(path=<span class="string">&#x27;lansinuote/ChnSentiCorp&#x27;</span>)</span><br><span class="line">dataset.save_to_disk(dataset_dict_path=<span class="string">&#x27;./data/ChnSentiCorp&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>保存后本地数据集结构如下:</p>
<p> <img src="/2025/03/12/llm-application-3/image-20250308154930837.png" alt="image-20250308154930837"></p>
<p>使用如下代码从本地数据集加载</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataset = load_from_disk(<span class="string">&#x27;./data/ChnSentiCorp&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(dataset)</span><br></pre></td></tr></table></figure>
<p>打印了数据集的结构:</p>
<p><img src="/2025/03/12/llm-application-3/image-20250308155022974.png" alt="image-20250308155022974"> </p>
<p>扩展，一般我们自己的数据集是<code>csv</code>文件，也可以用load_dataset加载。也可以将数据集保存为csv文件.</p>
<h6 id="3-1-2-转换数据格式"><a href="#3-1-2-转换数据格式" class="headerlink" title="3.1.2 转换数据格式"></a>3.1.2 转换数据格式</h6><p>将数据转换为模型输入的数据格式。制作过程为定义自己的数据类，继承Dataset，重写三个函数，模板如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span>  Dataset</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="comment"># 初始化数据集</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="comment"># 返回数据集长度</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="comment"># 对每条数据单独处理</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, item</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p>在本项目中, 定义MyDataset类如下，存放在<code>MyData.py</code>文件中:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span>  Dataset</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span>  load_from_disk</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化数据集</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, split</span>):</span><br><span class="line">        <span class="comment"># 从磁盘加载数据</span></span><br><span class="line">        <span class="variable language_">self</span>.dataset = load_from_disk(<span class="string">&#x27;./data/ChnSentiCorp&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> split == <span class="string">&quot;train&quot;</span>:</span><br><span class="line">            <span class="variable language_">self</span>.dataset = <span class="variable language_">self</span>.dataset[<span class="string">&#x27;train&#x27;</span>]</span><br><span class="line">        <span class="keyword">elif</span> split == <span class="string">&#x27;test&#x27;</span>:</span><br><span class="line">            <span class="variable language_">self</span>.dataset = <span class="variable language_">self</span>.dataset[<span class="string">&#x27;test&#x27;</span>]</span><br><span class="line">        <span class="keyword">elif</span> split == <span class="string">&#x27;validation&#x27;</span>:</span><br><span class="line">            <span class="variable language_">self</span>.dataset = <span class="variable language_">self</span>.dataset[<span class="string">&#x27;validation&#x27;</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;数据集中没有该结构&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回数据集长度</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.dataset)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对每条数据单独处理</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, item</span>):</span><br><span class="line">        text = <span class="variable language_">self</span>.dataset[item][<span class="string">&#x27;text&#x27;</span>]</span><br><span class="line">        label = <span class="variable language_">self</span>.dataset[item][<span class="string">&#x27;label&#x27;</span>]</span><br><span class="line">        <span class="keyword">return</span> text, label</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    dataset = MyDataset(<span class="string">&quot;test&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> dataset:</span><br><span class="line">        <span class="built_in">print</span>(data)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>运行后打印测试集:</p>
<p> <img src="/2025/03/12/llm-application-3/image-20250308161318112.png" alt="image-20250308161318112"></p>
<h5 id="3-2-模型设计"><a href="#3-2-模型设计" class="headerlink" title="3.2 模型设计"></a>3.2 模型设计</h5><p>使用增量微调, 即模型分为两个部分，先用Bert生成词嵌入，再在后面接上我们的分类模型，在训练时，冻结嵌入模型Bert，只训练分类模型。</p>
<p>定义自己的模型类，存放在<code>net.py</code>文件中:</p>
<p>通过打印模型结构，可以看到Bert模型最后将一个句子嵌入为768维的向量，所以我们增量模型的输入为768维的向量，输出为2维向量（二分类任务）<img src="/2025/03/12/llm-application-3/image-20250308162911668.png" alt="image-20250308162911668"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span>  BertModel</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义设备</span></span><br><span class="line">DEVICE = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(DEVICE)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载预训练模型</span></span><br><span class="line">pretrained = BertModel.from_pretrained(<span class="string">&quot;./model/bert-base-chinese&quot;</span>).to(DEVICE)</span><br><span class="line"><span class="comment"># 打印模型结构</span></span><br><span class="line"><span class="comment"># print(pretrained)</span></span><br><span class="line"><span class="comment"># 定义下游任务(增量模型)</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__() <span class="comment"># 先初始化父类</span></span><br><span class="line">        <span class="comment"># 定义一个全连接网络，实现二分类</span></span><br><span class="line">        <span class="variable language_">self</span>.fc = torch.nn.Linear(<span class="number">768</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 执行一次前向计算</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_ids, attention_mask, token_type_ids</span>):</span><br><span class="line">        <span class="comment"># 冻结Bert模型的参数，使其不参与训练</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            out = pretrained(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 增量模型参与训练</span></span><br><span class="line">        out = <span class="variable language_">self</span>.fc(out.last_hidden_state[:,<span class="number">0</span>]) <span class="comment"># 取的是最后一层最后一个的输出，因为序列编码中最后一个输出包含了序列的所有信息</span></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>为什么用增量微调: 数据量少，没办法训练整个模型，设备差，增量微调训练起来快。</p>
<h5 id="3-3-模型训练"><a href="#3-3-模型训练" class="headerlink" title="3.3 模型训练"></a>3.3 模型训练</h5><p>定义<code>tain.py</code>文件，分为几个步骤，加载数据集，加载模型，训练和监控。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span>  torch</span><br><span class="line"><span class="keyword">from</span> MyData <span class="keyword">import</span> MyDataset</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span>  DataLoader</span><br><span class="line"><span class="keyword">from</span> net <span class="keyword">import</span>  Model</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, AdamW</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义设备</span></span><br><span class="line">DEVICE = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载分词器和词表</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">&#x27;./model/bert-base-chinese&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义训练轮数</span></span><br><span class="line">EPOCH = <span class="number">3000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将传入的字符串进行编码</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">data</span>):</span><br><span class="line">    sents = [i[<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> data]</span><br><span class="line">    label = [i[<span class="number">1</span>] <span class="keyword">for</span> i <span class="keyword">in</span> data]</span><br><span class="line">    <span class="comment"># 编码</span></span><br><span class="line">    data = tokenizer.batch_encode_plus(</span><br><span class="line">        batch_text_or_text_pairs=sents,  <span class="comment"># 要编码的句子</span></span><br><span class="line">        truncation=<span class="literal">True</span>,  <span class="comment"># 截断超过max_length的部分</span></span><br><span class="line">        max_length=<span class="number">512</span>,  <span class="comment"># 最大长度</span></span><br><span class="line">        padding=<span class="string">&quot;max_length&quot;</span>,  <span class="comment"># 是否填充, 用0填充</span></span><br><span class="line">        return_tensors=<span class="string">&quot;pt&quot;</span>,  <span class="comment"># 返回的数组是什么类型，tf(Tensorflow), pt(pytorch张量), np(numpy), None(list)</span></span><br><span class="line"></span><br><span class="line">        return_length=<span class="literal">True</span> <span class="comment"># 返回序列查长度</span></span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># 取出结果</span></span><br><span class="line">    input_ids = data[<span class="string">&#x27;input_ids&#x27;</span>]</span><br><span class="line">    attention_mask = data[<span class="string">&#x27;attention_mask&#x27;</span>]</span><br><span class="line">    token_type_ids = data[<span class="string">&#x27;token_type_ids&#x27;</span>]</span><br><span class="line">    label = torch.LongTensor(label)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> input_ids, attention_mask, token_type_ids, label</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建数据集</span></span><br><span class="line">train_dataset = MyDataset(<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">train_loader = DataLoader(</span><br><span class="line">    dataset=train_dataset,</span><br><span class="line">    batch_size=<span class="number">20</span>,</span><br><span class="line">    shuffle=<span class="literal">True</span>, <span class="comment"># 每个epoch打乱一次</span></span><br><span class="line">    drop_last=<span class="literal">True</span>, <span class="comment"># 舍弃最后一个批次的数据，防止形状出错, 可能剩下几个数据,</span></span><br><span class="line">    collate_fn=collate_fn <span class="comment"># 对加载的数据进行编码</span></span><br><span class="line"></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 定义模型</span></span><br><span class="line">    model = Model().to(DEVICE)</span><br><span class="line">    <span class="comment"># 定义优化器</span></span><br><span class="line">    optimizer = AdamW(model.parameters())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义损失函数，二分类用交叉熵</span></span><br><span class="line">    loss_func = torch.nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练EPOCH次</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCH):</span><br><span class="line">        <span class="comment"># 遍历整个训练集</span></span><br><span class="line">        <span class="keyword">for</span> i, (input_ids, attention_mask, token_type_ids, label) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">            <span class="comment"># 数据和模型放到同个设备上</span></span><br><span class="line">            input_ids, attention_mask, token_type_ids, label = input_ids.to(DEVICE), attention_mask.to(DEVICE), token_type_ids.to(DEVICE), label.to(DEVICE)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 前向计算</span></span><br><span class="line">            out = model(input_ids, attention_mask, token_type_ids)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 根据输出计算误差</span></span><br><span class="line">            loss = loss_func(out, label)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 后向更新</span></span><br><span class="line">            optimizer.zero_grad() <span class="comment"># 清空梯度</span></span><br><span class="line">            loss.backward() <span class="comment"># 计算梯度</span></span><br><span class="line">            optimizer.step() <span class="comment"># 更新参数</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">                out = out.argmax(dim=<span class="number">1</span>) <span class="comment"># 分类结果</span></span><br><span class="line">                acc = (out==label).<span class="built_in">sum</span>().item()/<span class="built_in">len</span>(label) <span class="comment">#</span></span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;epoch:<span class="subst">&#123;epoch&#125;</span>, i<span class="subst">&#123;i&#125;</span>, loss:<span class="subst">&#123;loss.item()&#125;</span>, acc:<span class="subst">&#123;acc&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 每轮保存一次参数</span></span><br><span class="line">        torch.save(model.state_dict(), <span class="string">f&quot;./params/<span class="subst">&#123;epoch&#125;</span>_bert.pth&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;参数保存成功&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>运行后结果如下:</p>
<p><img src="/2025/03/12/llm-application-3/image-20250311213747020.png" alt="image-20250311213747020"></p>
<h5 id="3-4-模型评估"><a href="#3-4-模型评估" class="headerlink" title="3.4 模型评估"></a>3.4 模型评估</h5><p>模型评估分为两种：客观评估和主观评估，客观评估是指在测试集上验证指标，比如在这个二分类任务中，指标可以用准确率。</p>
<h6 id="3-4-1-客观评估"><a href="#3-4-1-客观评估" class="headerlink" title="3.4.1 客观评估"></a>3.4.1 客观评估</h6><p>客观评估主要步骤为</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">graph LR</span><br><span class="line">A[加载测试集]--&gt;B[加载模型]--&gt;C[获得输出]--&gt;D[计算指标]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>注意：推理时候需要开启模型的推理模式，使用以下语句</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure>
<p>主要影响模型的<code>Batch Normalization</code>层和<code>Droupout</code>层，当开启推理模式时，这两个层会有不同的行为:</p>
<ul>
<li><code>Batch Normalization</code>层: 使用全局均值和方差，而不是当前batch的均值和方差。</li>
<li><code>Droupout</code>层：不会随机丢弃神经元</li>
</ul>
<p>同时，在推理的时候不需要后向传播，所以可以不计算梯度，从而节约显存，加速推理， 使用以下语句:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 前向计算, 每个样本是输出是一个二维向量,表示两个类别的概率</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad(): <span class="comment"># 关闭梯度计算</span></span><br><span class="line">    out = model(input_ids, attention_mask, token_type_ids)</span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<p>测试的具体代码如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span>  torch</span><br><span class="line"><span class="keyword">from</span> MyData <span class="keyword">import</span> MyDataset</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span>  DataLoader</span><br><span class="line"><span class="keyword">from</span> net <span class="keyword">import</span>  Model</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义设备</span></span><br><span class="line">DEVICE = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载分词器和词表</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">&#x27;./model/bert-base-chinese&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将传入的字符串进行编码</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">data</span>):</span><br><span class="line">    sents = [i[<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> data]</span><br><span class="line">    label = [i[<span class="number">1</span>] <span class="keyword">for</span> i <span class="keyword">in</span> data]</span><br><span class="line">    <span class="comment"># 编码</span></span><br><span class="line">    data = tokenizer.batch_encode_plus(</span><br><span class="line">        batch_text_or_text_pairs=sents,  <span class="comment"># 要编码的句子</span></span><br><span class="line">        truncation=<span class="literal">True</span>,  <span class="comment"># 截断超过max_length的部分</span></span><br><span class="line">        max_length=<span class="number">512</span>,  <span class="comment"># 最大长度</span></span><br><span class="line">        padding=<span class="string">&quot;max_length&quot;</span>,  <span class="comment"># 是否填充, 用0填充</span></span><br><span class="line">        return_tensors=<span class="string">&quot;pt&quot;</span>,  <span class="comment"># 返回的数组是什么类型，tf(Tensorflow), pt(pytorch张量), np(numpy), None(list)</span></span><br><span class="line"></span><br><span class="line">        return_length=<span class="literal">True</span> <span class="comment"># 返回序列查长度</span></span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># 取出结果</span></span><br><span class="line">    input_ids = data[<span class="string">&#x27;input_ids&#x27;</span>]</span><br><span class="line">    attention_mask = data[<span class="string">&#x27;attention_mask&#x27;</span>]</span><br><span class="line">    token_type_ids = data[<span class="string">&#x27;token_type_ids&#x27;</span>]</span><br><span class="line">    label = torch.LongTensor(label)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> input_ids, attention_mask, token_type_ids, label</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建数据集</span></span><br><span class="line">test_dataset = MyDataset(<span class="string">&#x27;test&#x27;</span>)</span><br><span class="line">test_loader = DataLoader(</span><br><span class="line">    dataset=test_dataset,</span><br><span class="line">    batch_size=<span class="number">20</span>,</span><br><span class="line">    shuffle=<span class="literal">True</span>, <span class="comment"># 每个epoch打乱一次</span></span><br><span class="line">    drop_last=<span class="literal">True</span>, <span class="comment"># 舍弃最后一个批次的数据，防止形状出错, 可能剩下几个数据,</span></span><br><span class="line">    collate_fn=collate_fn <span class="comment"># 对加载的数据进行编码</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 定义模型</span></span><br><span class="line">    model = Model().to(DEVICE)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加载参数</span></span><br><span class="line">    model.load_state_dict(torch.load(<span class="string">&quot;./params/7_bert.pth&quot;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="comment"># 只进行前向计算</span></span><br><span class="line">    acc = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i, (input_ids, attention_mask, token_type_ids, label) <span class="keyword">in</span> <span class="built_in">enumerate</span>(test_loader):</span><br><span class="line">        <span class="comment"># 数据和模型放到同个设备上</span></span><br><span class="line">        input_ids, attention_mask, token_type_ids, label = input_ids.to(DEVICE), attention_mask.to(</span><br><span class="line">            DEVICE), token_type_ids.to(DEVICE), label.to(DEVICE)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 前向计算, 每个样本是输出是一个二维向量,表示两个类别的概率</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad(): <span class="comment"># 关闭梯度计算</span></span><br><span class="line">            out = model(input_ids, attention_mask, token_type_ids)</span><br><span class="line">        <span class="comment"># print(out)</span></span><br><span class="line">        <span class="comment"># 取概率大的那个的作为label</span></span><br><span class="line">        out = out.argmax(dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 统计预测正确的样本数</span></span><br><span class="line">        acc += (out == label).<span class="built_in">sum</span>().item()</span><br><span class="line">        <span class="built_in">print</span>(i, (out == label).<span class="built_in">sum</span>().item())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;准确率为<span class="subst">&#123;acc/<span class="built_in">len</span>(test_loader)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h6 id="3-4-2-主观评估"><a href="#3-4-2-主观评估" class="headerlink" title="3.4.2 主观评估"></a>3.4.2 主观评估</h6><p>主观评估就是让用户输出字符串，然后直接判断该字符串的标签。具体代码如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span>  torch</span><br><span class="line"><span class="keyword">from</span> MyData <span class="keyword">import</span> MyDataset</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span>  DataLoader</span><br><span class="line"><span class="keyword">from</span> net <span class="keyword">import</span>  Model</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义设备</span></span><br><span class="line">DEVICE = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载分词器和词表</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">&#x27;./model/bert-base-chinese&#x27;</span>)</span><br><span class="line">model = Model().to(DEVICE)</span><br><span class="line">names = [<span class="string">&quot;负向评价&quot;</span>,<span class="string">&quot;正向评价&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将传入的字符串进行编码</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">data</span>):</span><br><span class="line">    sents = [data]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 编码</span></span><br><span class="line">    data = tokenizer.batch_encode_plus(</span><br><span class="line">        batch_text_or_text_pairs=sents,  <span class="comment"># 要编码的句子</span></span><br><span class="line">        truncation=<span class="literal">True</span>,  <span class="comment"># 截断超过max_length的部分</span></span><br><span class="line">        max_length=<span class="number">512</span>,  <span class="comment"># 最大长度</span></span><br><span class="line">        padding=<span class="string">&quot;max_length&quot;</span>,  <span class="comment"># 是否填充, 用0填充</span></span><br><span class="line">        return_tensors=<span class="string">&quot;pt&quot;</span>,  <span class="comment"># 返回的数组是什么类型，tf(Tensorflow), pt(pytorch张量), np(numpy), None(list)</span></span><br><span class="line"></span><br><span class="line">        return_length=<span class="literal">True</span> <span class="comment"># 返回序列查长度</span></span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># 取出结果</span></span><br><span class="line">    input_ids = data[<span class="string">&#x27;input_ids&#x27;</span>]</span><br><span class="line">    attention_mask = data[<span class="string">&#x27;attention_mask&#x27;</span>]</span><br><span class="line">    token_type_ids = data[<span class="string">&#x27;token_type_ids&#x27;</span>]</span><br><span class="line">    <span class="keyword">return</span> input_ids, attention_mask, token_type_ids</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 加载模型训练参数</span></span><br><span class="line">    model.load_state_dict(torch.load(<span class="string">&quot;params/7_bert.pth&quot;</span>))</span><br><span class="line">    <span class="comment"># 开启测试模型</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        data = <span class="built_in">input</span>(<span class="string">&quot;请输入测试数据(输入q退出):&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> data == <span class="string">&#x27;q&#x27;</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;测试结束&quot;</span>)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        input_ids, attention_mask, token_type_ids = collate_fn(data)</span><br><span class="line">        input_ids, attention_mask, token_type_ids = input_ids.to(DEVICE), attention_mask.to(DEVICE), token_type_ids.to(</span><br><span class="line">            DEVICE)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将输出输入到模型, 得到输出</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            out = model(input_ids, attention_mask, token_type_ids)</span><br><span class="line">            out = out.argmax(dim=<span class="number">1</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;模型判定结果为: <span class="subst">&#123;names[out]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="四-扩展到其他多分类任务上"><a href="#四-扩展到其他多分类任务上" class="headerlink" title="四. 扩展到其他多分类任务上"></a>四. 扩展到其他多分类任务上</h4><p>如果想扩展到多分类任务上，比如扩展到8分类任务上, 只需修改一些地方，比如修改模型的全连接层，在二分类时，我们的增量模型是一个输出为二维的向量，8分类时只需要改为输出为八维。修改net.py为:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span>  BertModel</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义设备</span></span><br><span class="line">DEVICE = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(DEVICE)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载预训练模型</span></span><br><span class="line">pretrained = BertModel.from_pretrained(<span class="string">&quot;./model/bert-base-chinese&quot;</span>).to(DEVICE)</span><br><span class="line"><span class="comment"># 打印模型结构</span></span><br><span class="line"><span class="comment"># print(pretrained)</span></span><br><span class="line"><span class="comment"># 定义下游任务(增量模型)</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__() <span class="comment"># 先初始化父类</span></span><br><span class="line">        <span class="comment"># 定义一个全连接网络，实现二分类</span></span><br><span class="line">        <span class="variable language_">self</span>.fc = torch.nn.Linear(<span class="number">768</span>, <span class="number">8</span>) <span class="comment"># 将输出改为8</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 执行一次前向计算</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_ids, attention_mask, token_type_ids</span>):</span><br><span class="line">        <span class="comment"># 冻结Bert模型的参数，使其不参与训练</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            out = pretrained(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 增量模型参与训练</span></span><br><span class="line">        out = <span class="variable language_">self</span>.fc(out.last_hidden_state[:,<span class="number">0</span>]) <span class="comment"># 取的是最后一层最后一个的输出，因为序列编码中最后一个输出包含了序列的所有信息</span></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>另外还需要修改数据类，MyData.py, 具体修改可以根绝数据集格式更改。</p>
<p>其他不用改变，可以直接训练或者测试。</p>
<h4 id="五-补充"><a href="#五-补充" class="headerlink" title="五. 补充"></a>五. 补充</h4><h5 id="4-1-训练的状态"><a href="#4-1-训练的状态" class="headerlink" title="4.1 训练的状态"></a>4.1 训练的状态</h5><p>Ai模型在训练过程中存在三种状态:</p>
<ul>
<li>欠拟合：模型的分布弱于数据真实分布(可能的原因：训练时间不够，模型过于简单)</li>
<li>拟合：模型的分布恰好能够表达数据的核心分布规律（训练的理想状态）</li>
<li>过拟合：模型过度拟合数据的分布规律，会使得模型的结果依赖于数据中的噪声信息，一旦数据发生细微变化，就可能会导致结果错误。</li>
</ul>
<p>如下图:</p>
<p> <img src="/2025/03/12/llm-application-3/image-20250312000625421.png" alt="image-20250312000625421" style="zoom:80%;"></p>
<p>处理方法:</p>
<ul>
<li>欠拟合：继续训练</li>
<li>过拟合：无法返回，通过在训练过程中不断保存checkpoints，出现过拟合时找到拟合点，即为最佳拟合点。</li>
</ul>
<p>最佳拟合点判断, 使用验证集，在训练过程中同时测试在验证集上的loss，当发现模型在训练集上loss仍在下降，但在验证集上loss上升时，说明模型出现过拟合。</p>
<h5 id="4-2-Batch-Normaliztion-BN-的原理和作用"><a href="#4-2-Batch-Normaliztion-BN-的原理和作用" class="headerlink" title="4.2 Batch Normaliztion(BN)的原理和作用"></a>4.2 Batch Normaliztion(BN)的原理和作用</h5><p>在深度神经网络中，随着 <strong>层数加深</strong>，神经元的输入分布可能会发生变化（即 <strong>内部协变量偏移，Internal Covariate Shift</strong>）。这会导致：<br> ✅ <strong>梯度消失或梯度爆炸</strong>，训练变得不稳定。<br> ✅ <strong>模型收敛速度慢</strong>，需要更小的学习率。<br> ✅ <strong>对初始化敏感</strong>，不同的权重初始化可能导致不同的训练结果。</p>
<p><strong>Batch Normalization 通过归一化（Normalization）和可训练参数（Scaling &amp; Shifting）来解决这些问题。</strong></p>
<p><strong>Batch Normalization 的作用</strong>：</p>
<p>✅ <strong>(1) 让训练更稳定</strong></p>
<ul>
<li>归一化可以防止 <strong>梯度消失/梯度爆炸</strong>，使得训练更加稳定。</li>
</ul>
<p>✅ <strong>(2) 提高收敛速度</strong></p>
<ul>
<li>BN 让数据分布更稳定，可以使用 <strong>更大的学习率（learning rate）</strong>，加速训练。</li>
</ul>
<p>✅ <strong>(3) 降低对权重初始化的依赖</strong></p>
<ul>
<li>由于 BN 让数据保持稳定分布，网络在不同初始化下表现更一致。</li>
</ul>
<p>✅ <strong>(4) 具有一定的正则化效果</strong></p>
<ul>
<li>由于 BN 依赖于 batch 统计信息，训练时会引入一定的噪声，类似 Dropout，能减少过拟合。</li>
</ul>
]]></content>
      <categories>
        <category>大模型应用</category>
        <category>微调与训练</category>
      </categories>
  </entry>
  <entry>
    <title>大模型应用系列(四) GPT2的调用和微调，以及数据集的制作</title>
    <url>/2025/03/13/llm-application-4/</url>
    <content><![CDATA[<p>在古诗词数据集上微调GPT2，介绍了如何调用GPT，如何制作数据集，以及如何微调。</p>
<span id="more"></span>
<h4 id="一-GPT2中文模型的推理调用"><a href="#一-GPT2中文模型的推理调用" class="headerlink" title="一. GPT2中文模型的推理调用"></a>一. GPT2中文模型的推理调用</h4><h5 id="1-1-下载GPT2模型"><a href="#1-1-下载GPT2模型" class="headerlink" title="1.1 下载GPT2模型"></a>1.1 下载GPT2模型</h5><p>具体下载方法见LLM应用第一篇</p>
<p>从HF-mirror下载<code>gpt2-chinese-cluecorpussmall</code></p>
<h5 id="1-2-使用transformers调用模型生成文本"><a href="#1-2-使用transformers调用模型生成文本" class="headerlink" title="1.2 使用transformers调用模型生成文本"></a>1.2 使用transformers调用模型生成文本</h5><p>主要步骤有: 加载模型和分词器，构建模型生成Pipeline, 调用模型生成文本。代码如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 中文白话文生成</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> GPT2LMHeadModel, BertTokenizer, TextGenerationPipeline</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载模型和分词器</span></span><br><span class="line"><span class="comment"># 模型路径要到config.json所在目录</span></span><br><span class="line">model = GPT2LMHeadModel.from_pretrained(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用Pipeline调用模型</span></span><br><span class="line">text_generator = TextGenerationPipeline(model, tokenizer, device=<span class="string">&#x27;cpu&#x27;</span>) <span class="comment"># 如果有cuda，则写cuda</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成文本</span></span><br><span class="line"><span class="comment"># max_length控制生成长度, do_sample=True表示进行随机采样，每次生成的结果都不一样，为False时,每次生成的结果都一样</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">    <span class="built_in">print</span>(text_generator(<span class="string">&quot;这是很久之前的事情了，&quot;</span>, max_length=<span class="number">100</span>, do_sample=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>
<p>运行结果如下：因为开启了do_sample， 所以三次生成的文本都不同。</p>
<p><img src="/2025/03/13/llm-application-4/image-20250313223231958.png" alt="image-20250313223231958"></p>
<h4 id="二-本地训练GPT2中文模型"><a href="#二-本地训练GPT2中文模型" class="headerlink" title="二. 本地训练GPT2中文模型"></a>二. 本地训练GPT2中文模型</h4><h5 id="2-1-准备数据"><a href="#2-1-准备数据" class="headerlink" title="2.1 准备数据"></a>2.1 准备数据</h5><p>准备想要GPT生成的语料数据，比如中文诗词，不需要太多的标注。</p>
<p>数据集以及代码可以在 <a href="https://download.csdn.net/download/weixin_46091520/90481401">微调GPT中文生成模型，生成古诗风格资源-CSDN文库</a>获取</p>
<h5 id="2-2-构造数据类"><a href="#2-2-构造数据类" class="headerlink" title="2.2 构造数据类"></a>2.2 构造数据类</h5><p>数据类构造如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(file=<span class="string">&#x27;./data/chinese_poems.txt&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> file:</span><br><span class="line">            lines = file.readlines()</span><br><span class="line"></span><br><span class="line">        lines = [i.strip() <span class="keyword">for</span> i <span class="keyword">in</span> lines]</span><br><span class="line">        <span class="variable language_">self</span>.lines = lines</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.lines)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, item</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.lines[item]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    dataset = MyDataset()</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> dataset:</span><br><span class="line">        <span class="built_in">print</span>(data)</span><br></pre></td></tr></table></figure>
<h5 id="2-3-训练模型"><a href="#2-3-训练模型" class="headerlink" title="2.3 训练模型"></a>2.3 训练模型</h5><p>train.py代码如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AdamW</span><br><span class="line"><span class="keyword">from</span> transformers.optimization <span class="keyword">import</span> get_scheduler</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span>  data <span class="keyword">import</span> MyDataset</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span>  AutoModelForCausalLM, AutoTokenizer</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化数据集</span></span><br><span class="line">dataset = MyDataset()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载模型和分词器</span></span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(<span class="string">&#x27;gpt2-chinese-cluecorpussmall&#x27;</span>)</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&#x27;gpt2-chinese-cluecorpussmall&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将传入的字符串进行编码</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">data</span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 编码</span></span><br><span class="line">    data = tokenizer.batch_encode_plus(</span><br><span class="line">        batch_text_or_text_pairs=data,  <span class="comment"># 要编码的句子</span></span><br><span class="line">        padding=<span class="literal">True</span>, <span class="comment"># 填充</span></span><br><span class="line">        truncation=<span class="literal">True</span>,  <span class="comment"># 截断超过max_length的部分</span></span><br><span class="line">        max_length=<span class="number">512</span>,  <span class="comment"># 最大长度</span></span><br><span class="line">        return_tensors=<span class="string">&quot;pt&quot;</span>,  <span class="comment"># 返回的数组是什么类型，tf(Tensorflow), pt(pytorch张量), np(numpy), None(list)</span></span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># 把输入复制为标签</span></span><br><span class="line">    data[<span class="string">&#x27;labels&#x27;</span>] = data[<span class="string">&#x27;input_ids&#x27;</span>].clone()</span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用dataLoader创建数据加载器, 用于批量加载数据</span></span><br><span class="line">data_loader = DataLoader(</span><br><span class="line">    dataset=dataset,</span><br><span class="line">    batch_size=<span class="number">2</span>,</span><br><span class="line">    shuffle=<span class="literal">True</span>,</span><br><span class="line">    drop_last=<span class="literal">True</span>,</span><br><span class="line">    collate_fn=collate_fn</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(data_loader))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>():</span><br><span class="line">    <span class="comment"># 训练参数</span></span><br><span class="line">    EPOCH = <span class="number">3000</span></span><br><span class="line">    <span class="keyword">global</span> model <span class="comment"># 使用全局变量</span></span><br><span class="line">    DEVICE = <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line">    model = model.to(DEVICE)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义优化器</span></span><br><span class="line">    optimizer = AdamW(model.parameters(), lr=<span class="number">2e-5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义学习率调度器</span></span><br><span class="line">    scheduler = get_scheduler(name=<span class="string">&#x27;linear&#x27;</span>,      <span class="comment"># 线性调度器</span></span><br><span class="line">                              num_warmup_steps=<span class="number">0</span>, <span class="comment"># 预测步数</span></span><br><span class="line">                              num_training_steps=<span class="built_in">len</span>(data_loader), <span class="comment"># 训练步数</span></span><br><span class="line">                              optimizer=optimizer)</span><br><span class="line">    model.train() <span class="comment"># 模型开启训练模式</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCH):</span><br><span class="line">        <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_loader): <span class="comment"># 遍历数据</span></span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> data.keys():</span><br><span class="line">                data[k] = data[k].to(DEVICE)</span><br><span class="line"></span><br><span class="line">            out = model(**data) <span class="comment"># 前向计算</span></span><br><span class="line">            loss = out[<span class="string">&#x27;loss&#x27;</span>] <span class="comment"># 获取损失</span></span><br><span class="line"></span><br><span class="line">            loss.backward() <span class="comment"># 后向传播</span></span><br><span class="line">            torch.nn.utils.clip_grad_norm_(model.parameters(), <span class="number">1.0</span>)  <span class="comment"># 梯度裁剪，防止梯度爆炸</span></span><br><span class="line">            optimizer.step() <span class="comment"># 更新模型参数</span></span><br><span class="line">            scheduler.step() <span class="comment"># 更新学习率</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            model.zero_grad()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">50</span> == <span class="number">0</span>:  <span class="comment"># 每隔50个批次打印一次信息</span></span><br><span class="line">                labels = data[<span class="string">&quot;labels&quot;</span>][:, <span class="number">1</span>:]  <span class="comment"># 获取真实标签，忽略&lt;bos&gt;标记</span></span><br><span class="line">                out = out[<span class="string">&quot;logits&quot;</span>].argmax(dim=<span class="number">2</span>)[:, :-<span class="number">1</span>]  <span class="comment"># 获取预测结果，忽略&lt;eos&gt;标记</span></span><br><span class="line"></span><br><span class="line">                select = labels != <span class="number">0</span>  <span class="comment"># 选择非填充的标签</span></span><br><span class="line">                labels = labels[select]  <span class="comment"># 应用选择</span></span><br><span class="line">                out = out[select]  <span class="comment"># 应用选择</span></span><br><span class="line">                <span class="keyword">del</span> select  <span class="comment"># 删除不再使用的select</span></span><br><span class="line">                <span class="comment"># 计算准确率</span></span><br><span class="line">                acc = (labels == out).<span class="built_in">sum</span>().item() / labels.numel()  <span class="comment"># 计算准确率的公式</span></span><br><span class="line">                lr = optimizer.state_dict()[<span class="string">&quot;param_groups&quot;</span>][<span class="number">0</span>][<span class="string">&#x27;lr&#x27;</span>]  <span class="comment"># 获取当前学习率</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># 打印训练信息</span></span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;epoch:<span class="subst">&#123;epoch&#125;</span>,batch:<span class="subst">&#123;i&#125;</span>,loss:<span class="subst">&#123;loss.item()&#125;</span>,lr:<span class="subst">&#123;lr&#125;</span>,acc:<span class="subst">&#123;acc&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 保存最后一轮模型参数</span></span><br><span class="line">        torch.save(model.state_dict(), <span class="string">&quot;params/net.pt&quot;</span>)  <span class="comment"># 保存模型参数到指定路径</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;权重保存成功！&quot;</span>)  <span class="comment"># 打印成功信息</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 当该脚本作为主程序运行时，调用训练函数</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    train()  <span class="comment"># 开始训练过程</span></span><br></pre></td></tr></table></figure>
<h4 id="三-补充"><a href="#三-补充" class="headerlink" title="三. 补充"></a>三. 补充</h4><h5 id="3-1-生成模型的训练数据"><a href="#3-1-生成模型的训练数据" class="headerlink" title="3.1 生成模型的训练数据"></a>3.1 生成模型的训练数据</h5><p>训练的数据不含标签, 只含文本, 文本就是标签，模型根据前面生成下一个字，然后把后面的文本作为标签，计算loss。</p>
]]></content>
      <categories>
        <category>大模型应用</category>
        <category>微调与训练</category>
      </categories>
  </entry>
  <entry>
    <title>大模型应用系列(五) 在云服务器上训练gpt2模型以及通过后处理控制大模型输出</title>
    <url>/2025/03/17/llm-application-5/</url>
    <content><![CDATA[<p>本地连接云服务器，在云服务器上训练模型，通过后处理控制模型的输出。</p>
<span id="more"></span>
<h4 id="一-购买并远程连接云服务器"><a href="#一-购买并远程连接云服务器" class="headerlink" title="一. 购买并远程连接云服务器"></a>一. 购买并远程连接云服务器</h4><h5 id="1-1-租服务器"><a href="#1-1-租服务器" class="headerlink" title="1.1 租服务器"></a>1.1 租服务器</h5><p>选在AutoDL平台， 注册后点击算力平台，租对应的服务器，这里我选择按时付费，RTX4090</p>
<p>也可以选择其他平台的服务器。</p>
<p><img src="/2025/03/17/llm-application-5/image-20250314235047571.png" alt="image-20250314235047571"></p>
<h5 id="1-2-远程连接"><a href="#1-2-远程连接" class="headerlink" title="1.2 远程连接"></a>1.2 远程连接</h5><p>使用vscode，下载插件Remote-SSH, 复制上面的ssh命令，远程连接服务器。</p>
<h5 id="1-3-训练GPT2"><a href="#1-3-训练GPT2" class="headerlink" title="1.3 训练GPT2"></a>1.3 训练GPT2</h5><p>将上一节的代码和模型复制到服务器，模型可以从hf-mirror重新下载，复制时直接复制到vscode的目录下就行(先在vscode打开与远程文件夹)，注意修改模型路径。以及根据显存情况修改batchsize, 最好修改到占用90%显存。查看显存可以使用<code>nvitop</code></p>
<p>使用以下命令下载</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pip install nvitop</span><br></pre></td></tr></table></figure>
<p>使用以下命令查看显存占用情况</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">nvitop</span><br></pre></td></tr></table></figure>
<p>直接运行train.py文件即可，但这样如果ssh连接中断，训练也会停止，用以下命令可以保证ssh连接中断训练也不停止</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">nohup python tarin.py &amp;</span><br></pre></td></tr></table></figure>
<p>会在目录下生成<code>nohup.out</code>文件用于保存终端输出。</p>
<p><img src="/2025/03/17/llm-application-5/image-20250315000032021.png" alt="image-20250315000032021"></p>
<h4 id="二-测试训练好的GPT2并进行后处理"><a href="#二-测试训练好的GPT2并进行后处理" class="headerlink" title="二. 测试训练好的GPT2并进行后处理"></a>二. 测试训练好的GPT2并进行后处理</h4><p>本次训练目标是训练一个能生成古诗分格的GPT2。我在服务器上训练了14个epoch，测试部分可以在本地进行，将训练好的参数下载到本地，通过以下代码测试。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 中文白话文生成</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> GPT2LMHeadModel, BertTokenizer, TextGenerationPipeline</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载模型和分词器</span></span><br><span class="line">model = GPT2LMHeadModel.from_pretrained(<span class="string">&#x27;gpt2-chinese-cluecorpussmall&#x27;</span>)</span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">&#x27;gpt2-chinese-cluecorpussmall&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载模型参数 map_location将参数和模型放到同个设备上</span></span><br><span class="line">model.load_state_dict(torch.load(<span class="string">&#x27;./params/epoch-14&#x27;</span>, map_location=<span class="string">&#x27;cpu&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用Pipeline调用模型</span></span><br><span class="line">text_generator = TextGenerationPipeline(model, tokenizer, device=<span class="string">&#x27;cpu&#x27;</span>) <span class="comment"># 如果有cuda，则写cuda</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成文本</span></span><br><span class="line"><span class="comment"># max_length控制生成长度, do_sample=True表示进行随机采样，每次生成的结果都不一样，为False时,每次生成的结果都一样</span></span><br><span class="line">text = text_generator(<span class="string">&quot;天高&quot;</span>, max_length=<span class="number">100</span>, do_sample=<span class="literal">True</span>, truncation=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(text[<span class="number">0</span>][<span class="string">&#x27;generated_text&#x27;</span>])</span><br></pre></td></tr></table></figure>
<p>输出结果如下，比如我们想输出四句诗，每个句子五个字，共以天高开头个字，(加上标点):</p>
<p><img src="/2025/03/17/llm-application-5/image-20250316232421499.png" alt="image-20250316232421499"></p>
<p>可以看出，和不训练相比(不加载模型参数，输出如下)，训练后的模型更接近诗词的形式。</p>
<p><img src="/2025/03/17/llm-application-5/image-20250316232446392.png" alt="image-20250316232446392"></p>
<p>但仍存在一些不足，比如我们原意是想输出每句诗五个字，共四句诗，但输出中有一些句子不是五个字，有一些特殊字符。</p>
<p>我们可以通过后处理使输出更符合我们要的形式。</p>
<h4 id="三-后处理"><a href="#三-后处理" class="headerlink" title="三. 后处理"></a>三. 后处理</h4><p>大模型只能按照上文推测下一个字符，但它不能严格控制输出格式，所以涉及格式时，我们需要通过后处理进行严格控制。我们需要重新定义生成函数，比如要生成五言绝句，则定义如下函数:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">generate</span>(<span class="params">text, row, col</span>):</span><br></pre></td></tr></table></figure>
<p>其中text是提示词, row是生成文本的行数, col是每行的字数，首先这个函数要定义为递归函数，因为我们不知道循环的次数，模型在生成过程中会生成一些不合格的输入，我们会抛弃，所以模型具体生成次数我们不知道。</p>
<p>具体逻辑是在生成过程中获取模型下一个输入的logit，然后根据格式将对应不合法的字符的概率设置为零。从而控制模型的输出。</p>
<p>具体代码如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 通过后处理控制生成格式，使其生成诗词形式。# 中文白话文生成</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> GPT2LMHeadModel, BertTokenizer, TextGenerationPipeline</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载模型和分词器</span></span><br><span class="line">model = GPT2LMHeadModel.from_pretrained(<span class="string">&#x27;gpt2-chinese-cluecorpussmall&#x27;</span>)</span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">&#x27;gpt2-chinese-cluecorpussmall&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载模型参数 map_location将参数和模型放到同个设备上</span></span><br><span class="line">model.load_state_dict(torch.load(<span class="string">&#x27;./params/epoch-14&#x27;</span>, map_location=<span class="string">&#x27;cpu&#x27;</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义生成函数, 用于生成五言绝句， text是提示词, row是生成文本的行数, col是每行的字数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate</span>(<span class="params">text, row, col</span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义一个内部递归函数, 用于生成文本</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generate_loop</span>(<span class="params">data</span>):</span><br><span class="line">        <span class="comment"># 关闭梯度计算, 加速推理</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="comment"># 使用data字典中的数据作为模型输入，得到输出</span></span><br><span class="line">            out = model(**data)</span><br><span class="line">        <span class="comment"># 获取最后一个字符的概率(logits未归一化的概率输出)</span></span><br><span class="line">        out = out[<span class="string">&#x27;logits&#x27;</span>]</span><br><span class="line">        <span class="comment"># 选择最后一个的logits， 即下个词对应的logits</span></span><br><span class="line">        out = out[:, -<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 找到概率前50的值, 以此为分界线, 小于该值的全部舍去，然后从top_k中随机采样，这样更有创造性(每次不一样)</span></span><br><span class="line">        topk_value = torch.topk(out, <span class="number">50</span>).values</span><br><span class="line">        <span class="comment"># 获取每个输出序列前50个最大的logits，(为保持维度不变，需要对结果增加一个维度，因为索引操作会降维)</span></span><br><span class="line">        topk_value = topk_value[:, -<span class="number">1</span>].unsqueeze(dim=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 将所有logit小于前50个的其他logit设置为负无穷，这样保持了输出形状，在选择的时候也不会选到这些值。</span></span><br><span class="line">        out = out.masked_fill(out &lt; topk_value, -<span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将特殊字符的logits设置为负无穷, 防止模型生成特殊字符</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="string">&quot;,.()《》【】&#123;&#125;&quot;</span>:</span><br><span class="line">            out[:, tokenizer.get_vocab()[i]] = -<span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 去除特殊符号</span></span><br><span class="line">        out[:, tokenizer.get_vocab()[<span class="string">&quot;[SEP]&quot;</span>]] = -<span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>)</span><br><span class="line">        out[:, tokenizer.get_vocab()[<span class="string">&quot;[UNK]&quot;</span>]] = -<span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>)</span><br><span class="line">        out[:, tokenizer.get_vocab()[<span class="string">&quot;[CLS]&quot;</span>]] = -<span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>)</span><br><span class="line">        <span class="comment"># 根据概率采样, 无放回采样, 避免重复生成内容</span></span><br><span class="line">        out = out.softmax(dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 从概率分布中进行采样，选择下一个词的ID</span></span><br><span class="line">        out = out.multinomial(num_samples=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 强制添加标点符号</span></span><br><span class="line">        <span class="comment"># 计算当前生成的文本长度和预期长度的比例</span></span><br><span class="line">        c = data[<span class="string">&#x27;input_ids&#x27;</span>].shape[<span class="number">1</span>] / (col + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果当前长度是预期长度的整数倍，则添加符号</span></span><br><span class="line">        <span class="keyword">if</span> c % <span class="number">1</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">if</span> c % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="comment"># 偶数位添加句号</span></span><br><span class="line">                out[:, <span class="number">0</span>] = tokenizer.get_vocab()[<span class="string">&quot;.&quot;</span>]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># 奇数位添加逗号</span></span><br><span class="line">                out[:, <span class="number">0</span>] = tokenizer.get_vocab()[<span class="string">&#x27;,&#x27;</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将生成的新词ID添加到输入序列的末尾</span></span><br><span class="line">        data[<span class="string">&#x27;input_ids&#x27;</span>] = torch.cat([data[<span class="string">&#x27;input_ids&#x27;</span>], out], dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更新注意力掩码, 标记所有位置</span></span><br><span class="line">        data[<span class="string">&#x27;attention_mask&#x27;</span>] = torch.ones_like(data[<span class="string">&#x27;input_ids&#x27;</span>])</span><br><span class="line">        <span class="comment"># 更新token前ID类型，通常在Bert中使用，但GPT不用</span></span><br><span class="line">        data[<span class="string">&#x27;token_type_ids&#x27;</span>] = torch.ones_like(data[<span class="string">&#x27;input_ids&#x27;</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment">#更新标签，将输入ID复制到标签中，用于预测下个token</span></span><br><span class="line">        data[<span class="string">&#x27;labels&#x27;</span>] = data[<span class="string">&#x27;input_ids&#x27;</span>].clone()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 检查生成的文本长度是否达到或超过指定的行数和列数</span></span><br><span class="line">        <span class="keyword">if</span> data[<span class="string">&#x27;input_ids&#x27;</span>].shape[<span class="number">1</span>] &gt;= row * col + row + <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 如果达到长度要求, 返回data</span></span><br><span class="line">            <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果没达到长度要求, 递归调用</span></span><br><span class="line">        <span class="keyword">return</span> generate_loop(data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 生成3首诗词</span></span><br><span class="line">    <span class="comment"># 使用tokenizer对输入文本进行编码，并重复3次生成3个样本。</span></span><br><span class="line">    data = tokenizer.batch_encode_plus([text] * <span class="number">3</span>,return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">    <span class="comment"># 移除编码后的序列中的最后一个token(结束符号)</span></span><br><span class="line">    data[<span class="string">&quot;input_ids&quot;</span>] = data[<span class="string">&quot;input_ids&quot;</span>][:,:-<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 创建一个与input_ids形状相同的全1张量，用于注意力掩码</span></span><br><span class="line">    data[<span class="string">&quot;attention_mask&quot;</span>] = torch.ones_like(data[<span class="string">&quot;input_ids&quot;</span>])</span><br><span class="line">    <span class="comment"># 创建一个与input_ids形状相同的全0张量，用于token类型ID</span></span><br><span class="line">    data[<span class="string">&quot;token_type_ids&quot;</span>] = torch.zeros_like(data[<span class="string">&quot;input_ids&quot;</span>])</span><br><span class="line">    <span class="comment"># 复制input_ids到labels，用于模型的目标</span></span><br><span class="line">    data[<span class="string">&#x27;labels&#x27;</span>] = data[<span class="string">&quot;input_ids&quot;</span>].clone()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 调用generate_loop函数开始生成文本</span></span><br><span class="line">    data = generate_loop(data)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 遍历生成的3个样本</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">        <span class="comment"># 打印输出样本索引和对应的解码后的文本</span></span><br><span class="line">        <span class="built_in">print</span>(i,tokenizer.decode(data[<span class="string">&quot;input_ids&quot;</span>][i]))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    generate(<span class="string">&quot;白&quot;</span>, row=<span class="number">4</span>, col=<span class="number">5</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>运行结果如下:</p>
<p><img src="/2025/03/17/llm-application-5/image-20250317001714886.png" alt="image-20250317001714886"></p>
<p>可以看到, 通过后处理, 三首古诗的格式都复合物五言绝句。</p>
]]></content>
      <categories>
        <category>大模型应用</category>
        <category>微调与训练</category>
      </categories>
  </entry>
  <entry>
    <title>大模型应用系列(六)  ollama,vllm,LMDeploy 部署大模型</title>
    <url>/2025/03/21/llm-application-6/</url>
    <content><![CDATA[<p>介绍如何从modelscope下载模型，以及三种常用的本地模型部署工具 ollama，vllm, LMDeploy 的下载，部署和调用方法。</p>
<span id="more"></span>
<h4 id="一-从modelscope下载模型"><a href="#一-从modelscope下载模型" class="headerlink" title="一. 从modelscope下载模型"></a>一. 从modelscope下载模型</h4><p>modelscope和huggingface类似，但魔塔社区是国内网站，下载比较快，但模型比huggingface少, 平时下载还是推荐从hf-mirror下载。</p>
<p><a href="https://www.modelscope.cn/">https://www.modelscope.cn/</a></p>
<p>下载方式和huggingface类似，首先安装modelscope</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pip install modelscope</span><br></pre></td></tr></table></figure>
<p>接着下载模型</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">modelscope download --model Qwen/QwQ-32B --local_dir ./dir</span><br></pre></td></tr></table></figure>
<p>另外，魔塔社区现在提供36小时的云服务器，可以白嫖，选择一个模型，点击下图中的Notebook快速开发</p>
<p><img src="/2025/03/21/llm-application-6/image-20250318213251684.png" alt="image-20250318213251684"></p>
<p>可以选择两种服务器，其中CPU服务器是无限的，新用户GPU服务器有36个小时。</p>
<p><img src="/2025/03/21/llm-application-6/image-20250318213353513.png" alt="image-20250318213353513"></p>
<p>选择合适的基础镜像，即可开机，但不能SSH远程连接。</p>
<p><img src="/2025/03/21/llm-application-6/image-20250318213939357.png" alt="image-20250318213939357"></p>
<p>可以看到，提供了一张A10GPU。</p>
<h4 id="二-本地调用Qwen模型"><a href="#二-本地调用Qwen模型" class="headerlink" title="二. 本地调用Qwen模型"></a>二. 本地调用Qwen模型</h4><p>和之前用GPT2模型类似，首先加载模型和分词器，接着封装信息，输入到模型获取输出。具体模型如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> modelscope <span class="keyword">import</span> AutoModelForCausalLM, AutoTokenizer</span><br><span class="line"></span><br><span class="line">model_path = <span class="string">&quot;./Qwen2.5-0.5B-Instruct&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载模型</span></span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(</span><br><span class="line">    model_path,</span><br><span class="line">    torch_dtype=<span class="string">&quot;auto&quot;</span>,</span><br><span class="line">    device_map=<span class="string">&quot;auto&quot;</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># 加载分词器</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_path)</span><br><span class="line"></span><br><span class="line">prompt = <span class="string">&quot;讲一个猫和老鼠的故事&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入消息格式</span></span><br><span class="line">messages = [</span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: prompt&#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将消息转化为对话模板</span></span><br><span class="line">text = tokenizer.apply_chat_template(</span><br><span class="line">    messages,</span><br><span class="line">    tokenize=<span class="literal">False</span>,</span><br><span class="line">    add_generation_prompt=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将处理后的文本令牌化并转换为模型的输出向量</span></span><br><span class="line">model_inputs = tokenizer([text], return_tensors=<span class="string">&quot;pt&quot;</span>).to(model.device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据输入到模型得到输出</span></span><br><span class="line">generated_ids = model.generate(</span><br><span class="line">    **model_inputs,</span><br><span class="line">    max_new_tokens=<span class="number">32768</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">generated_ids = [</span><br><span class="line">    output_ids[<span class="built_in">len</span>(input_ids):] <span class="keyword">for</span> input_ids, output_ids <span class="keyword">in</span> <span class="built_in">zip</span>(model_inputs.input_ids, generated_ids)</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对输出的内容进行解码还原</span></span><br><span class="line">response = tokenizer.batch_decode(generated_ids, skip_special_tokens=<span class="literal">True</span>)[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(response)</span><br></pre></td></tr></table></figure>
<p>运行后输出如下:</p>
<p> <img src="/2025/03/21/llm-application-6/image-20250318222647855.png" alt="image-20250318222647855"></p>
<h4 id="三-Ollama-部署Qwen"><a href="#三-Ollama-部署Qwen" class="headerlink" title="三. Ollama 部署Qwen"></a>三. Ollama 部署Qwen</h4><p>我的环境是PyTorch 2.5.1</p>
<p>Python 3.12(ubuntu22.04)</p>
<p>CUDA 12.4</p>
<p>ollama是一款轻便的部署框架，适合个人使用，优先是使用方便，缺点是功能简单，性能较差。</p>
<h5 id="3-1-下载Ollama"><a href="#3-1-下载Ollama" class="headerlink" title="3.1 下载Ollama"></a>3.1 下载Ollama</h5><p>进入以下官网，点击down，直接下载，有时下载很慢甚至连接失败，要多次尝试。</p>
<p><a href="https://ollama.com/">Ollama</a></p>
<p><img src="/2025/03/21/llm-application-6/image-20250318223311005.png" alt="image-20250318223311005"></p>
<p>使用以下命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">curl -fsSL https://ollama.org.cn/install.sh | sh</span><br></pre></td></tr></table></figure>
<h5 id="3-2-运行ollama"><a href="#3-2-运行ollama" class="headerlink" title="3.2 运行ollama"></a>3.2 运行ollama</h5><p>安装完成后，需要启动ollama</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ollama serve</span><br></pre></td></tr></table></figure>
<p><img src="/2025/03/21/llm-application-6/image-20250320004301818.png" alt="image-20250320004301818"></p>
<p>可以看到，ollama服务的端口和IP为: <code>127.0.0.1:11434</code></p>
<p>所以启动前要保证这个端口不被占用。</p>
<p>服务开启后，该终端不能关闭，新开终端。输入命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ollama list</span><br></pre></td></tr></table></figure>
<p>查看ollama保存的模型</p>
<p><img src="/2025/03/21/llm-application-6/image-20250320004534360.png" alt="image-20250320004534360"></p>
<p>可以看到，目前没有模型，到ollama官网搜索想要的模型</p>
<h6 id="3-2-1-直接运行"><a href="#3-2-1-直接运行" class="headerlink" title="3.2.1 直接运行"></a>3.2.1 直接运行</h6><p><img src="/2025/03/21/llm-application-6/image-20250320004714284.png" alt="image-20250320004714284" style="zoom:80%;"></p>
<p>比如搜索Qwen2.5-0.5B,复制对应的指令</p>
<p> <img src="/2025/03/21/llm-application-6/image-20250320005004767.png" alt="image-20250320005004767" style="zoom:80%;"></p>
<p>输入<code>ollama run qwen2.5:0.5b</code> 后会从ollama官网拉取模型并运行。</p>
<p><img src="/2025/03/21/llm-application-6/image-20250320005329620.png" alt="image-20250320005329620"></p>
<p>拉取后会直接运行，可以和大模型对话，输入<code>/bye</code>可以终止。输入</p>
<p><code>ollama list</code> 查看模型，可以看到已经下载的模型</p>
<p><img src="/2025/03/21/llm-application-6/image-20250320010502719.png" alt="image-20250320010502719"></p>
<h6 id="3-2-2-使用OpenAI的API风格调用"><a href="#3-2-2-使用OpenAI的API风格调用" class="headerlink" title="3.2.2 使用OpenAI的API风格调用"></a>3.2.2 使用OpenAI的API风格调用</h6><p>单论对话代码如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> openai <span class="keyword">import</span> OpenAI</span><br><span class="line">client = OpenAI(</span><br><span class="line">    base_url=<span class="string">&quot;http://localhost:8000/v1&quot;</span>,</span><br><span class="line">    api_key=<span class="string">&quot;token-abc123&quot;</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">completion = client.chat.completions.create(</span><br><span class="line">  model=<span class="string">&quot;/root/autodl-tmp/Qwen2.5-0.5B-Instruct&quot;</span>,</span><br><span class="line">  messages=[</span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;讲一个猫和老鼠的故事&quot;</span>&#125;</span><br><span class="line">  ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(completion.choices[<span class="number">0</span>].message.content)</span><br></pre></td></tr></table></figure>
<p>多轮对话代码如下, 单轮可以照着改。要注意model_name和<code>ollama list</code> 查到的模型名字一样，端口是上面ollama服务的端口和地址，这里是<code>127.0.0.1:11434</code>, api_key 随便写</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#多轮对话</span></span><br><span class="line"><span class="keyword">from</span> openai <span class="keyword">import</span> OpenAI</span><br><span class="line"></span><br><span class="line">base_url = <span class="string">&quot;http://localhost:11434/v1/&quot;</span></span><br><span class="line">api_key = <span class="string">&quot;suibianxie&quot;</span></span><br><span class="line">model_name = <span class="string">&quot;qwen2.5:0.5b&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#定义多轮对话方法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_chat_session</span>():</span><br><span class="line">    <span class="comment">#初始化客户端</span></span><br><span class="line">    client = OpenAI(base_url=base_url,api_key=api_key)</span><br><span class="line">    <span class="comment">#初始化对话历史</span></span><br><span class="line">    chat_history = []</span><br><span class="line">    <span class="comment">#启动对话循环</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="comment">#获取用户输入</span></span><br><span class="line">        user_input = <span class="built_in">input</span>(<span class="string">&quot;用户：&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> user_input.lower() == <span class="string">&quot;exit&quot;</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;退出对话。&quot;</span>)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="comment">#更新对话历史(添加用户输入)</span></span><br><span class="line">        chat_history.append(&#123;<span class="string">&quot;role&quot;</span>:<span class="string">&quot;user&quot;</span>,<span class="string">&quot;content&quot;</span>:user_input&#125;)</span><br><span class="line">        <span class="comment">#调用模型回答</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            chat_complition = client.chat.completions.create(messages=chat_history,model=model_name)</span><br><span class="line">            <span class="comment">#获取最新回答</span></span><br><span class="line">            model_response = chat_complition.choices[<span class="number">0</span>]</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;AI:&quot;</span>,model_response.message.content)</span><br><span class="line">            <span class="comment">#更新对话历史（添加AI模型的回复）</span></span><br><span class="line">            chat_history.append(&#123;<span class="string">&quot;role&quot;</span>:<span class="string">&quot;assistant&quot;</span>,<span class="string">&quot;content&quot;</span>:model_response.message.content&#125;)</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;发生错误：&quot;</span>,e)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    run_chat_session()</span><br></pre></td></tr></table></figure>
<p>运行该文件后即可和大模型进行多轮对话</p>
<p><img src="/2025/03/21/llm-application-6/image-20250320011506320.png" alt="image-20250320011506320"></p>
<h5 id="3-3-需要注意的问题"><a href="#3-3-需要注意的问题" class="headerlink" title="3.3 需要注意的问题"></a>3.3 需要注意的问题</h5><ol>
<li>Ollama目前只支持GGUF格式，和其他框架不同，所以如果我们要先从huggingface上下载模型，需要下载GGUF格式。</li>
<li>GGUF格式是被量化后的模型，但是Ollama也能运行没量化的模型，具体方法后面补充。</li>
</ol>
<h4 id="四-vllm部署Qwen"><a href="#四-vllm部署Qwen" class="headerlink" title="四. vllm部署Qwen"></a>四. vllm部署Qwen</h4><h5 id="4-1-安装vllm"><a href="#4-1-安装vllm" class="headerlink" title="4.1 安装vllm"></a>4.1 安装vllm</h5><p>官方文档: </p>
<p><a href="https://vllm.hyper.ai/docs/getting-started/quickstart/">快速入门 | vLLM 中文站</a></p>
<p>vllm只支持cuda12.4 和cuda11.8, 安装的时候如果cuda版本不符合会重新下载12.4的版本，所以要使用新的conda环境进行安装。</p>
<p>安装命令如下:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda create -n vllmEnv</span><br><span class="line">conda activate vllmEnv</span><br><span class="line">pip install vllm</span><br></pre></td></tr></table></figure>
<h5 id="4-2-开启vllm服务器"><a href="#4-2-开启vllm服务器" class="headerlink" title="4.2 开启vllm服务器"></a>4.2 开启vllm服务器</h5><p>使用如下命令,  其中 ./Qwen2.5-0.5B-Instruct为模型命令，最好用绝对路径。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vllm serve /mnt/workspace/Qwen2.5-0.5B-Instruct</span><br></pre></td></tr></table></figure>
<p>开启成功后显示如下:</p>
<p><img src="/2025/03/21/llm-application-6/image-20250318225726971.png" alt="image-20250318225726971"></p>
<h5 id="4-3-调用模型"><a href="#4-3-调用模型" class="headerlink" title="4.3 调用模型"></a>4.3 调用模型</h5><p>vllm支持OpenAI格式调用，可以通过如下代码调用模型</p>
<p><strong>注意：</strong>代码中api_key在实际引用中要和启动服务时的<code>--api_key</code>参数一致，在本地部署测试中可以随便写，但一定要有。另外，model和启动服务时指定的模型路径一致。</p>
<p>模型路径要用绝对路径，可以使用命令<code>pwd</code>查看当前根目录路径。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> openai <span class="keyword">import</span> OpenAI</span><br><span class="line">client = OpenAI(</span><br><span class="line">    base_url=<span class="string">&quot;http://localhost:8000/v1&quot;</span>, <span class="comment"># 模型部署在本地</span></span><br><span class="line">    api_key=<span class="string">&quot;token-abc123&quot;</span>, <span class="comment"># 随便写</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">completion = client.chat.completions.create(</span><br><span class="line">  model=<span class="string">&quot;/mnt/workspace/Qwen2.5-0.5B-Instruct&quot;</span>, <span class="comment"># 模型路径，要用绝对路径</span></span><br><span class="line">  messages=[</span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;讲一个猫和老鼠的故事&quot;</span>&#125;</span><br><span class="line">  ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(completion.choices[<span class="number">0</span>].message.content)</span><br></pre></td></tr></table></figure>
<p>服务器的终端不能关闭，重新开一个终端，运行上述代码。</p>
<p>运行后输出如下:</p>
<p><img src="/2025/03/21/llm-application-6/image-20250318225619712.png" alt="image-20250318225619712"></p>
<p>多轮对话是把每轮的输入输出加入到message中。示例代码如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#多轮对话</span></span><br><span class="line"><span class="keyword">from</span> openai <span class="keyword">import</span> OpenAI</span><br><span class="line"></span><br><span class="line">base_url = <span class="string">&quot;http://localhost:8000/v1/&quot;</span></span><br><span class="line">api_key = <span class="string">&quot;suibianxie&quot;</span></span><br><span class="line">model_name = <span class="string">&quot;/mnt/workspace/Qwen2.5-0.5B-Instruct&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#定义多轮对话方法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_chat_session</span>():</span><br><span class="line">    <span class="comment">#初始化客户端</span></span><br><span class="line">    client = OpenAI(base_url=base_url,api_key=api_key)</span><br><span class="line">    <span class="comment">#初始化对话历史</span></span><br><span class="line">    chat_history = []</span><br><span class="line">    <span class="comment">#启动对话循环</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="comment">#获取用户输入</span></span><br><span class="line">        user_input = <span class="built_in">input</span>(<span class="string">&quot;用户：&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> user_input.lower() == <span class="string">&quot;exit&quot;</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;退出对话。&quot;</span>)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="comment">#更新对话历史(添加用户输入)</span></span><br><span class="line">        chat_history.append(&#123;<span class="string">&quot;role&quot;</span>:<span class="string">&quot;user&quot;</span>,<span class="string">&quot;content&quot;</span>:user_input&#125;)</span><br><span class="line">        <span class="comment">#调用模型回答</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            chat_complition = client.chat.completions.create(messages=chat_history,model=model_name)</span><br><span class="line">            <span class="comment">#获取最新回答</span></span><br><span class="line">            model_response = chat_complition.choices[<span class="number">0</span>]</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;AI:&quot;</span>,model_response.message.content)</span><br><span class="line">            <span class="comment">#更新对话历史（添加AI模型的回复）</span></span><br><span class="line">            chat_history.append(&#123;<span class="string">&quot;role&quot;</span>:<span class="string">&quot;assistant&quot;</span>,<span class="string">&quot;content&quot;</span>:model_response.message.content&#125;)</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;发生错误：&quot;</span>,e)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    run_chat_session()</span><br></pre></td></tr></table></figure>
<p>运行结果如下图:</p>
<p> <img src="/2025/03/21/llm-application-6/image-20250318231857684.png" alt="image-20250318231857684"></p>
<h4 id="五-LMDeploy-部署Qwen"><a href="#五-LMDeploy-部署Qwen" class="headerlink" title="五. LMDeploy 部署Qwen"></a>五. LMDeploy 部署Qwen</h4><p>官网: <a href="https://lmdeploy.readthedocs.io/zh-cn/latest/get_started/installation.html">安装 — lmdeploy</a></p>
<h5 id="5-1-安装LMDeploy"><a href="#5-1-安装LMDeploy" class="headerlink" title="5.1 安装LMDeploy"></a>5.1 安装LMDeploy</h5><p>首先创建新的虚拟环境，我创建的版本是 LMDeploy支持的python版本是3.8-3.12， 支持的cuda是12+以上，11+也可以，我的版本是cuda12.4</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda create -n lmdeployEnv python=3.8 -y</span><br><span class="line">conda activate lmdeployEnv</span><br><span class="line">pip install lmdeploy</span><br></pre></td></tr></table></figure>
<p>从hf-mirror下载模型，比如我下载了qwen2.5-0.5B-Instruct，使用</p>
<h5 id="5-2-启动服务"><a href="#5-2-启动服务" class="headerlink" title="5.2 启动服务"></a>5.2 启动服务</h5><p>lmdeploy部署命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">lmdeploy serve api_server /root/autodl-tmp/Qwen2.5-0.5B-Instruct --server-port 8000</span><br></pre></td></tr></table></figure>
<p>启动成功: 注意，模型使用绝对路径，避免出错。</p>
<p><img src="/2025/03/21/llm-application-6/image-20250321012852828.png" alt="image-20250321012852828"></p>
<p>可以看到，模型部署在<code>http://0.0.0.0:8000</code> 上，使用openai接口可以直接访问，</p>
<h5 id="5-3-调用模型"><a href="#5-3-调用模型" class="headerlink" title="5.3 调用模型"></a>5.3 调用模型</h5><p>单轮对话代码:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> openai <span class="keyword">import</span> OpenAI</span><br><span class="line">client = OpenAI(</span><br><span class="line">    base_url=<span class="string">&quot;http://localhost:8000/v1&quot;</span>, <span class="comment"># 模型部署在本地</span></span><br><span class="line">    api_key=<span class="string">&quot;token-abc123&quot;</span>, <span class="comment"># 随便写</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">completion = client.chat.completions.create(</span><br><span class="line">  model=<span class="string">&quot;/root/autodl-tmp/Qwen2.5-0.5B-Instruct&quot;</span>, <span class="comment"># 模型路径，要用绝对路径</span></span><br><span class="line">  messages=[</span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;讲一个猫和老鼠的故事&quot;</span>&#125;</span><br><span class="line">  ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(completion.choices[<span class="number">0</span>].message.content)</span><br></pre></td></tr></table></figure>
<p>结果如下:</p>
<p><img src="/2025/03/21/llm-application-6/image-20250321013718366.png" alt="image-20250321013718366"></p>
<p>多轮对话代码如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#多轮对话</span></span><br><span class="line"><span class="keyword">from</span> openai <span class="keyword">import</span> OpenAI</span><br><span class="line"></span><br><span class="line">base_url = <span class="string">&quot;http://localhost:8000/v1/&quot;</span></span><br><span class="line">api_key = <span class="string">&quot;suibianxie&quot;</span></span><br><span class="line">model_name = <span class="string">&quot;/root/autodl-tmp/Qwen2.5-0.5B-Instruct&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#定义多轮对话方法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_chat_session</span>():</span><br><span class="line">    <span class="comment">#初始化客户端</span></span><br><span class="line">    client = OpenAI(base_url=base_url,api_key=api_key)</span><br><span class="line">    <span class="comment">#初始化对话历史</span></span><br><span class="line">    chat_history = []</span><br><span class="line">    <span class="comment">#启动对话循环</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="comment">#获取用户输入</span></span><br><span class="line">        user_input = <span class="built_in">input</span>(<span class="string">&quot;用户：&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> user_input.lower() == <span class="string">&quot;exit&quot;</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;退出对话。&quot;</span>)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="comment">#更新对话历史(添加用户输入)</span></span><br><span class="line">        chat_history.append(&#123;<span class="string">&quot;role&quot;</span>:<span class="string">&quot;user&quot;</span>,<span class="string">&quot;content&quot;</span>:user_input&#125;)</span><br><span class="line">        <span class="comment">#调用模型回答</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            chat_complition = client.chat.completions.create(messages=chat_history,model=model_name)</span><br><span class="line">            <span class="comment">#获取最新回答</span></span><br><span class="line">            model_response = chat_complition.choices[<span class="number">0</span>]</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;AI:&quot;</span>,model_response.message.content)</span><br><span class="line">            <span class="comment">#更新对话历史（添加AI模型的回复）</span></span><br><span class="line">            chat_history.append(&#123;<span class="string">&quot;role&quot;</span>:<span class="string">&quot;assistant&quot;</span>,<span class="string">&quot;content&quot;</span>:model_response.message.content&#125;)</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;发生错误：&quot;</span>,e)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    run_chat_session()</span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<p><img src="/2025/03/21/llm-application-6/image-20250321013857206.png" alt="image-20250321013857206"></p>
<h4 id="六-其他问题"><a href="#六-其他问题" class="headerlink" title="六. 其他问题"></a>六. 其他问题</h4><h5 id="6-1-移动anaconda位置"><a href="#6-1-移动anaconda位置" class="headerlink" title="6.1 移动anaconda位置"></a>6.1 移动anaconda位置</h5><p>AutoDL 给的系统盘只有30G，数据盘有50G，如果安装太多环境，会导致系统跑满了，可以把Anaconda移动到数据盘。找到根目录下的miniconda文件夹，右键复制路径.</p>
<p>先把文件夹挪过去:</p>
<p>/root/miniconda3是原路径，/root/autodl-tmp/是移动目的目录，会自动再创建要给miniconda3目录</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mv /root/miniconda3 /root/autodl-tmp/miniconda3</span><br></pre></td></tr></table></figure>
<p>然后创建软链接，相当于快捷方式</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ln -s /root/autodl-tmp/miniconda3 /root/miniconda3</span><br></pre></td></tr></table></figure>
<h5 id="6-2-查看当前目录绝对路径"><a href="#6-2-查看当前目录绝对路径" class="headerlink" title="6.2 查看当前目录绝对路径"></a>6.2 查看当前目录绝对路径</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pwd</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大模型应用</category>
        <category>部署</category>
      </categories>
  </entry>
  <entry>
    <title>大模型应用系列(七)  LLama Factory和OpenWebui的安装和使用</title>
    <url>/2025/03/30/llm-application-7/</url>
    <content><![CDATA[<p>详细介绍了LLamaFactory和OpenWebui的安装和使用, 以及如何在LLamaFactory上进行推理，微调。以及如何通过脚本对模型进行Lora微调。最后解释量化原理。</p>
<span id="more"></span>
<h4 id="一-LLama-Factory的安装"><a href="#一-LLama-Factory的安装" class="headerlink" title="一. LLama Factory的安装"></a>一. LLama Factory的安装</h4><p>我的服务器配置为:</p>
<p>RTX 4090, cuda 12.4 python=3.10 pytorch=2.5.1</p>
<p>本文涉及的所有模型，数据集下载方法参考:</p>
<p><a href="https://codezrq.github.io/2025/03/06/llm-application-2/">Huggingface的安装和使用 | CodeRQ</a></p>
<p>参考官网资料即可：</p>
<p><a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/README_zh.md#安装-llama-factory">LLaMA-Factory/README_zh.md at main · hiyouga/LLaMA-Factory · GitHub</a></p>
<p>安装命令，首先拉去llamaFactory代码仓，然后进入代码仓目录，即LLamA-Factory， 接着执行安装指令。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git</span><br><span class="line">cd LLaMA-Factory</span><br><span class="line">pip install -e . # 官网要装其他包，但建议先装基础就行，其他的运行时缺啥补啥就行。</span><br></pre></td></tr></table></figure>
<p>可能网络问题连不上git，可以在本地拉取后上传到服务器。</p>
<p>官网给出了使用的三种基本操作：LoRA微调，推理，合并</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml</span><br><span class="line">llamafactory-cli chat examples/inference/llama3_lora_sft.yaml</span><br><span class="line">llamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml</span><br></pre></td></tr></table></figure>
<p>但建议直接用可视化界面，监控方便一些。启动界面的时候路径一定要在 <code>LLaMA-Factory</code>中</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">llamafactory-cli webui</span><br></pre></td></tr></table></figure>
<p>但是这个界面是在服务器上启动的，本地理论上是访问不到的，如果用vscode，它自带端口转发，会将服务器上的端口转发到本地，可以直接访问。</p>
<p><img src="/2025/03/30/llm-application-7/image-20250322085717610.png" alt="image-20250322085717610"></p>
<p>启动后界面如下：</p>
<p><img src="/2025/03/30/llm-application-7/image-20250322085822264.png" alt="image-20250322085822264"></p>
<h4 id="二-使用LLamaFacory进行LoRA微调"><a href="#二-使用LLamaFacory进行LoRA微调" class="headerlink" title="二. 使用LLamaFacory进行LoRA微调"></a>二. 使用LLamaFacory进行LoRA微调</h4><div class="table-container">
<table>
<thead>
<tr>
<th>模型</th>
<th>数据集</th>
</tr>
</thead>
<tbody>
<tr>
<td>Qwen2.5-0.5B-Instruct</td>
<td>LLamaFactory自带数据集identity.json</td>
</tr>
</tbody>
</table>
</div>
<h5 id="2-1-准备数据集"><a href="#2-1-准备数据集" class="headerlink" title="2.1 准备数据集"></a>2.1 准备数据集</h5><p>LLamaFactory在文件夹<code>./LLamA-Factory/data/</code>下给出了一些实例数据集，选择其中的<code>identity.json</code>作为微调示例。示例数据如下，将其中的<code>&#123;&#123;name&#125;&#125;,&#123;&#123;author&#125;&#125;</code>替换成对应的数据即可。</p>
<p><img src="/2025/03/30/llm-application-7/image-20250322090324378.png" alt="image-20250322090324378"></p>
<p>我修改为如下内容：</p>
<p><img src="/2025/03/30/llm-application-7/image-20250322090639811.png" alt="image-20250322090639811"></p>
<p>如果是自己准备的数据集，也要放大data目录下，同时在其中的dataset_info.json文件中配置自己数据集的信息，这里使用的是示例数据集，因此已经配置好，配置完在UI界面才可以找到该数据集。</p>
<p><img src="/2025/03/30/llm-application-7/image-20250322091121652.png" alt="image-20250322091121652"></p>
<h5 id="2-2-准备模型"><a href="#2-2-准备模型" class="headerlink" title="2.2 准备模型"></a>2.2 准备模型</h5><p>从huggingface上下载<code>qwen2.5-0.5B-Instruct</code>，具体方法见前文。</p>
<h5 id="2-3-在UI界面上配置模型和数据，开始LoRA微调"><a href="#2-3-在UI界面上配置模型和数据，开始LoRA微调" class="headerlink" title="2.3 在UI界面上配置模型和数据，开始LoRA微调"></a>2.3 在UI界面上配置模型和数据，开始LoRA微调</h5><p>配置如下:</p>
<p><img src="/2025/03/30/llm-application-7/image-20250322094406107.png" alt="image-20250322094406107"></p>
<p>点击开始按钮，开始训练，同时开启新的终端监测GPU使用情况，检测方法前文说过，</p>
<p><img src="/2025/03/30/llm-application-7/image-20250322094557848.png" alt="image-20250322094557848"></p>
<p>这里显存利用率较低，可以增加batch-size,一般怎加到利用率为90%为最佳，我改为50.</p>
<p>如果点击中断后重新开始，会报错（报错要看后台）：</p>
<p><img src="/2025/03/30/llm-application-7/image-20250322095440471.png" alt="image-20250322095440471"></p>
<p>这是因为训练结果的保存目录已经创建，它默认不会覆盖，这个目录在LLamaFactory下的saves文件加中，将它里面的东西删除即可。（删到只剩saves）也可以改一下输出目录的名称：</p>
<p><img src="/2025/03/30/llm-application-7/image-20250322101243548.png" alt="image-20250322101243548"></p>
<p>但是发现个问题，AutoDL上训练如果batch-size开太大有问题，显存在动但训练进度不动。我发现设置为10就可以，但显存也没满。AutoDL的毛病，其他服务器应该就不会。</p>
<p>重新点击开始。后台看到训练在进行：</p>
<p><img src="/2025/03/30/llm-application-7/image-20250322100441051.png" alt="image-20250322100441051"></p>
<p>界面中会给出损失曲线：</p>
<p><img src="/2025/03/30/llm-application-7/image-20250322100524476.png" alt="image-20250322100524476"></p>
<p>浅蓝色的是实际的loss曲线，深蓝色是loss曲线平滑后的结果，便于观察趋势，当loss曲线趋于平滑，即变化不大，就可以终止训练了。</p>
<p>训练的权重会保存在saves文件夹下，默认每100个epoch保存一次。</p>
<p>如果中断后想从上次训练的checkpoints继续训练，则加载模型时加载对应的LoRA参数，同时修改输出目录，点击开始即可。</p>
<p><img src="/2025/03/30/llm-application-7/image-20250322101553974.png" alt="image-20250322101553974"></p>
<p><img src="/2025/03/30/llm-application-7/image-20250322101623058.png" alt="image-20250322101623058"></p>
<p>可以看到，一开始loss就很小，说明是从上次的检查点开始的</p>
<p><img src="/2025/03/30/llm-application-7/image-20250322101913757.png" alt="image-20250322101913757"></p>
<h4 id="三-测试训练效果"><a href="#三-测试训练效果" class="headerlink" title="三. 测试训练效果"></a>三. 测试训练效果</h4><p>使用LLamaFactory的chat功能</p>
<h5 id="3-1首先加载原模型："><a href="#3-1首先加载原模型：" class="headerlink" title="3.1首先加载原模型："></a>3.1首先加载原模型：</h5><p><img src="/2025/03/30/llm-application-7/image-20250322102441651.png" alt="image-20250322102441651"></p>
<p>可以直接在下面对话款对话，结果如下:</p>
<p><img src="/2025/03/30/llm-application-7/image-20250322102507513.png" alt="image-20250322102507513"></p>
<h5 id="3-2-加载训练后的模型"><a href="#3-2-加载训练后的模型" class="headerlink" title="3.2 加载训练后的模型"></a>3.2 加载训练后的模型</h5><p>先卸载刚才的模型，</p>
<p>在检查点路径中选择对应的检查点，绝对路径</p>
<p>点击加载模型</p>
<p><img src="/2025/03/30/llm-application-7/image-20250322102635341.png" alt="image-20250322102635341"></p>
<p>结果如下：</p>
<p><img src="/2025/03/30/llm-application-7/image-20250322102736647.png" alt="image-20250322102736647"></p>
<p>输出和训练数据风格一致，说明训练起效果了。</p>
<h4 id="四-在自己创建的数据集上微调"><a href="#四-在自己创建的数据集上微调" class="headerlink" title="四. 在自己创建的数据集上微调"></a>四. 在自己创建的数据集上微调</h4><h5 id="4-1-制作训练集"><a href="#4-1-制作训练集" class="headerlink" title="4.1 制作训练集"></a>4.1 制作训练集</h5><p>首先从官网上观察数据集格式，数据集是json文件, 其中数据是json object 列表，分为单轮对话和多轮对话。</p>
<p>单轮对话格式如下: 其中<code>instruction</code>才是输入，<code>input</code> 是对<code>instruction</code>的补充，可以为空。</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">[</span></span><br><span class="line">  <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;instruction&quot;</span><span class="punctuation">:</span> <span class="string">&quot;识别并解释给定列表中的两个科学理论：细胞理论和日心说。&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;input&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;output&quot;</span><span class="punctuation">:</span> <span class="string">&quot;细胞理论是生物科学的一个理论，它认为所有生命体都是由微小的基本单元——细胞所构成。这是生物学的一个基础理论，认为细胞是所有生物的基本结构和功能单位，所有的生物都是由一个或多个细胞组成，细胞只能通过细胞分裂产生新的细胞。这一理论由薛定谔、施瓦内和雪莱于1839年首次提出。\n\n日心说是指太阳是太阳系的中心，也就是说，行星围绕太阳旋转的理论。这个理论打破了传统的地心说观点，认为地球并不是宇宙的中心。日心说的提出者是尼古拉·哥白尼，他在16世纪初发表了他的著作《天体运行论》，阐述了太阳系行星围绕太阳运行的模型，为天文学的发展做出了巨大贡献。&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;instruction&quot;</span><span class="punctuation">:</span> <span class="string">&quot;输入三支篮球队的名称并生成一个适当的口号。&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;input&quot;</span><span class="punctuation">:</span> <span class="string">&quot;输入：俄克拉荷马城雷霆队，芝加哥公牛队，布鲁克林网队。&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;output&quot;</span><span class="punctuation">:</span> <span class="string">&quot;输出： \&quot;雷霆，公牛和网队：各显神通，角逐群雄!\&quot;&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">]</span></span><br></pre></td></tr></table></figure>
<p>多轮对话格式如下: 注意，<code>instruction</code> 是本轮对话，即第三轮，并且多轮之间要有逻辑关系，不然微调出来的模型是混乱的。</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">[</span></span><br><span class="line">  <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;instruction&quot;</span><span class="punctuation">:</span> <span class="string">&quot;人类指令（必填）&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;input&quot;</span><span class="punctuation">:</span> <span class="string">&quot;人类输入（选填）&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;output&quot;</span><span class="punctuation">:</span> <span class="string">&quot;模型回答（必填）&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;system&quot;</span><span class="punctuation">:</span> <span class="string">&quot;系统提示词（选填）&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;history&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">      <span class="punctuation">[</span><span class="string">&quot;第一轮指令（选填）&quot;</span><span class="punctuation">,</span> <span class="string">&quot;第一轮回答（选填）&quot;</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">      <span class="punctuation">[</span><span class="string">&quot;第二轮指令（选填）&quot;</span><span class="punctuation">,</span> <span class="string">&quot;第二轮回答（选填）&quot;</span><span class="punctuation">]</span></span><br><span class="line">    <span class="punctuation">]</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">]</span></span><br></pre></td></tr></table></figure>
<p>我选择弱智吧的问答作为训练集，可以从HF-mirror下载（下载方法见前文(<strong>插入链接</strong>)<code>LooksJuicy/ruozhiba</code> 内容如下:</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">[</span></span><br><span class="line">  <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;instruction&quot;</span><span class="punctuation">:</span> <span class="string">&quot;只剩一个心脏了还能活吗？&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;output&quot;</span><span class="punctuation">:</span> <span class="string">&quot;能，人本来就只有一个心脏。&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;instruction&quot;</span><span class="punctuation">:</span> <span class="string">&quot;爸爸再婚，我是不是就有了个新娘？&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;output&quot;</span><span class="punctuation">:</span> <span class="string">&quot;不是的，你有了一个继母。\&quot;新娘\&quot;是指新婚的女方，而你爸爸再婚，他的新婚妻子对你来说是继母。&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line"> <span class="punctuation">]</span></span><br></pre></td></tr></table></figure>
<p>要把它转为LLamaFactory可以识别的格式，转化脚本如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="comment"># 要修改文件路径</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;./data/ruozhiba/ruozhiba_qa.json&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> file:</span><br><span class="line">    data_list = json.load(file)</span><br><span class="line"></span><br><span class="line">result_list = []</span><br><span class="line"><span class="keyword">for</span> each <span class="keyword">in</span> data_list:</span><br><span class="line">    result_list.append(&#123;<span class="string">&quot;instruction&quot;</span>:each[<span class="string">&#x27;instruction&#x27;</span>],</span><br><span class="line">                        <span class="string">&quot;input&quot;</span>: <span class="string">&quot;&quot;</span>,</span><br><span class="line">                        <span class="string">&quot;output&quot;</span>: each[<span class="string">&#x27;output&#x27;</span>]&#125;)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(result_list))</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;./data/ruozhiba/duozhiba_qa_llamaFactory.json&quot;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> file:</span><br><span class="line">    json.dump(result_list, file, ensure_ascii=<span class="literal">False</span>, indent=<span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p>修改后格式如下:</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">[</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;instruction&quot;</span><span class="punctuation">:</span> <span class="string">&quot;只剩一个心脏了还能活吗？&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;input&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;output&quot;</span><span class="punctuation">:</span> <span class="string">&quot;能，人本来就只有一个心脏。&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;instruction&quot;</span><span class="punctuation">:</span> <span class="string">&quot;爸爸再婚，我是不是就有了个新娘？&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;input&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;output&quot;</span><span class="punctuation">:</span> <span class="string">&quot;不是的，你有了一个继母。\&quot;新娘\&quot;是指新婚的女方，而你爸爸再婚，他的新婚妻子对你来说是继母。&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">]</span></span><br></pre></td></tr></table></figure>
<h5 id="4-2-模型微调"><a href="#4-2-模型微调" class="headerlink" title="4.2 模型微调"></a>4.2 模型微调</h5><p>注意配置数据集时先将处理好的数据集放到data目录下，然后在<code>dataset_info.json</code> 中配置文件信息。插入：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="attr">&quot;duozhiba_qa_llamaFactory&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;file_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;duozhiba_qa_llamaFactory.json&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br></pre></td></tr></table></figure>
<p>参考3.3的配置方法，微调Qwen2.5-0.5B-Instruct, 另外，可以选择多个数据集，会混合起来训练。</p>
<p> <img src="/2025/03/30/llm-application-7/image-20250322110317630.png" alt="image-20250322110317630"></p>
<p>训练到损失趋于平缓即可停止。</p>
<p> <img src="/2025/03/30/llm-application-7/image-20250322122711208.png" alt="image-20250322122711208"></p>
<h4 id="五-测试训练效果"><a href="#五-测试训练效果" class="headerlink" title="五. 测试训练效果"></a>五. 测试训练效果</h4><p>LLama-Factory给出了三种微调后的处理方式：Evaluation&amp;Predict（测试）, Chat（对话），Export(模型合并，量化)</p>
<h5 id="5-1-Evaluation-amp-Predict"><a href="#5-1-Evaluation-amp-Predict" class="headerlink" title="5.1 Evaluation &amp; Predict"></a>5.1 Evaluation &amp; Predict</h5><p>选择模型，最后保存的checkpoint，测试集(如果没有特别划分，可以直接用训练集，这里直接用训练集)，截断长度和训练时一致，batch_size可以改变，具体配置如下:</p>
<p><img src="/2025/03/30/llm-application-7/image-20250322124104289.png" alt="image-20250322124104289"></p>
<p><img src="/2025/03/30/llm-application-7/image-20250322124026735.png" alt="image-20250322124026735"></p>
<p>点击开始，后台会报错，因为一些依赖没有安装，缺啥补啥就行。</p>
<p><img src="/2025/03/30/llm-application-7/image-20250322124217867.png" alt="image-20250322124217867"></p>
<p>只会进行一个epoch。测试结果如下:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;predict_bleu-4&quot;: 88.64074418449198,</span><br><span class="line">    &quot;predict_model_preparation_time&quot;: 0.0065,</span><br><span class="line">    &quot;predict_rouge-1&quot;: 92.81370474598931,</span><br><span class="line">    &quot;predict_rouge-2&quot;: 90.33300949197861,</span><br><span class="line">    &quot;predict_rouge-l&quot;: 91.9221340909091,</span><br><span class="line">    &quot;predict_runtime&quot;: 329.3329,</span><br><span class="line">    &quot;predict_samples_per_second&quot;: 4.543,</span><br><span class="line">    &quot;predict_steps_per_second&quot;: 0.455</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>各个指标的含义在8.1中解释。</p>
<h5 id="5-2-chat"><a href="#5-2-chat" class="headerlink" title="5.2 chat"></a>5.2 chat</h5><p>同 四，加载基座模型，设置LoRA配置即可。</p>
<p>效果如下:</p>
<p> <img src="/2025/03/30/llm-application-7/image-20250322125150092.png" alt="image-20250322125150092" style="zoom:50%;"></p>
<p><img src="/2025/03/30/llm-application-7/image-20250322125228457.png" alt="image-20250322125228457"></p>
<p>和训练数据类似，说明微调起效果了。</p>
<p><strong>注意：</strong>每次输入一次对话产生输出后，要清空历史，chat对话框默认是上下文对话，如果不清空历史，上文会影响本轮输出，结果会出错。比如</p>
<p> <img src="/2025/03/30/llm-application-7/image-20250322125529070.png" alt="image-20250322125529070"></p>
<h5 id="5-3-合并模型Export"><a href="#5-3-合并模型Export" class="headerlink" title="5.3 合并模型Export"></a>5.3 合并模型Export</h5><p>设置好基座模型，LoRA路径，导出目录即可。</p>
<p>其中最大分块大小一般选4G，因为一些机器超过4会有问题。</p>
<p><img src="/2025/03/30/llm-application-7/image-20250322125931429.png" alt="image-20250322125931429"></p>
<p>点击开始。导出后会在目标路径生成一个和base模型格式一样的文件</p>
<p><img src="/2025/03/30/llm-application-7/image-20250322130116083.png" alt="image-20250322130116083"></p>
<p>直接运行该模型就能得到微调后的效果。</p>
<h5 id="5-4-量化"><a href="#5-4-量化" class="headerlink" title="5.4 量化"></a>5.4 量化</h5><p>量化主要时用int8甚至更少的比特数表示参数，是为了降低对硬件的依赖，但同样的会造成模型效果的变差，这里量化微调后的模型，LLama-Factory只支持量化合并后的模型，所以只能合并后再量化.</p>
<p>这里量化到int8，量化过程中需要校准，校准数据集都可以，具体校准方法在补充中说明。</p>
<p><img src="/2025/03/30/llm-application-7/image-20250322132401286.png" alt="image-20250322132401286"></p>
<p>需要安装一些包，缺啥补啥就行</p>
<p>注意：auto-gptq包和pytorch版本，cuda版本，python版本有关，我的版本可以直接安装，有时不可以，就先创建一个新的虚拟环境(按照我的配置，特别是python，cuda,pytorch) 然后先安装auto-gtpq，再安装llamafactory，就不容易出错。</p>
<p>另外，量化过程需要从huggingface上下载文件，如果服务器没有梯子，可以配置huggingface国内源，方法之前的文章有写，直接用命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export HF_ENDPOINT=https://hf-mirror.com</span><br></pre></td></tr></table></figure>
<p>量化完成后出现量化后的模型：</p>
<p><img src="/2025/03/30/llm-application-7/image-20250322135645030.png" alt="image-20250322135645030"></p>
<p>测试该模型效果：</p>
<p><img src="/2025/03/30/llm-application-7/image-20250322135940845.png" alt="image-20250322135940845"></p>
<p>结果混乱了，并不是量化过程出错。但说明这次量化失败了，主要这个模型本来就很小，量化后精度损失太大了，按理来说，模型越大，量化后精度损失越小。</p>
<h4 id="六-使用OpenWebui"><a href="#六-使用OpenWebui" class="headerlink" title="六. 使用OpenWebui"></a>六. 使用OpenWebui</h4><h5 id="6-1-open-webui-下载"><a href="#6-1-open-webui-下载" class="headerlink" title="6.1 open webui 下载"></a>6.1 open webui 下载</h5><p>注意：openwebui要在python=3.11下安装，所以要新开一个虚拟环境，新开后直接安装就行</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda create -n openwebui python==3.11 -y</span><br><span class="line">conda activate openwebui</span><br><span class="line">pip install open-webui</span><br></pre></td></tr></table></figure>
<h5 id="6-2-启动"><a href="#6-2-启动" class="headerlink" title="6.2 启动"></a>6.2 启动</h5><p>首先启动vllm，启动教程前面文章讲过了: <a href="https://codezrq.github.io/2025/03/21/llm-application-6/#more">ollama,vllm,LMDeploy 部署大模型 | CodeRQ</a></p>
<p>也可以用其他工具部署，只要记住接口就行了。</p>
<p>接着启动open-webui</p>
<p>启动前要配置几个东西:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export HF_ENDPOINT=https://hf-mirror.com # 要从huggingface下载东西，配置国内huggingface镜像源</span><br><span class="line">export ENABLE_OLLAMA_API=False # open-webui默认用ollama部署，如果用的vllm或者其他，则关闭，如果用ollama就不用</span><br><span class="line">export OPENAI_API_BASE_URL=http://127.0.0.1:8000/v1 # 服务对应的端口，vllm端口在8000</span><br></pre></td></tr></table></figure>
<p>配置完了直接启动</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">open-webui serve</span><br></pre></td></tr></table></figure>
<p>就会启动了，但第一次启动会比较慢，要安装东西，要等一等。</p>
<p><img src="/2025/03/30/llm-application-7/image-20250322142754073.png" alt="image-20250322142754073"></p>
<h5 id><a href="#" class="headerlink" title></a><img src="/2025/03/30/llm-application-7/image-20250322143824215.png" alt="image-20250322143824215"></h5><p>我这里服务器卡住了，按理到这里就是成功了，会弹窗出来。</p>
<h5 id="6-3-解决弹不出窗口的问题"><a href="#6-3-解决弹不出窗口的问题" class="headerlink" title="6.3 解决弹不出窗口的问题"></a>6.3 解决弹不出窗口的问题</h5><p>一开始我以为是服务器问题，没有深究，后来通过查阅资料，发现是新版Ollama的问题，我们之前启动LLama-Factory的时候其实服务是启动在服务器上的，之所以我们本地能通过浏览器访问，是因为vscode帮我们做了端口转发，而open-webui启动的时候不会自动做端口转发，所以我们需要自己做端口转发。</p>
<p>在vscode中配置端口转发，把服务器的8080端口转发到本地，即可在本地打开</p>
<p><img src="/2025/03/30/llm-application-7/image-20250328003144480.png" alt="image-20250328003144480"></p>
<p>首次登录需要设置邮箱密码。</p>
<p> <img src="/2025/03/30/llm-application-7/image-20250328003251767.png" alt="image-20250328003251767"></p>
<p>设置完密码还需要等很久，之后会出现对话界面</p>
<p><img src="/2025/03/30/llm-application-7/image-20250328005035684.png" alt="image-20250328005035684"></p>
<h4 id="七-通过脚本直接Lora微调Qwen"><a href="#七-通过脚本直接Lora微调Qwen" class="headerlink" title="七. 通过脚本直接Lora微调Qwen"></a>七. 通过脚本直接Lora微调Qwen</h4><p>参考连接 <a href="https://github.com/datawhalechina/self-llm/blob/master/examples/Chat-嬛嬛/readme.md">self-llm/examples/Chat-嬛嬛/readme.md at master · datawhalechina/self-llm</a></p>
<p>主要LoRA微调 Qwen2.5-0.5B-Instruct，使其输出分格类似甄嬛。直接通过脚本微调。</p>
<h5 id="7-1-获取数据集"><a href="#7-1-获取数据集" class="headerlink" title="7.1  获取数据集"></a>7.1  获取数据集</h5><p>通过以下链接获取，然后上传到服务器。 <a href="https://github.com/datawhalechina/self-llm/blob/master/dataset/huanhuan.json">self-llm/dataset/huanhuan.json at master · datawhalechina/self-llm</a></p>
<h5 id="7-2-配置环境"><a href="#7-2-配置环境" class="headerlink" title="7.2 配置环境"></a>7.2 配置环境</h5><p>我的配置为: PyTorch 2.5.1; Python 3.12(ubuntu22.04); CUDA 12.4</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conda create -n myenv python==<span class="number">3.12</span> -y</span><br><span class="line">conda activate myenv</span><br><span class="line">pip install transformers accelerate peft datasets</span><br></pre></td></tr></table></figure>
<h5 id="7-3-模型微调"><a href="#7-3-模型微调" class="headerlink" title="7.3 模型微调"></a>7.3 模型微调</h5><p>可以参考 <strong>3.3和5.2</strong> 的方法使用LLama-Factory进行微调，这里展示直接通过脚本微调的方式。训练代码如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer, GenerationConfig</span><br><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> LoraConfig, TaskType, get_peft_model</span><br><span class="line"></span><br><span class="line"><span class="comment"># 基座模型路径</span></span><br><span class="line">model_path = <span class="string">&quot;&quot;</span></span><br><span class="line"><span class="comment"># 训练集路径</span></span><br><span class="line">train_path = <span class="string">&quot;&quot;</span></span><br><span class="line"><span class="comment"># 微调的lora参数输出路径</span></span><br><span class="line">output_path = <span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理训练集</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process_func</span>(<span class="params">example</span>):</span><br><span class="line">    MAX_LENGTH = <span class="number">384</span>    <span class="comment"># Llama分词器会将一个中文字切分为多个token，因此需要放开一些最大长度，保证数据的完整性</span></span><br><span class="line">    input_ids, attention_mask, labels = [], [], []</span><br><span class="line">    instruction = tokenizer(<span class="string">f&quot;&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n现在你要扮演皇帝身边的女人--甄嬛&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\n\n<span class="subst">&#123;example[<span class="string">&#x27;instruction&#x27;</span>] + example[<span class="string">&#x27;input&#x27;</span>]&#125;</span>&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\n\n&quot;</span>, add_special_tokens=<span class="literal">False</span>)  <span class="comment"># add_special_tokens 不在开头加 special_tokens</span></span><br><span class="line">    response = tokenizer(<span class="string">f&quot;<span class="subst">&#123;example[<span class="string">&#x27;output&#x27;</span>]&#125;</span>&lt;|eot_id|&gt;&quot;</span>, add_special_tokens=<span class="literal">False</span>)</span><br><span class="line">    input_ids = instruction[<span class="string">&quot;input_ids&quot;</span>] + response[<span class="string">&quot;input_ids&quot;</span>] + [tokenizer.pad_token_id]</span><br><span class="line">    attention_mask = instruction[<span class="string">&quot;attention_mask&quot;</span>] + response[<span class="string">&quot;attention_mask&quot;</span>] + [<span class="number">1</span>]  <span class="comment"># 因为eos token咱们也是要关注的所以 补充为1</span></span><br><span class="line">    labels = [-<span class="number">100</span>] * <span class="built_in">len</span>(instruction[<span class="string">&quot;input_ids&quot;</span>]) + response[<span class="string">&quot;input_ids&quot;</span>] + [tokenizer.pad_token_id]  </span><br><span class="line">    <span class="comment"># 尽量不要截断</span></span><br><span class="line"><span class="comment">#    if len(input_ids) &gt; MAX_LENGTH:  # 做一个截断</span></span><br><span class="line"><span class="comment">#       input_ids = input_ids[:MAX_LENGTH]</span></span><br><span class="line"><span class="comment">#      attention_mask = attention_mask[:MAX_LENGTH]</span></span><br><span class="line"><span class="comment">#        labels = labels[:MAX_LENGTH]</span></span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        <span class="string">&quot;input_ids&quot;</span>: input_ids,</span><br><span class="line">        <span class="string">&quot;attention_mask&quot;</span>: attention_mask,</span><br><span class="line">        <span class="string">&quot;labels&quot;</span>: labels</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 加载模型</span></span><br><span class="line">    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=<span class="string">&quot;auto&quot;</span>,torch_dtype=torch.bfloat16)</span><br><span class="line">    model.enable_input_require_grads() <span class="comment"># 开启梯度检查点时，要执行该方法</span></span><br><span class="line">    <span class="comment"># 加载分词器</span></span><br><span class="line">    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=<span class="literal">False</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">    tokenizer.pad_token = tokenizer.eos_token</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将JSON文件转换为CSV文件</span></span><br><span class="line">    <span class="comment"># 加载训练集</span></span><br><span class="line">    df = pd.read_json(train_path)</span><br><span class="line">    ds = Dataset.from_pandas(df)</span><br><span class="line">    tokenized_id = ds.<span class="built_in">map</span>(process_func, remove_columns=ds.column_names)</span><br><span class="line">		</span><br><span class="line">    config = LoraConfig(</span><br><span class="line">        task_type=TaskType.CAUSAL_LM, </span><br><span class="line">        target_modules=[<span class="string">&quot;q_proj&quot;</span>, <span class="string">&quot;k_proj&quot;</span>, <span class="string">&quot;v_proj&quot;</span>, <span class="string">&quot;o_proj&quot;</span>, <span class="string">&quot;gate_proj&quot;</span>, <span class="string">&quot;up_proj&quot;</span>, <span class="string">&quot;down_proj&quot;</span>],</span><br><span class="line">        inference_mode=<span class="literal">False</span>, <span class="comment"># 训练模式</span></span><br><span class="line">        r=<span class="number">8</span>, <span class="comment"># Lora 秩</span></span><br><span class="line">        lora_alpha=<span class="number">32</span>, <span class="comment"># Lora alaph，具体作用参见 Lora 原理</span></span><br><span class="line">        lora_dropout=<span class="number">0.1</span><span class="comment"># Dropout 比例</span></span><br><span class="line">    )</span><br><span class="line">    model = get_peft_model(model, config)</span><br><span class="line">    model.print_trainable_parameters() <span class="comment"># 打印总训练参数</span></span><br><span class="line"></span><br><span class="line">    args = TrainingArguments(</span><br><span class="line">        output_dir=output_path,</span><br><span class="line">        per_device_train_batch_size=<span class="number">4</span>,</span><br><span class="line">        gradient_accumulation_steps=<span class="number">4</span>,</span><br><span class="line">        logging_steps=<span class="number">10</span>,</span><br><span class="line">        num_train_epochs=<span class="number">3</span>,</span><br><span class="line">        save_steps=<span class="number">100</span>, <span class="comment"># 为了快速演示，这里设置10，建议你设置成100，每100个steps保存一次</span></span><br><span class="line">        learning_rate=<span class="number">1e-4</span>,</span><br><span class="line">        save_on_each_node=<span class="literal">True</span>,</span><br><span class="line">        gradient_checkpointing=<span class="literal">True</span></span><br><span class="line">    )</span><br><span class="line">    trainer = Trainer(</span><br><span class="line">        model=model,</span><br><span class="line">        args=args,</span><br><span class="line">        train_dataset=tokenized_id,</span><br><span class="line">        data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=<span class="literal">True</span>),</span><br><span class="line">    )</span><br><span class="line">    trainer.train() <span class="comment"># 开始训练 </span></span><br><span class="line">    <span class="comment"># 在训练参数中设置了自动保存策略此处并不需要手动保存。</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>运行<code>train.py</code> ，开始训练:</p>
<p><img src="/2025/03/30/llm-application-7/image-20250323221203024.png" alt="image-20250323221203024"></p>
<h5 id="8-4-模型合并"><a href="#8-4-模型合并" class="headerlink" title="8.4 模型合并"></a>8.4 模型合并</h5><p>训练是输出是lora参数，并不是整个模型，如果想用vllm或者其他工具部署，可以将lora参数合并到基座模型，导出微调后的整个模型。合并和导出代码如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> PeftModel</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForCausalLM, LlamaTokenizer</span><br><span class="line"><span class="keyword">from</span> transformers.generation.utils <span class="keyword">import</span> GenerationConfig</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">apply_lora</span>(<span class="params">model_name_or_path, output_path, lora_path</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Loading the base model from <span class="subst">&#123;model_name_or_path&#125;</span>&quot;</span>)</span><br><span class="line">    base_tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=<span class="literal">False</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">    base = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=<span class="string">&quot;cuda:0&quot;</span>, torch_dtype=torch.bfloat16, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># base.generation_config = GenerationConfig.from_pretrained(model_name_or_path)</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Loading the LoRA adapter from <span class="subst">&#123;lora_path&#125;</span>&quot;</span>)</span><br><span class="line"> </span><br><span class="line">    lora_model = PeftModel.from_pretrained(</span><br><span class="line">        base,</span><br><span class="line">        lora_path,</span><br><span class="line">        torch_dtype=torch.float16,</span><br><span class="line">    )</span><br><span class="line"> </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Applying the LoRA&quot;</span>)</span><br><span class="line">    model = lora_model.merge_and_unload()</span><br><span class="line"> </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Saving the target model to <span class="subst">&#123;output_path&#125;</span>&quot;</span>)</span><br><span class="line">    model.save_pretrained(output_path)</span><br><span class="line">    base_tokenizer.save_pretrained(output_path)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 更换为自己对应的路径</span></span><br><span class="line">    lora_path = <span class="string">&quot;/root/autodl-tmp/output/huanhuan-lora/checkpoint-699&quot;</span></span><br><span class="line">    model_path = <span class="string">&quot;/root/autodl-tmp/Qwen2.5-0.5B-Instruct&quot;</span></span><br><span class="line">    output = <span class="string">&quot;/root/autodl-tmp/Qwen2.5-0.5B-Instruct-huanhuan&quot;</span></span><br><span class="line"></span><br><span class="line">    apply_lora(model_path,output,lora_path)</span><br></pre></td></tr></table></figure>
<h5 id="8-5-测试模型"><a href="#8-5-测试模型" class="headerlink" title="8.5 测试模型"></a>8.5 测试模型</h5><p>合并后的模型和基座模型结构一样，可以用上面的工具部署，这里仅为了测试效果，所以直接通过脚本测试。代码如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM, AutoTokenizer</span><br><span class="line"></span><br><span class="line">model_name = <span class="string">&quot;/root/autodl-tmp/Qwen2.5-0.5B-Instruct-huanhuan&quot;</span></span><br><span class="line"></span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(</span><br><span class="line">    model_name,</span><br><span class="line">    torch_dtype=<span class="string">&quot;auto&quot;</span>,</span><br><span class="line">    device_map=<span class="string">&quot;auto&quot;</span></span><br><span class="line">)</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_name)</span><br><span class="line"></span><br><span class="line">prompt = <span class="string">&quot;你是谁?&quot;</span></span><br><span class="line">messages = [</span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;现在你要扮演皇帝身边的女人--甄嬛&quot;</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: prompt&#125;</span><br><span class="line">]</span><br><span class="line">text = tokenizer.apply_chat_template(</span><br><span class="line">    messages,</span><br><span class="line">    tokenize=<span class="literal">False</span>,</span><br><span class="line">    add_generation_prompt=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line">model_inputs = tokenizer([text], return_tensors=<span class="string">&quot;pt&quot;</span>).to(model.device)</span><br><span class="line"></span><br><span class="line">generated_ids = model.generate(</span><br><span class="line">    **model_inputs,</span><br><span class="line">    max_new_tokens=<span class="number">512</span></span><br><span class="line">)</span><br><span class="line">generated_ids = [</span><br><span class="line">    output_ids[<span class="built_in">len</span>(input_ids):] <span class="keyword">for</span> input_ids, output_ids <span class="keyword">in</span> <span class="built_in">zip</span>(model_inputs.input_ids, generated_ids)</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">response = tokenizer.batch_decode(generated_ids, skip_special_tokens=<span class="literal">True</span>)[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(response)</span><br></pre></td></tr></table></figure>
<p>运行后输出如下:</p>
<p><img src="/2025/03/30/llm-application-7/image-20250323221548136.png" alt="image-20250323221548136"></p>
<p>符合预期效果，说明微调起效。</p>
<h4 id="八-补充"><a href="#八-补充" class="headerlink" title="八. 补充"></a>八. 补充</h4><h5 id="8-1-LLamaFactory-evaluation中各个指标的含义"><a href="#8-1-LLamaFactory-evaluation中各个指标的含义" class="headerlink" title="8.1 LLamaFactory evaluation中各个指标的含义"></a>8.1 LLamaFactory evaluation中各个指标的含义</h5><p>在 5.1 中通过LLama-Factory进行测试，结果如下:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;predict_bleu-4&quot;: 88.64074418449198,</span><br><span class="line">    &quot;predict_model_preparation_time&quot;: 0.0065,</span><br><span class="line">    &quot;predict_rouge-1&quot;: 92.81370474598931,</span><br><span class="line">    &quot;predict_rouge-2&quot;: 90.33300949197861,</span><br><span class="line">    &quot;predict_rouge-l&quot;: 91.9221340909091,</span><br><span class="line">    &quot;predict_runtime&quot;: 329.3329,</span><br><span class="line">    &quot;predict_samples_per_second&quot;: 4.543,</span><br><span class="line">    &quot;predict_steps_per_second&quot;: 0.455</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这几个结果的含义如下:</p>
<ol>
<li>predict_bleu-4</li>
</ol>
<h5 id="8-2-量化校准的原理"><a href="#8-2-量化校准的原理" class="headerlink" title="8.2  量化校准的原理"></a>8.2  量化校准的原理</h5><h6 id="8-2-1-为什么需要量化"><a href="#8-2-1-为什么需要量化" class="headerlink" title="8.2.1 为什么需要量化"></a>8.2.1 为什么需要量化</h6><p>量化的主要目的是节约显存，提升计算效率以及加快通信。如下表，deepseek-r1-7b模型以不同类ing加载和随不同数据类型所占用的显存也不一样</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>FP32</th>
<th>FP16</th>
<th>Int8</th>
<th>Int3</th>
</tr>
</thead>
<tbody>
<tr>
<td>显存占用</td>
<td>28G</td>
<td>14G</td>
<td>7G</td>
<td>3.5G</td>
</tr>
</tbody>
</table>
</div>
<p>总的来说，量化就是把Float类型(FP32,FP16)的模型参数和激活值，用整数(Int8. Int4)来代替，同时尽可能减少量化后模型推理的误差。</p>
<h6 id="8-2-2-对称量化vs非对称量化"><a href="#8-2-2-对称量化vs非对称量化" class="headerlink" title="8.2.2 对称量化vs非对称量化"></a>8.2.2 对称量化vs非对称量化</h6><p>（1）<strong>对称量化：</strong>如下图，对称量化的原理就是找到， xf 中绝对值的最大值，然后对其进行缩放得到量化后的值，然后进行反量化得到原来的值，可以看出量化是存在一定误差的，具体原理如下图：</p>
<p><img src="/2025/03/30/llm-application-7/v2-45fd7d5ede2b518c31e40c24a5085f32_r.jpg" alt="img"></p>
<p>这样量化的缺点就是，坐标轴上有一段数值空间被浪费了，对应图中-127那一部分。基于这个缺点，为了让量化后的坐标轴上的数值被充分利用，引入非对称量化。</p>
<p>（2）<strong>非对称量化：</strong>非对称量化有一个额外的参数Z调整零点的映射，这个参数通常称为零点。非对称量化表示的范围没有严格的限制，可以根据浮点值的范围，选取任意的想要表示的范围。因此非对称量化的效果通常比对称量化好，但是需要额外存储以及推理时计算零点相关的内容。</p>
<p><img src="/2025/03/30/llm-application-7/v2-196d043ae9197b591b4e11fbb42e7025_r.jpg" alt="img"></p>
<p>因此，对称量化具有计算简单，精度低等特点，非对称量化的计算有一个额外的参数Z调整零点的映射，因此计算复杂，但精度相对较高。</p>
<p>（3）<strong>在矩阵乘法中的应用</strong>：下图为对称量化在居中乘法当中的应用示意图（非对称量化也类似），通过量化，可以把浮点矩阵的乘法，转化为整数矩阵的乘法，虽然存在一定误差，但误差不大。在矩阵乘法中采用量化可以降低计算复杂度，提升矩阵乘法效率。</p>
<p><img src="/2025/03/30/llm-application-7/v2-9e6a9b5ee93c5a6ceedefb6a010cbb3c_1440w.jpg" alt="img"></p>
<h6 id="8-2-3-如何对神经网络进行量化"><a href="#8-2-3-如何对神经网络进行量化" class="headerlink" title="8.2.3 如何对神经网络进行量化"></a>8.2.3 如何对神经网络进行量化</h6><p><strong>为什么量化对神经网络精度影响不大？</strong></p>
<ol>
<li>因为一般权重和输入都经过Normalization, 基本数值范围都不大。</li>
<li>因为激活函数，数值影响会被平滑。</li>
<li>绝大部分神经网络都是分类问题，最后都是概率值，只要最后某种类别的概率高于其他类别就可以，不需要绝对数值。</li>
</ol>
<h6 id="8-2-4-动态量化vs静态量化"><a href="#8-2-4-动态量化vs静态量化" class="headerlink" title="8.2.4 动态量化vs静态量化"></a>8.2.4 动态量化vs静态量化</h6><p>量化在神经网络中的是对每一层而言，每一层进行量化计算，每一层输出时进行反量化。具体而言，量化在神经网络当中的应用又可分为动态量化（Post Training Quantization Dynamic, PTQ Dynamic）与静态量化(Post Training Quantization Static), PTQ Static</p>
<p><strong>(1) 动态量化流程</strong></p>
<ol>
<li>将训练好的模型权重量化为Int8，并保存量化参数。</li>
<li>在模型推理时，对每一层输入的FP32激活值，动态进行量化为Int8。</li>
<li>在每一层对量化后的Int8权重和Int8激活值进行计算。</li>
<li>在每一层输出时将结果反量化为FP32。</li>
<li>将FP32激活值传入到下一层。</li>
</ol>
<p>可用下图表示：</p>
<p><img src="/2025/03/30/llm-application-7/v2-aaac743a8729c5195eda146208fc2c6c_r.jpg" alt="img"></p>
<p>动态量化存在的问题：</p>
<ol>
<li>每一次推理每一层都要对输入统计量化参数，耗时。</li>
<li>每一层计算完都转化为FP32，存入显存，占用显存带宽。</li>
</ol>
<p><strong>（2）静态量化流程</strong></p>
<p>针对动态量化的问题1，通过用有代表性的输入数据跑一遍整个网络，通过统计得到每层大概的量化参数来解决；问题2，这一层的输出是下一层的输入。下一层还是要量化，通过在这一层直接量化好再传给下一层方法来解决。这就是静态量化。</p>
<ol>
<li>将训练好的模型权重量化为Int8, 并保存量化参数。</li>
<li>校准：利用一些数据进行模型推理，用这些数据在神经网络每一层产生的激活估算除激活值的量化参数。这样就不用推理时每次根据实际激活值计算量化参数。</li>
<li>在每一层对量化后的Int8权重和Int8激活值进行计算。</li>
<li>在每一层输出时将结果反量化为FP32，同时根据校准产生的激活值量化参数，把激活值量化为Int8，把量化参数放入量化后的激活值中。</li>
<li>将Int8的激活值和它的量化参数传入到下一层。</li>
</ol>
<p>可用下图表示：</p>
<p><img src="/2025/03/30/llm-application-7/v2-dbd849709bb56ddd2109a0353a2cdae3_r.jpg" alt="img"></p>
<h5 id="8-3-提醒"><a href="#8-3-提醒" class="headerlink" title="8.3 提醒"></a>8.3 提醒</h5><p>整个项目东西有点多，有几点容易出错：</p>
<ul>
<li>时刻注意，你要在虚拟环境中，开启webui的命令行路径在LLama-Factory文件夹中。</li>
<li>有时下载包后仍显示没有该包，要重启终端。</li>
<li>尽量用python3.10，兼容性好一些。</li>
<li>openwebui要用python3.11</li>
</ul>
]]></content>
      <categories>
        <category>大模型应用</category>
        <category>微调与训练</category>
      </categories>
  </entry>
  <entry>
    <title>大模型应用系列(八)  LoRA与QLoRA</title>
    <url>/2025/03/30/llm-application-8/</url>
    <content><![CDATA[<p>LoRA与QLoRA的异同，以及如何在LLamaFactory上使用LoRA与QLoRA</p>
<span id="more"></span>
<h4 id="一-补充"><a href="#一-补充" class="headerlink" title="一. 补充"></a>一. 补充</h4><h5 id="LoRA-和-QLoRA"><a href="#LoRA-和-QLoRA" class="headerlink" title="LoRA 和 QLoRA"></a>LoRA 和 QLoRA</h5><p>LoRA: LoRA是一种用于微调大模型的技术，通过低秩近似分解方法适配数十亿参数到特定领域。</p>
<p>QLoRA: QLoRA是一种高效的大模型微调方法，显著降低了显存使用量，同时保持了bf16微调的性能。它通过一个固定的，4位量化的预训练语言模型进行反向传播梯度到低秩适配器来实现这一目标。</p>
<p>大模型的大指的是参数量大，在微调过程中，我们可以采用降低模型参数精度来节约显存，以此来增大batch_size加快训练。</p>
<p>问题：模型的量化是以牺牲模型精度为代价的，训练过程中使用量化不会降低模型的精度，因为训练过程中参数还没有确定，AI训练的结果不是准确的值，而是数据分布，模型之所以可以量化，是因为量化后虽然参数绝对值变化了，但他们拟合的分布不变。</p>
<p>注意: 在量化微调中，量化只发生在内部训练过程，并不影响模型最终的数据类型，模型原有参数类型在量化微调训练中会量化，但参数保存时又会恢复，因此量化微调并不影响模型本身的参数类型。</p>
<h5 id="微调的目标"><a href="#微调的目标" class="headerlink" title="微调的目标"></a>微调的目标</h5><p>微调的理想目标是让模型在测试数据集上达到拟合状态(损失收敛)， 根据测试集损失或者精度来表达，生成模型通过客观评价并不可靠。</p>
<h5 id="模型微调多少个epoch才会拟合"><a href="#模型微调多少个epoch才会拟合" class="headerlink" title="模型微调多少个epoch才会拟合"></a>模型微调多少个epoch才会拟合</h5><p>一般3-5个epoch就会其效果，但多少个epoch拟合是未知的，模型拟合和两个因素有关：模型的复杂程度和数据集的复杂程度，一般来说，模型越复杂，数据集越复杂，需要的epoch越多。在具体实验中，一般会给一个较大的epoch(比如几千)，主要防止模型未达到拟合训练就结束了（因为训练过程人不会一直看着）。</p>
<p>一般训练到损失收敛就可以了(loss不怎么下降了)</p>
<h4 id="二-LLama-Factory的安装"><a href="#二-LLama-Factory的安装" class="headerlink" title="二. LLama-Factory的安装"></a>二. LLama-Factory的安装</h4><p>我的服务器配置为:</p>
<p>RTX 4090, cuda 12.4 python=3.10 pytorch=2.5.1</p>
<p>参考：<a href="https://codezrq.github.io/2025/03/30/llm-application-7/#more">大模型应用系列(七) LLama Factory和OpenWebui的安装和使用 | CodeRQ</a></p>
<h4 id="三-准备模型和数据集"><a href="#三-准备模型和数据集" class="headerlink" title="三. 准备模型和数据集"></a>三. 准备模型和数据集</h4><p>使用的是多轮数据集，法律相关，可以从下面链接下载：</p>
<p><a href="https://download.csdn.net/download/weixin_46091520/90532538">大模型微调的法律知识多轮数据集资源-CSDN文库</a></p>
<p>将数据集上传到data文件夹中，并在<code>dataset_info.json</code>中添加数据集文件信息:</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="attr">&quot;fintech&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;file_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;fintechy.json&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br></pre></td></tr></table></figure>
<p>模型下载可以参考: <a href="https://codezrq.github.io/2025/03/06/llm-application-2/#more">Huggingface的安装和使用 | CodeRQ</a></p>
<p>这里选择的模型是<code>Qwen2.5-1.5B-Instruct</code></p>
<h4 id="四-进行QLoRA微调"><a href="#四-进行QLoRA微调" class="headerlink" title="四. 进行QLoRA微调"></a>四. 进行QLoRA微调</h4><p>启动LLamaFatory</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd LLaMA-Factory</span><br><span class="line">llamafactory-cli webui</span><br></pre></td></tr></table></figure>
<p>设置参数：要注意QLoRA的量化等级，指的是在训练过程中模型的量化等级，量化等级越高模型精度越低，跟模型大小也有关，如果模型很大，那量化等级可以开高一点。这里才1.5B，所以只开了INT8</p>
<p><img src="/2025/03/30/llm-application-8/image-20250325224602064.png" alt="image-20250325224602064"></p>
<p>开启QLoRA之后，还要调整LoRA参数，通过大量实验得出，QLoRA中LoRA的秩最低要32，最好是60左右，（一般是32到128之间）如果模型比较大，那可以小一点，但如果模型很小，LoRA的秩就要大一些。另外LoRA缩放系数一般设为LoRA的秩的两倍。LoRA秩越大，显存占用越大。</p>
<p><img src="/2025/03/30/llm-application-8/image-20250325225237010.png" alt="image-20250325225237010"></p>
<p>配置好后点击启动开始QLoRA微调，一开始loss反而上升是正常现象，一般五次后能下降就是正常的。</p>
<p> <img src="/2025/03/30/llm-application-8/image-20250325230416093.png" alt="image-20250325230416093"></p>
<p><img src="/2025/03/30/llm-application-8/image-20250325231029112.png" alt="image-20250325231029112"></p>
<p>这里显存利用率还没到90%, 说明batch_size还可以加大。</p>
<p><img src="/2025/03/30/llm-application-8/image-20250326002151737.png" alt="image-20250326002151737"></p>
<p>训练到loss下降缓慢就可以停止了。 </p>
<p>另外，为了进一步加快微调，还可以开启flashattn2， 但它要求GPU架构在sm80以上。</p>
<h4 id="五-测试结果"><a href="#五-测试结果" class="headerlink" title="五. 测试结果"></a>五. 测试结果</h4><p><img src="/2025/03/30/llm-application-8/image-20250326003115177.png" alt="image-20250326003115177"></p>
<p>符合训练集。</p>
<h4 id="六-对比LoRA和QLoRA"><a href="#六-对比LoRA和QLoRA" class="headerlink" title="六. 对比LoRA和QLoRA"></a>六. 对比LoRA和QLoRA</h4><h5 id="6-1-使用LoRA微调"><a href="#6-1-使用LoRA微调" class="headerlink" title="6.1 使用LoRA微调"></a>6.1 使用LoRA微调</h5><p><img src="/2025/03/30/llm-application-8/image-20250326003532963.png" alt="image-20250326003532963"></p>
<h5 id="6-2-使用QLoRA微调"><a href="#6-2-使用QLoRA微调" class="headerlink" title="6.2 使用QLoRA微调"></a>6.2 使用QLoRA微调</h5><p><img src="/2025/03/30/llm-application-8/image-20250326004207803.png" alt="image-20250326004207803"></p>
<p>对比LoRA和QLoRA可以看出，QLoRA占用的显存更小，但虽然是int8量化，并不是两倍的关系，因为只是模型参数量化了，由于截断长度是512, 数据占了很大显存，模型只有1.5B，比较小，所以显存降低不明想。</p>
<h5 id="6-3-开启flashttn2优化"><a href="#6-3-开启flashttn2优化" class="headerlink" title="6.3 开启flashttn2优化"></a>6.3 开启flashttn2优化</h5><p><img src="/2025/03/30/llm-application-8/image-20250326004612163.png" alt="image-20250326004612163"></p>
<p><img src="/2025/03/30/llm-application-8/image-20250326004620148.png" alt="image-20250326004620148"></p>
<p>可以看到，开启flashattn2后显存占用变高，但训练速度会更快。</p>
]]></content>
      <categories>
        <category>大模型应用</category>
        <category>微调与训练</category>
      </categories>
  </entry>
  <entry>
    <title>大模型应用系列(九) 对话模板对齐</title>
    <url>/2025/03/30/llm-application-9/</url>
    <content><![CDATA[<p>解决在微调和部署时对话模板不统一的问题。</p>
<span id="more"></span>
<h4 id="一-发现问题"><a href="#一-发现问题" class="headerlink" title="一. 发现问题"></a>一. 发现问题</h4><p>我们在<strong><a href="https://codezrq.github.io/2025/03/30/llm-application-7/">大模型应用系列(七) LLama Factory和OpenWebui的安装和使用 | CodeRQ</a></strong>中学习了如何通过脚本或者LLama-Factory微调大模型，<strong><a href="https://codezrq.github.io/2025/03/21/llm-application-6/">大模型应用系列(六) ollama,vllm,LMDeploy 部署大模型 | CodeRQ</a></strong></p>
<p>中学习了如何使用Ollama， vllm, LMDeploy部署大模型，我们在使用的过程中就会发现，如果在LLamaFactory上微调到基本收敛了，此时在LLamaFactory上的对话框输出是正常的，符合预期的，但是使用其他部署工具部署的模型输出又不一样了。以下展示一些不同, 这里的模型是<strong><a href="https://codezrq.github.io/2025/03/30/llm-application-7/">大模型应用系列(七) LLama Factory和OpenWebui的安装和使用 | CodeRQ</a></strong>训练的弱智吧对话AI，</p>
<h5 id="1-1-LLamaFactory"><a href="#1-1-LLamaFactory" class="headerlink" title="1.1 LLamaFactory"></a>1.1 LLamaFactory</h5><p><img src="/2025/03/30/llm-application-9/image-20250328011605973.png" alt="image-20250328011605973"></p>
<h5 id="1-2-openWebui"><a href="#1-2-openWebui" class="headerlink" title="1.2 openWebui"></a>1.2 openWebui</h5><p>  <img src="/2025/03/30/llm-application-9/image-20250328005447733.png" alt="image-20250328005447733"></p>
<h5 id="1-3-vllm"><a href="#1-3-vllm" class="headerlink" title="1.3 vllm"></a>1.3 vllm</h5><p>这里直接使用之前的vllm部署中多轮对话的代码</p>
<p><img src="/2025/03/30/llm-application-9/image-20250328010119136.png" alt="image-20250328010119136"></p>
<p>对比上面五个结果，可以看出，模型的输出都有区别，其中LLamaFactory的输出最符合数据集(和数据集一样，因为训练loss基本不下降了)。</p>
<h4 id="二-分析"><a href="#二-分析" class="headerlink" title="二. 分析"></a>二. 分析</h4><p>发生上述原因主要是不同的框架使用的对话模板不同，首先所有大模型的文件中存在模型自定义的对话模板，但是不同的大模型的对话模板是不同的，下面分析不同模型和框架的对话模板：</p>
<h5 id="2-1-Qwen-对话模板"><a href="#2-1-Qwen-对话模板" class="headerlink" title="2.1 Qwen 对话模板"></a>2.1 Qwen 对话模板</h5><p>大模型自带的对话模板可以从它文件中的<code>tokenizer_config.json</code>中查看，比如qwen2.5的对话模板为:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&quot;chat_template&quot;: &quot;&#123;%- if tools %&#125;\n    &#123;&#123;- &#x27;&lt;|im_start|&gt;system\\n&#x27; &#125;&#125;\n    &#123;%- if messages[0][&#x27;role&#x27;] == &#x27;system&#x27; %&#125;\n        &#123;&#123;- messages[0][&#x27;content&#x27;] &#125;&#125;\n    &#123;%- else %&#125;\n        &#123;&#123;- &#x27;You are Qwen, created by Alibaba Cloud. You are a helpful assistant.&#x27; &#125;&#125;\n    &#123;%- endif %&#125;\n    &#123;&#123;- \&quot;\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within &lt;tools&gt;&lt;/tools&gt; XML tags:\\n&lt;tools&gt;\&quot; &#125;&#125;\n    &#123;%- for tool in tools %&#125;\n        &#123;&#123;- \&quot;\\n\&quot; &#125;&#125;\n        &#123;&#123;- tool | tojson &#125;&#125;\n    &#123;%- endfor %&#125;\n    &#123;&#123;- \&quot;\\n&lt;/tools&gt;\\n\\nFor each function call, return a json object with function name and arguments within &lt;tool_call&gt;&lt;/tool_call&gt; XML tags:\\n&lt;tool_call&gt;\\n&#123;\\\&quot;name\\\&quot;: &lt;function-name&gt;, \\\&quot;arguments\\\&quot;: &lt;args-json-object&gt;&#125;\\n&lt;/tool_call&gt;&lt;|im_end|&gt;\\n\&quot; &#125;&#125;\n&#123;%- else %&#125;\n    &#123;%- if messages[0][&#x27;role&#x27;] == &#x27;system&#x27; %&#125;\n        &#123;&#123;- &#x27;&lt;|im_start|&gt;system\\n&#x27; + messages[0][&#x27;content&#x27;] + &#x27;&lt;|im_end|&gt;\\n&#x27; &#125;&#125;\n    &#123;%- else %&#125;\n        &#123;&#123;- &#x27;&lt;|im_start|&gt;system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.&lt;|im_end|&gt;\\n&#x27; &#125;&#125;\n    &#123;%- endif %&#125;\n&#123;%- endif %&#125;\n&#123;%- for message in messages %&#125;\n    &#123;%- if (message.role == \&quot;user\&quot;) or (message.role == \&quot;system\&quot; and not loop.first) or (message.role == \&quot;assistant\&quot; and not message.tool_calls) %&#125;\n        &#123;&#123;- &#x27;&lt;|im_start|&gt;&#x27; + message.role + &#x27;\\n&#x27; + message.content + &#x27;&lt;|im_end|&gt;&#x27; + &#x27;\\n&#x27; &#125;&#125;\n    &#123;%- elif message.role == \&quot;assistant\&quot; %&#125;\n        &#123;&#123;- &#x27;&lt;|im_start|&gt;&#x27; + message.role &#125;&#125;\n        &#123;%- if message.content %&#125;\n            &#123;&#123;- &#x27;\\n&#x27; + message.content &#125;&#125;\n        &#123;%- endif %&#125;\n        &#123;%- for tool_call in message.tool_calls %&#125;\n            &#123;%- if tool_call.function is defined %&#125;\n                &#123;%- set tool_call = tool_call.function %&#125;\n            &#123;%- endif %&#125;\n            &#123;&#123;- &#x27;\\n&lt;tool_call&gt;\\n&#123;\&quot;name\&quot;: \&quot;&#x27; &#125;&#125;\n            &#123;&#123;- tool_call.name &#125;&#125;\n            &#123;&#123;- &#x27;\&quot;, \&quot;arguments\&quot;: &#x27; &#125;&#125;\n            &#123;&#123;- tool_call.arguments | tojson &#125;&#125;\n            &#123;&#123;- &#x27;&#125;\\n&lt;/tool_call&gt;&#x27; &#125;&#125;\n        &#123;%- endfor %&#125;\n        &#123;&#123;- &#x27;&lt;|im_end|&gt;\\n&#x27; &#125;&#125;\n    &#123;%- elif message.role == \&quot;tool\&quot; %&#125;\n        &#123;%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \&quot;tool\&quot;) %&#125;\n            &#123;&#123;- &#x27;&lt;|im_start|&gt;user&#x27; &#125;&#125;\n        &#123;%- endif %&#125;\n        &#123;&#123;- &#x27;\\n&lt;tool_response&gt;\\n&#x27; &#125;&#125;\n        &#123;&#123;- message.content &#125;&#125;\n        &#123;&#123;- &#x27;\\n&lt;/tool_response&gt;&#x27; &#125;&#125;\n        &#123;%- if loop.last or (messages[loop.index0 + 1].role != \&quot;tool\&quot;) %&#125;\n            &#123;&#123;- &#x27;&lt;|im_end|&gt;\\n&#x27; &#125;&#125;\n        &#123;%- endif %&#125;\n    &#123;%- endif %&#125;\n&#123;%- endfor %&#125;\n&#123;%- if add_generation_prompt %&#125;\n    &#123;&#123;- &#x27;&lt;|im_start|&gt;assistant\\n&#x27; &#125;&#125;\n&#123;%- endif %&#125;\n&quot;,</span><br></pre></td></tr></table></figure>
<p>同样，可以从qwen1.5的<code>tokenizer_config.json</code>文件中查看qwen1.5的对话模板</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&quot;chat_template&quot;: &quot;&#123;% for message in messages %&#125;&#123;% if loop.first and messages[0][&#x27;role&#x27;] != &#x27;system&#x27; %&#125;&#123;&#123; &#x27;&lt;|im_start|&gt;system\nYou are a helpful assistant&lt;|im_end|&gt;\n&#x27; &#125;&#125;&#123;% endif %&#125;&#123;&#123;&#x27;&lt;|im_start|&gt;&#x27; + message[&#x27;role&#x27;] + &#x27;\n&#x27; + message[&#x27;content&#x27;] + &#x27;&lt;|im_end|&gt;&#x27; + &#x27;\n&#x27;&#125;&#125;&#123;% endfor %&#125;&#123;% if add_generation_prompt %&#125;&#123;&#123; &#x27;&lt;|im_start|&gt;assistant\n&#x27; &#125;&#125;&#123;% endif %&#125;&quot;,</span><br></pre></td></tr></table></figure>
<p>对比可以看出, qwen2.5和qwen1.5的对话模板是不一样的。同样，比如GPT的对话模板和Qwen更是不一样的，也就是说大模型的对话模板并没有统一标准，实际上也不能统一，因为不同大模型架构是不同的，qwen1.5和qwen2.5架构也是有区别的，所以对话模板也不同。但这样我们的输入都需要按对话模板转为后才输入到大模型，同样的输入经过不同的对话模板就会变得不同。</p>
<h5 id="2-2-llamaFactory-对话模板"><a href="#2-2-llamaFactory-对话模板" class="headerlink" title="2.2 llamaFactory 对话模板"></a>2.2 llamaFactory 对话模板</h5><p>LLamaFactory中，我们在配置的时候，有个比较奇怪的地方，我们用的是自己的模型，按理来说只需要把模型路径传进去就行，为什么还要选择它的基座模型是什么。我们注意到选择不同系列的模型，变化的除了模型路径(可以使用我们自己的模型路径替换)还有对话模板。其实这个选项主要就是为了选择对话模板，也就是说LLamaFactory内置了很多对话模板，通过模型名称选择对应的对话模板，同时我们也注意到Qwen的qwen, qwen1.5,qwen2,qwen2.5系列对应的对话模板都是一样的。</p>
<p><img src="/2025/03/30/llm-application-9/image-20250328012417165.png" alt="image-20250328012417165"></p>
<p>也就是说LLamaFactory的对话模板不是用大模型的模板，而是用自定义的模板。LLamaFactory的对话模板定义在在源码中的<code>LLaMA-Factory/src/llamafactory/data/template.py</code>文件中，比如Qwen的对话模板定义如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># copied from chatml template</span></span><br><span class="line">register_template(</span><br><span class="line">    name=<span class="string">&quot;qwen&quot;</span>,</span><br><span class="line">    format_user=StringFormatter(slots=[<span class="string">&quot;&lt;|im_start|&gt;user\n&#123;&#123;content&#125;&#125;&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n&quot;</span>]),</span><br><span class="line">    format_assistant=StringFormatter(slots=[<span class="string">&quot;&#123;&#123;content&#125;&#125;&lt;|im_end|&gt;\n&quot;</span>]),</span><br><span class="line">    format_system=StringFormatter(slots=[<span class="string">&quot;&lt;|im_start|&gt;system\n&#123;&#123;content&#125;&#125;&lt;|im_end|&gt;\n&quot;</span>]),</span><br><span class="line">    format_function=FunctionFormatter(slots=[<span class="string">&quot;&#123;&#123;content&#125;&#125;&lt;|im_end|&gt;\n&quot;</span>], tool_format=<span class="string">&quot;qwen&quot;</span>),</span><br><span class="line">    format_observation=StringFormatter(</span><br><span class="line">        slots=[<span class="string">&quot;&lt;|im_start|&gt;user\n&lt;tool_response&gt;\n&#123;&#123;content&#125;&#125;\n&lt;/tool_response&gt;&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n&quot;</span>]</span><br><span class="line">    ),</span><br><span class="line">    format_tools=ToolFormatter(tool_format=<span class="string">&quot;qwen&quot;</span>),</span><br><span class="line">    default_system=<span class="string">&quot;You are a helpful assistant.&quot;</span>,</span><br><span class="line">    stop_words=[<span class="string">&quot;&lt;|im_end|&gt;&quot;</span>],</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>所以当我们在前端选择的时候，就会注册一个对话模板，LLamaFactory的对话模板是参考官方的，但是还是不一样的，我们微调Qwen的时候，不管是qwen1.5还是qwen2.5，都是qwen的模板，微调完，我们用LLamaFactory的Chat功能，它也是用qwen的对话模板，这时候输出是正常的，但当我们使用vllm部署后，它默认使用的是模型自带的模板，就和我们微调时的模板不一样，就可能产生不一样的输出。</p>
<h5 id="2-3-vllm对话模板"><a href="#2-3-vllm对话模板" class="headerlink" title="2.3 vllm对话模板"></a>2.3 vllm对话模板</h5><p>vllm对话模板的介绍可以从其官方文档中找到<a href="https://vllm.hyper.ai/docs/serving/openai-compatible-server#聊天模板">OpenAI 兼容服务器 | vLLM 中文站</a></p>
<p>文档中给出了自定义对话模板的办法，即通过<code>--chat-template</code>定义指明对话模板文件，对话模板的格式为<code>Jinja2</code>， 它是python的模板引擎，具体介绍可在其官网找到<a href="https://docs.jinkan.org/docs/jinja2/">欢迎来到 Jinja2 — Jinja2 2.7 documentation</a></p>
<p>大部分大模型的对话模板都是用<code>Jinjia2</code>, 也有一些用<code>json</code></p>
<h5 id="2-3-OpenWebui对话模板"><a href="#2-3-OpenWebui对话模板" class="headerlink" title="2.3 OpenWebui对话模板"></a>2.3 OpenWebui对话模板</h5><p>OpenWebui会覆盖掉我们定义的模板，并且没有提供自定义对话模板，所以还没找到解决办法。</p>
<h4 id="三-解决方案"><a href="#三-解决方案" class="headerlink" title="三. 解决方案"></a>三. 解决方案</h4><h5 id="3-1-统一LLamaFactory和vllm对话模板"><a href="#3-1-统一LLamaFactory和vllm对话模板" class="headerlink" title="3.1 统一LLamaFactory和vllm对话模板"></a>3.1 统一LLamaFactory和vllm对话模板</h5><p>做法：把vllm的对话模板替换为LLamaFactory的对话模板。</p>
<p>问题：为什么不把LLamaFactory的对话模板替换为模型自带模板？</p>
<p>解答：LLamaFactory把数据处理封装成服务，我们只需要按照LLama规定的格式(<code>json</code>)，处理数据集，导入数据集后它后面还是会像之前微调<code>Bert</code> <a href="https://codezrq.github.io/2025/03/12/llm-application-3/#more">大模型应用系列(三) Bert微调-评论情感分析 | CodeRQ</a> 那样封装成dataLoader,如果把对话模板改了，封装过程可能也得改变，就得改它的源码，是比较困难的 ，这也是LLamaFactory为什么qwen1.5和qwen2.5使用相同模板，因为如果每个系列的模型都用不同模板，那要定义很多封装过程，维护成本太高。vllm提供了修改对话模板的接口，我们目的是保持vllm和微调时对话模板一致，所以我们选择更改vllm对话模板。</p>
<p>具体做法：把<strong>2.2</strong>中的对话模板转化为<code>Jinja2</code>格式，启动vllm时传入参数。</p>
<p>通过阅读LLamaFactory源码，我发现它在文件<code>LLaMA-Factory/src/llamafactory/data/template.py</code>中提供了获取<code>jinja2</code>格式对话模板的函数。</p>
<p>这个方法的功能就是传入预训练模型的名称，返回<code>jinja2</code>格式的对话模板, 这个函数定义在类<code>template</code>中，但它私有化了，我们无法调用，</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_get_jinja_template</span>(<span class="params">self, tokenizer: <span class="string">&quot;PreTrainedTokenizer&quot;</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">    prefix = <span class="variable language_">self</span>._convert_slots_to_jinja(<span class="variable language_">self</span>.format_prefix.apply(), tokenizer)</span><br><span class="line">    system_message = <span class="variable language_">self</span>._convert_slots_to_jinja(</span><br><span class="line">        <span class="variable language_">self</span>.format_system.apply(), tokenizer, placeholder=<span class="string">&quot;system_message&quot;</span></span><br><span class="line">    )</span><br><span class="line">    user_message = <span class="variable language_">self</span>._convert_slots_to_jinja(<span class="variable language_">self</span>.format_user.apply(), tokenizer)</span><br><span class="line">    assistant_message = <span class="variable language_">self</span>._convert_slots_to_jinja(<span class="variable language_">self</span>.format_assistant.apply(), tokenizer)</span><br><span class="line">    jinja_template = <span class="string">&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> prefix:</span><br><span class="line">        jinja_template += <span class="string">&quot;&#123;&#123; &quot;</span> + prefix + <span class="string">&quot; &#125;&#125;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.default_system:</span><br><span class="line">            jinja_template += <span class="string">&quot;&#123;% set system_message = &#x27;&quot;</span> + <span class="variable language_">self</span>._jinja_escape(<span class="variable language_">self</span>.default_system) + <span class="string">&quot;&#x27; %&#125;&quot;</span></span><br><span class="line"></span><br><span class="line">            jinja_template += (</span><br><span class="line">                <span class="string">&quot;&#123;% if messages[0][&#x27;role&#x27;] == &#x27;system&#x27; %&#125;&#123;% set loop_messages = messages[1:] %&#125;&quot;</span></span><br><span class="line">                <span class="string">&quot;&#123;% set system_message = messages[0][&#x27;content&#x27;] %&#125;&#123;% else %&#125;&#123;% set loop_messages = messages %&#125;&#123;% endif %&#125;&quot;</span></span><br><span class="line">                <span class="string">&quot;&#123;% for message in loop_messages %&#125;&quot;</span></span><br><span class="line">                <span class="string">&quot;&#123;% if loop.index0 == 0 and system_message is defined %&#125;&quot;</span></span><br><span class="line">                <span class="string">&quot;&#123;% set content = &quot;</span> + system_message + <span class="string">&quot; + message[&#x27;content&#x27;] %&#125;&quot;</span></span><br><span class="line">                <span class="string">&quot;&#123;% else %&#125;&#123;% set content = message[&#x27;content&#x27;] %&#125;&#123;% endif %&#125;&quot;</span></span><br><span class="line">                <span class="string">&quot;&#123;% if message[&#x27;role&#x27;] == &#x27;user&#x27; %&#125;&quot;</span></span><br><span class="line">                <span class="string">&quot;&#123;&#123; &quot;</span> + user_message + <span class="string">&quot; &#125;&#125;&quot;</span></span><br><span class="line">                <span class="string">&quot;&#123;% elif message[&#x27;role&#x27;] == &#x27;assistant&#x27; %&#125;&quot;</span></span><br><span class="line">                <span class="string">&quot;&#123;&#123; &quot;</span> + assistant_message + <span class="string">&quot; &#125;&#125;&quot;</span></span><br><span class="line">                <span class="string">&quot;&#123;% endif %&#125;&quot;</span></span><br><span class="line">                <span class="string">&quot;&#123;% endfor %&#125;&quot;</span></span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">return</span> jinja_template</span><br></pre></td></tr></table></figure>
<p>它也没有提供调用这个函数的接口，但是提供了另一个把tokenizer中的对话模板转为LLamaFactory定义的<code>jinja2</code>格式模板的方法:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">fix_jinja_template</span>(<span class="params">self, tokenizer: <span class="string">&quot;PreTrainedTokenizer&quot;</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">    <span class="string">r&quot;&quot;&quot;Replace the jinja template in the tokenizer.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> tokenizer.chat_template <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> <span class="variable language_">self</span>.replace_jinja_template:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            tokenizer.chat_template = <span class="variable language_">self</span>._get_jinja_template(tokenizer)</span><br><span class="line">        <span class="keyword">except</span> ValueError <span class="keyword">as</span> e:</span><br><span class="line">            logger.info_rank0(<span class="string">f&quot;Cannot add this chat template to tokenizer: <span class="subst">&#123;e&#125;</span>.&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>我们可以先定义一个<code>Tokenizer</code>（随便定义都可以），获取<code>template</code>对象，然后传入到<code>fix_jinja_template</code>函数中获取对话模板。具体代码如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># mytest.py</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将项目根目录添加到 Python 路径</span></span><br><span class="line">root_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))</span><br><span class="line">sys.path.append(root_dir)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> llamafactory.data.template <span class="keyword">import</span> TEMPLATES</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 初始化分词器（任意支持的分词器均可）只是为了获取一个tokenizer对象</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;/root/autodl-tmp/Qwen2.5-0.5B-Instruct&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 获取模板对象</span></span><br><span class="line">template_name = <span class="string">&quot;qwen&quot;</span>  <span class="comment"># 替换为你需要查看的模板名称，要跟LLamaFactory的名称一致</span></span><br><span class="line">template = TEMPLATES[template_name]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 修复分词器的 Jinja 模板</span></span><br><span class="line">template.fix_jinja_template(tokenizer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 直接输出模板的 Jinja 格式</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;=&quot;</span> * <span class="number">40</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Template [<span class="subst">&#123;template_name&#125;</span>] 的 Jinja 格式:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;=&quot;</span> * <span class="number">40</span>)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.chat_template)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将模板内容保存到文件</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;chat_template.jinja&quot;</span>, <span class="string">&quot;w&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> file:</span><br><span class="line">    file.write(tokenizer.chat_template)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;模板已保存为 chat_template.jinja&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>把这个文件放在<code>template.py</code>的同级目录下。最后在同级目录下生成的<code>chat_template.jinja</code>文件即为LLamaFacotory的Qwen对话模板jinja格式。</p>
<p>接着在启动vllm时通过<code>--chat-template</code> 指定对话模板:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vllm serve /root/autodl-tmp/Qwen2.5-0.5B-Instruct-ruozhiba --chat-template /root/autodl-tmp/LLaMA-Factory/src/llamafactory/data/chat_template.jinja</span><br></pre></td></tr></table></figure>
<p>通过指定对话模板，vllm输出和原先训练数据一致。</p>
<p><img src="/2025/03/30/llm-application-9/image-20250330211852058.png" alt="image-20250330211852058"></p>
<h5 id="3-2-统一LMDeploy和LLamaFactory的对话模板"><a href="#3-2-统一LMDeploy和LLamaFactory的对话模板" class="headerlink" title="3.2 统一LMDeploy和LLamaFactory的对话模板"></a>3.2 统一LMDeploy和LLamaFactory的对话模板</h5><p>LMDeploy的对话模板是<code>json</code>格式，可以将上面得到的<code>jinjia2</code>格式转化为<code>json</code>格式，具体方法见官网<a href="https://lmdeploy.readthedocs.io/zh-cn/latest/advance/chat_template.html">自定义对话模板 — lmdeploy</a></p>
<p>或者LLamaFactoyr注册的对话模板本来就是<code>json</code>格式，直接打印以下函数的输出即可即可</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># copied from chatml template</span></span><br><span class="line">register_template(</span><br><span class="line">    name=<span class="string">&quot;qwen&quot;</span>,</span><br><span class="line">    format_user=StringFormatter(slots=[<span class="string">&quot;&lt;|im_start|&gt;user\n&#123;&#123;content&#125;&#125;&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n&quot;</span>]),</span><br><span class="line">    format_assistant=StringFormatter(slots=[<span class="string">&quot;&#123;&#123;content&#125;&#125;&lt;|im_end|&gt;\n&quot;</span>]),</span><br><span class="line">    format_system=StringFormatter(slots=[<span class="string">&quot;&lt;|im_start|&gt;system\n&#123;&#123;content&#125;&#125;&lt;|im_end|&gt;\n&quot;</span>]),</span><br><span class="line">    format_function=FunctionFormatter(slots=[<span class="string">&quot;&#123;&#123;content&#125;&#125;&lt;|im_end|&gt;\n&quot;</span>], tool_format=<span class="string">&quot;qwen&quot;</span>),</span><br><span class="line">    format_observation=StringFormatter(</span><br><span class="line">        slots=[<span class="string">&quot;&lt;|im_start|&gt;user\n&lt;tool_response&gt;\n&#123;&#123;content&#125;&#125;\n&lt;/tool_response&gt;&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n&quot;</span>]</span><br><span class="line">    ),</span><br><span class="line">    format_tools=ToolFormatter(tool_format=<span class="string">&quot;qwen&quot;</span>),</span><br><span class="line">    default_system=<span class="string">&quot;You are a helpful assistant.&quot;</span>,</span><br><span class="line">    stop_words=[<span class="string">&quot;&lt;|im_end|&gt;&quot;</span>],</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h4 id="四-补充"><a href="#四-补充" class="headerlink" title="四. 补充"></a>四. 补充</h4><p>微调时用LLamFactory的模板会导致模型变差吗?</p>
<p>不会，模板只改变输出格式，不改变模型能力，我们微调时使用LLamaFactory的chat方法测试，证明在这种模板下已经获得预期效果了，所以我们部署是要做到是还原模型的效果，所以部署工具用LLamaFactoryd 对话模板。</p>
]]></content>
      <categories>
        <category>大模型应用</category>
        <category>部署</category>
      </categories>
  </entry>
  <entry>
    <title>【论文阅读】Efficient Memory Management for Large Language Model Serving with PagedAttention</title>
    <url>/2025/01/28/llm_1/</url>
    <content><![CDATA[<p>从虚拟内存的分页管理得到的灵感，用分页管理管理llm的kvcache，使其能够动态扩缩容，重用，碎片小。</p>
<span id="more"></span>
<h3 id="【论文阅读】Efficient-Memory-Management-for-Large-Language-Model-Serving-with-PagedAttention"><a href="#【论文阅读】Efficient-Memory-Management-for-Large-Language-Model-Serving-with-PagedAttention" class="headerlink" title="【论文阅读】Efficient Memory Management for Large Language Model Serving with PagedAttention"></a>【论文阅读】Efficient Memory Management for Large Language Model Serving with PagedAttention</h3><p>出处: ACM-2023   <a href="https://arxiv.org/abs/2309.06180">[2309.06180] Efficient Memory Management for Large Language Model Serving with PagedAttention (arxiv.org)</a></p>
<ul>
<li>源码: <a href="https://github.com/vllm-project/vllm.git">项目地址</a></li>
</ul>
<h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><p>高吞吐量的LLM服务需同时处理多个请求。但是现有系统非常困难，因为KV cache非常巨大并且是动态伸缩的，因为显存管理不善，导致碎片和重复，造成显存的巨大浪费，从而限制了batch的大小和吞吐量。为了解决这个问题，本文借鉴操作系统的分页内存管理方法，提出PagedAttention。基于这个方法，实现了vLLM，它能够实现：1) 接近零的KV cache浪费；2) 同一请求内和不同请求间KV cache的灵活共享。实验证明本方法的吞吐量是SOTA系统的2-4倍。</p>
<h4 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h4><p>像GPT和PaLM这样的大型语言模型(llm)的出现使编程助理和通用聊天机器人等新应用成为可能，它们开始深刻地影响我们的工作和日常生活。许多云计算公司[34,44]正在竞相提供这些应用程序作为托管服务。然而，运行这些应用程序是非常昂贵的，需要大量的硬件加速器，如gpu。</p>
<p>llm的核心是自回归的Transformer模型。该模型基于输入(提示)和迄今为止生成的输出标记的前一个序列，一次生成一个单词(token)。对于每个请求，重复这个昂贵的过程，直到模型输出一个终止令牌。这种顺序生成过程使工作负载受到内存限制，使gpu的计算能力得不到充分利用，并限制了服务吞吐量。</p>
<p> <img src="/2025/01/28/llm_1/90f34ab726a84868a23baf38716f1415.png" alt="在这里插入图片描述"></p>
<p>通过将多个请求批处理在一起，可以提高吞吐量。但是，为了批量处理许多请求，应该有效地管理每个请求的内存空间。例如，图1(左)说明了在具有40GB RAM的NVIDIA A100 GPU上13B参数LLM的内存分布。大约65%的内存分配给模型参数，这些权重在服务期间保持静态。接近30%的内存用于存储请求的动态状态。对于transformer，这些状态由与注意力机制相关的key值和value值组成，通常被称为KV cache，它表示来自早期令牌的上下文，以按顺序生成新的输出令牌。剩下的一小部分用于其他数据，包括激活——在评估LLM时产生的短暂张量。由于模型权重是恒定的，并且激活仅占用GPU内存的一小部分，因此管理KV缓存的方式对于确定最大批大小至关重要。当管理效率低下时，KV高速缓存会显著限制批处理大小，从而限制LLM的吞吐量，如图1(右)所示。</p>
<p>在本文中，我们观察到现有的LLM服务系统无法有效地管理KV缓存。这主要是因为它们将请求的KV缓存存储在连续内存空间中，因为大多数深度学习框架要求将张量存储在连续内存中。然而，与传统深度学习工作负载中的张量不同，KV cache 具有独特的特征:随着模型生成新的token，它会随着时间的推移动态增长和缩小，并且它的生命周期和长度是未知的。这些特点使现有系统的方法在两个方面显着效率低下：</p>
<p> <img src="/2025/01/28/llm_1/5b4d27afa1d44d4ba1fe56329043ad0d.png" alt="在这里插入图片描述"></p>
<p>首先，现有系统存在内部和外部内存碎片。为了在连续空间中存储请求的KV cache，它们预先分配了一个具有请求最大长度的连续内存块(例如，2048个token)。这可能导致严重的内部碎片，因为请求的实际长度可能比它的最大长度短得多(例如，图11)。此外，即使预先知道实际长度，预分配仍然是低效的:由于在请求的生命周期内保留了整个块，其他较短的请求不能利用当前未使用的块的任何部分。此外，外部内存碎片也很重要，因为每个请求的预分配大小可能不同。事实上，我们在图2中的分析结果显示，在现有系统中，只有20.4% - 38.2%的KV cache内存用于存储实际 token 状态。</p>
<p>其次，现有系统无法利用内存共享的机会。LLM服务通常使用高级解码算法，例如parallel sampling 和 beam search，每个请求生成多个输出。在这些场景中，请求由多个序列组成，这些序列可以部分共享它们的KV cache。然而，在现有系统中，内存共享是不可能的，因为序列的KV cache存储在单独的连续空间中。</p>
<p>为了解决上述限制，我们提出了PagedAttention，这是一种注意力算法，灵感来自于操作系统(OS)对内存碎片和共享的解决方案:带分页的虚拟内存。PagedAttention将请求的KV缓存划分为块，每个块可以包含固定数量的token的key和value。在PagedAttention中，KV cache的块不一定存储在连续的空间中。因此，我们可以像在OS的虚拟内存中那样更灵活地管理KV cache:可以将块视为页，token视为字节，request视为进程。这种设计通过使用相对较小的块并按需分配来减轻内部碎片。此外，它消除了外部碎片，因为所有块都具有相同的大小。最后，它支持以块粒度、跨与相同请求关联的不同序列甚至跨不同请求共享内存。</p>
<p>在这项工作中，我们在PagedAttention的基础上构建了一个高吞吐量的分布式LLM服务引擎vLLM，它在KV高速缓存中实现了接近零的浪费。vLLM使用与PagedAttention共同设计的块级内存管理和抢占式请求调度。vLLM支持GPT[5]、OPT[62]、LLaMA[52]等流行的llm，支持不同大小的llm，包括超过单个GPU内存容量的llm。我们对各种模型和工作负载的评估表明，与最先进的系统相比，vLLM将LLM服务吞吐量提高了2-4倍，而完全不影响模型的准确性。对于更长的序列、更大的模型和更复杂的解码算法，改进更加明显。</p>
<p>综上所述，我们做出了以下贡献:</p>
<ul>
<li>我们确定了在服务llm时内存分配方面的挑战，并量化了它们对服务性能的影响。</li>
<li>受操作系统中虚拟内存和分页的启发，提出了一种基于非连续分页内存中KV cache 的注意力算法PagedAttention。</li>
<li>我们设计并实现了vLLM，一个基于PagedAttention的分布式LLM服务引擎。</li>
<li>我们在各种情况下评估了vLLM，并证明它大大优于以前的核心解决方案，如FasterTransformer[31]和Orca[60]。</li>
</ul>
<h4 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h4><p>在本节中，我们描述了典型LLM的生成和服务过程，以及LLM服务中使用的iteration 调度。</p>
<h5 id="2-1-Transformer-Based-Large-Language-Models"><a href="#2-1-Transformer-Based-Large-Language-Models" class="headerlink" title="2.1 Transformer-Based Large Language Models"></a>2.1 Transformer-Based Large Language Models</h5><p>语言建模的任务是对标记列表 $(x_1, \dots, x_n)$ 的概率进行建模。由于语言具有自然的顺序排序，因此通常将整个序列的联合概率分解为条件概率的乘积(也称为自回归分解[3]):</p>
<script type="math/tex; mode=display">
P(x)=P(x_1) \cdot P(x_2|x_1)\cdot P(x_n|x_1,\cdots,x_{n-1})</script><p>Transformer[53]已经成为在大范围内对上述概率进行建模的事实上的标准架构。基于transformer的语言模型最重要的组件是它的自注意力层。对于输入隐藏层状态 $(x_1,\cdots,x_n) \in \mathbb{R}^{n\times d}$ ，自注意力层首先对每个位置向量进行线性变换，得到Query, key 和 value 向量:</p>
<script type="math/tex; mode=display">
q_i=W_qx_i,k_i=W_kx_i,v_i=W_vx_i</script><p>然后，自注意层通过将某一位置的 Query 向量与其之前的所有 key 向量相乘来计算关注分数 $a_{ij}$，并计算输出 $o_{i}$ 作为 value 向量的加权平均值:</p>
<script type="math/tex; mode=display">
a_{ij}=\frac{exp(q_i^Tk_j/\sqrt{d})}{\sum_{t=1}^i exp(q_i^Tk_t/\sqrt{d})}, o_i=\sum\limits_{j=1}^{i}a_{ij}v_{j}.</script><p>除Eq. 4中的计算外，Transformer模型中的所有其他组成部分，包括嵌入层、前馈层、层归一化[2]、剩余连接[22]、输出logit计算以及Eq. 2中的Query, key, value 值转换，都是按照位置独立应用的，形式为: $y_i=f(x_i)$。</p>
<h5 id="2-2-LLM-Service-amp-Autoregressive-Generation"><a href="#2-2-LLM-Service-amp-Autoregressive-Generation" class="headerlink" title="2.2 LLM Service &amp; Autoregressive Generation"></a>2.2 LLM Service &amp; Autoregressive Generation</h5><p>经过训练后，llm通常被部署为生成服务(例如，生成API[34]或聊天机器人[19,35])。对LLM服务的请求提供了一个输入提示token序列$(x_1,\cdots,x_n)$，LLM服务根据式(1)生成一个输出令牌列表$(x_{n+1},\cdots,x{n+T})$。我们将提示符列表和输出列表的连接称为序列。</p>
<p>LLM只能逐个采样并生成新的令牌，并且每个新token的生成过程取决于该序列中所有先前的tonken，特别是它们的key和value。在这个顺序生成过程中，通常缓存现有token的key和value向量，以生成未来的token，称为KV cache。注意，一个token的KV cache依赖于它之前的所有token。这意味着同一token在序列中出现在不同位置的KV cache 将是不同的。</p>
<p>给定一个请求prompt，LLM服务中的生成计算可以分解为两个阶段:</p>
<p><strong>The prompt phase: </strong> 将用户的整个prompt  $(x_1,\cdots,x_n)$ 作为输入计算第一个token的概率$P(x_{n+1}|x_1,\cdots,x_n)$ ，在此过程中，还生成了key向量$k_1,\cdots,k_n$ 和 value 向量$v_1,\cdots,v_n$，由于token $(x_1,\cdots,x_n)$ 都是已知的，提示阶段的计算可以使用矩阵乘法运算并行化。因此，这一阶段可以有效地利用gpu固有的并行性。</p>
<p><strong>The autoregressive generation phase: </strong> 依次生成剩余的新token，在第 t 个迭代，模型需要token $x_{n+t}$ 作为输入并使用key向量 $k_1,\cdots,k_{n+t}$ 和value 向量 $v_1,\cdots,v_{n+t}$ 计算概率 $P(x_{n+t+1}|x_1,\cdots,x_{n+t})$ ,注意，$1\to n + t-1$ 的key和value向量在之前的迭代已经缓存了，这个迭代值生成新的token的key 和 value 。当序列达到最大长度(由用户或者llm限制)或发出序列结束($<eos>$)token时，自回归阶段完成。由于数据依赖性，不同迭代的计算不能并行化，通常采用矩阵-向量乘法，效率较低。因此，这一阶段严重未充分利用GPU计算并成为内存限制，带来单个请求的大部分延迟。</eos></p>
<h5 id="2-3-Batching-Techniques-for-LLMs"><a href="#2-3-Batching-Techniques-for-LLMs" class="headerlink" title="2.3 Batching Techniques for LLMs"></a>2.3 Batching Techniques for LLMs</h5><p>通过批量处理多个请求，可以提高llm服务的计算利用率。由于请求共享相同的模型权重，因此移动权重的开销在批处理请求中平摊，并且当批处理大小足够大时，可能会被计算开销所抵消(相比变得很小)。但是，由于两个原因，将请求批处理到LLM服务是非常重要的。首先，请求可能在不同的时间到达。直接的批处理策略要么让较早的请求等待较晚的请求，要么将之后的请求延迟到较早的请求完成，从而导致严重的排队延迟。其次，请求可能具有不同的输入和输出长度(图11)。直接的批处理技术填充请求的输入和输出，以平衡它们的长度，但会浪费GPU计算和内存。</p>
<p>为了解决这个问题，人们提出了细粒度的批处理机制，如cellular batching[16]和iteration-level scheduling[60]。与在请求级别工作的传统方法不同，这些技术在迭代级别操作。在每次迭代之后，完成的请求将从批处理中删除，并添加新的请求。因此，可以在等待单个迭代后处理新请求，而不是等待整个批处理完成。</p>
<p>此外，使用特殊的GPU内核，这些技术消除了填充输入和输出的需要。通过减少排队延迟和填充带来的低效率，细粒度批处理机制显著提高了LLM服务的吞吐量。</p>
<h4 id="3-Memory-Callenge-in-LLM-Serving"><a href="#3-Memory-Callenge-in-LLM-Serving" class="headerlink" title="3. Memory Callenge in LLM Serving"></a>3. Memory Callenge in LLM Serving</h4><p>尽管细粒度批处理减少了计算浪费，并使请求能够以更灵活的方式进行批处理，但可以批处理的请求数量仍然受到GPU内存容量的限制，特别是分配给存储KV cache 的空间。换句话说，服务系统的吞吐量受内存限制。克服这种内存限制需要解决内存管理中的以下挑战:</p>
<p><strong>Large KV cache</strong> 。KV缓存大小随着请求数量的增加而快速增长。例如，对于13B参数的OPT模型[62]，单个令牌的KV缓存需要800 KB的空间，计算为2(key 和 value)× 5120(hidden state size)× 40(number of layers)× 2(bytes per FP16)。由于OPT可以生成多达2048个token的序列，因此存储一个请求的KV cache所需的内存可能高达1.6 GB。并发GPU的内存容量为几十GB。即使将所有可用内存分配给KV cache，也只能容纳几十个请求。此外，低效的内存管理会进一步减小批处理大小，如图2所示。此外，从目前的趋势来看，GPU的计算速度的增长速度超过了内存容量的增长速度。例如，从NVIDIA A100到H100, FLOPS提高了2倍以上，但GPU内存最大保持在80GB。因此，我们相信内存将成为越来越重要的瓶颈。</p>
<p><strong>Complex decoding algorithms.</strong> LLM服务提供了一系列解码算法供用户选择，每种算法对内存管理复杂性的影响各不相同。例如，当用户从单个输入提示请求多个随机样本(程序建议中的典型用例)时，提示部分的KV cache 可以共享，在我们的实验中(§6.3)，它占总KV缓存的12%，以最小化内存使用。另一方面，在自回归生成阶段，由于不同的样本结果及其对环境和位置的依赖，KV cache 应该保持不共享。<strong>KV cache 共享的程度取决于所采用的具体解码算法。</strong> 在beam search [49]等更复杂的算法中，不同的请求可以共享其KV cache 的更大部分(高达55%的内存节省，参见§6.3)，并且共享模式随着解码过程的推进而变化。</p>
<p><strong>Scheduling for unknown input &amp; output lengths.</strong> LLM服务的请求在其输入和输出长度方面表现出可变性。这就要求内存管理系统能够适应各种提示长度。此外，随着解码时请求的输出长度增加，其KV cache 所需的内存也会扩展，并且可能耗尽用于传入请求或正在生成的现有提示的内存。系统需要做出调度决策，例如从GPU内存中删除或交换某些请求的KV缓存。</p>
<h5 id="3-1-Memory-Management-in-Existing-Systems"><a href="#3-1-Memory-Management-in-Existing-Systems" class="headerlink" title="3.1 Memory Management in Existing Systems"></a>3.1 Memory Management in Existing Systems</h5><p>由于当前深度学习框架中的大多数运算符要求将张量存储在连续内存中，以前的LLM服务系统也将一个请求的KV缓存存储为跨不同位置的连续张量。由于LLM的输出长度不可预测，因此它们根据请求的最大可能序列长度静态地为请求分配一块内存，而不考虑请求的实际输入或最终输出长度。</p>
<p> <img src="/2025/01/28/llm_1/c86039995d1b47249a4346281689a445.png" alt="在这里插入图片描述"></p>
<p>图3显示了两个请求:请求A的最大可能序列长度为2048，请求B的最大可能序列长度为512。现有系统中的块预分配方案有三个主要的内存浪费来源:为未来token 保留的内存、由于过度供应潜在的最大序列长度而导致的内部碎片，以及来自内存分配器(如buddy分配器)的外部碎片。外部碎片永远不会用于生成的令牌，这在服务请求之前是已知的。内部碎片也未被使用，但这只有在请求完成采样后才会实现。它们都是纯粹的内存浪费。虽然保留的内存最终会被使用，但是在整个请求期间保留这个空间，特别是当保留的空间很大时，会占用本来可以用来处理其他请求的空间。我们在图2中可视化了我们的实验中内存浪费的平均百分比，揭示了以前系统中的实际有效内存可以低至20.4%。</p>
<p>虽然compaction[54]已经被提出作为一种潜在的碎片解决方案，但由于大量KV缓存，在性能敏感的LLM服务系统中执行压缩是不切实际的。即使使用了压缩，为每个请求预先分配的块空间也会阻止现有内存管理系统中特定于解码算法的内存共享。</p>
<h4 id="4-Method"><a href="#4-Method" class="headerlink" title="4. Method"></a>4. Method</h4><p> <img src="/2025/01/28/llm_1/062049b20dcc430d999bc89bf80af483.png" alt="在这里插入图片描述"></p>
<p>在这项工作中，我们开发了一种新的注意力算法PagedAttention，并构建了一个LLM服务引擎vLLM，以解决§3中概述的挑战。vLLM的架构如图4所示。vLLM采用集中式调度程序来协调分布式GPU工作线程的执行。KV cache 管理器以分页方式有效地管理KV cache，由PagedAttention启用。具体来说，KV cache 管理器通过集中调度程序发送的指令来管理GPU工作线程上的物理KV cache。</p>
<p>接下来，我们在§4.1中描述PagedAttention算法。我们分别在§4.2中展示KV cache 管理器的设计以及它如何促进§4.3中的PagedAttention。然后，我们展示了这种设计如何促进各种解码方法(§4.4)的有效内存管理，并处理可变长度的输入和输出序列(§4.5)。最后，我们展示了vLLM的系统设计如何在分布式设置中工作(第4.6节)。</p>
<h5 id="4-1-PageAttention"><a href="#4-1-PageAttention" class="headerlink" title="4.1 PageAttention"></a>4.1 PageAttention</h5><p>为了解决§3中的内存挑战，我们引入了PagedAttention，这是一种受操作系统中分页的经典思想启发的注意力算法[25]。与传统的注意力算法不同，PagedAttention允许在非连续的内存空间中存储连续的键和值。具体来说，PagedAttention将每个序列的KV cache 划分为KV block。每个块包含固定数量的token的key和value向量，我们将其记为KV block size。记key block为 $K_j=(k_{(j-1)B+1},\cdots,k_{jB})$ ，记value block为$V_j=(v_{(j-1)B+1},\cdots,v_{jB})$ ，注意力可以逐块计算为:</p>
<script type="math/tex; mode=display">
A_{ij}=\frac{exp(q_i^TK_j/\sqrt{d})}{\sum_{t=1}^{\lceil /B\rceil }exp(q_i^TK_t/\sqrt{d})}, o_i = \sum\limits_{j=1}^{\lceil i / B\rceil} V_j A_{ij}^T</script><p>其中$A_{ij}=(a_{i,(j-1)B+1},\cdots,a_{i,j}B)$表示第j个KV block中的行向量。在注意力计算过程中，PagedAttention内核分别识别和提取不同的KV块，我们在图5中展示了一个PagedAttention的例子:键和值向量分布在三个块上，并且这三个块在物理内存上不是连续的。每次，内核将查询tokend 的Query向量和block中的key向量$K_j$ (例如，block 0中的key向量”Four score and seven”)相乘，计算出注意力分数 $A_{ij}$， 然后将变量 $A_{ij}$ 和block中的value向量$V_j$相乘得到最终的注意力分数。 </p>
<p> <img src="/2025/01/28/llm_1/d64723720b434d668211104cb6a749cc.png" alt="在这里插入图片描述"></p>
<p>总之，PagedAttention算法允许将KV块存储在非连续的物理内存中，从而在vLLM中实现更灵活的分页内存管理。</p>
<h5 id="4-2-KV-Cache-Manager"><a href="#4-2-KV-Cache-Manager" class="headerlink" title="4.2 KV Cache Manager"></a>4.2 KV Cache Manager</h5><p>vLLM内存管理器背后的关键思想类似于操作系统中的虚拟内存[25]。操作系统将内存划分为固定大小的页面，并将用户程序的逻辑页面映射到物理页面。连续的逻辑页可以对应于非连续的物理内存页，允许用户程序访问内存，就好像它是连续的一样。此外，物理内存空间不需要提前完全预留，使操作系统可以根据需要动态分配物理页面。vLLM使用虚拟内存背后的思想来管理LLM服务中的KV缓存。通过PagedAttention，我们将KV缓存组织为固定大小的KV块，就像虚拟内存中的页面一样。</p>
<p>请求的KV cache 表示为一系列逻辑KV块，从左到右填充为生成的新token及其KV cache。最后一个KV区块的未填充位置保留给未来生成的token使用。在GPU节点上，块引擎分配一个连续的GPU DRAM块，并将其划分为物理KV块(这也在CPU RAM上完成，用于交换 ,§4.5)。KV块管理器还维护块表——每个请求的逻辑和物理KV块之间的映射。每个块表项记录一个逻辑块对应的物理块和填充位置的数量。分离逻辑和物理KV块允许vLLM动态增长KV缓存，而无需提前为所有位置保留它，这消除了现有系统中的大部分内存浪费，如图2所示。</p>
<h5 id="4-3-Decoding-with-PagedAttention-and-vLLM"><a href="#4-3-Decoding-with-PagedAttention-and-vLLM" class="headerlink" title="4.3 Decoding with PagedAttention and vLLM"></a>4.3 Decoding with PagedAttention and vLLM</h5><p> <img src="/2025/01/28/llm_1/973ddad656ae4831b5623c6cf729b0da.png" alt="在这里插入图片描述"></p>
<p>接下来，我们通过一个示例，如图6所示，来演示vLLM如何在单个输入序列的解码过程中执行PagedAttention并管理内存:</p>
<p>①就像在OS的虚拟内存中一样，vLLM不需要为最初可能生成的最大序列长度保留内存。相反，它只保留必要的KV块，以容纳在提示计算期间生成的KV缓存。在本例中，提示符有7个令牌，因此vLLM将前2个逻辑KV块(0和1)映射到2个物理KV块(分别为7和1)。在预填充步骤中，vLLM使用传统的自关注算法(例如[13])生成提示符和第一个输出令牌的KV缓存。然后，vLLM将前4个令牌的KV缓存存储在逻辑块0中，并将随后的3个令牌存储在逻辑块1中。剩余的槽保留给后续的自回归生成阶段。</p>
<p>②在第一个自回归解码步骤中，vLLM使用物理块7和1上的PagedAttention算法生成新的令牌。由于在最后一个逻辑块中仍然有一个槽可用，因此新生成的KV缓存存储在那里，并且块表的#filled记录被更新。</p>
<p>③在第二步解码时，由于最后一个逻辑块已满，vLLM将新生成的KV缓存存储在新的逻辑块中;vLLM为它分配一个新的物理块(物理块3)，并将这个映射存储在块表中。</p>
<p>全局而言，对于每次解码迭代，vLLM首先选择一组候选序列进行批处理(参见§4.5)，并为新需要的逻辑块分配物理块。然后,vLLM连接所有当前迭代的输入token作为一个序列，并将其输入LLM。在LLM的计算过程中，vLLM使用PagedAttention内核访问之前以逻辑KV块形式存储的KV缓存，并将新生成的KV缓存保存到物理KV块中。在KV块中存储多个令牌(块大小&gt; 1)使PagedAttention内核能够跨多个位置并行处理KV缓存，从而增加硬件利用率并减少延迟。然而，更大的块大小也会增加内存碎片。我们在§7.2中研究了块大小的影响。</p>
<p>同样，当生成更多令牌及其KV缓存时，vLLM会动态地将新的物理块分配给逻辑块。由于所有的块都是从左到右填充的，并且只有在之前的所有块都已满时才分配新的物理块，因此vLLM将请求的所有内存浪费限制在一个块内，因此它可以有效地利用所有内存，如图2所示。这允许将更多请求放入内存中进行批处理，从而提高吞吐量。一旦一个请求完成了它的生成，它的KV块可以被释放来存储其他请求的KV缓存。在图7中，我们展示了一个vLLM管理两个序列的内存的示例。两个序列的逻辑块映射到GPU worker中块引擎保留的空间内的不同物理块。两个序列的相邻逻辑块不需要在物理GPU内存中连续，两个序列可以有效地利用物理块的空间。</p>
<p> <img src="/2025/01/28/llm_1/e507ff7cc78044088cd9ae2ce3fff6a6.png" alt="在这里插入图片描述"></p>
<h5 id="4-4-Application-to-Other-Decoding-Scenarios"><a href="#4-4-Application-to-Other-Decoding-Scenarios" class="headerlink" title="4.4 Application to Other Decoding Scenarios"></a>4.4 Application to Other Decoding Scenarios</h5><p>§4.3展示了PagedAttention和vLLM如何处理基本的解码算法，例如贪婪解码和采样，将一个用户提示作为输入并生成单个输出序列。在许多成功的LLM应用程序中[18,34]，LLM服务必须提供更复杂的解码场景，表现出复杂的访问模式和更多的内存共享机会。在本节中，我们将展示vLLM对它们的一般适用性。</p>
<p><strong>Parallel sampling.</strong> 并行采样。在基于LLM的程序助手中[6,18]，LLM为单个输入提示生成多个采样输出;用户可以从各种候选输出中选择自己喜欢的输出。到目前为止，我们已经隐含地假设了一个请求生成单个序列。在本文的其余部分中，我们假设一个请求生成多个序列的更一般的情况。在并行采样中，一个请求包含多个共享相同输入提示的样本，从而允许共享提示的KV缓存。通过它的PagedAttention和分页内存管理，vLLM可以很容易地实现这种共享并节省内存。在本文的其余部分中，我们假设一个请求生成多个序列的更一般的情况。在并行采样中，一个请求包含多个共享相同输入提示的样本，从而允许共享提示的KV缓存。通过它的PagedAttention和分页内存管理，vLLM可以很容易地实现这种共享并节省内存。</p>
<p> <img src="/2025/01/28/llm_1/136cfecb0ff74f36abe81a6ce50bd312.png" alt="在这里插入图片描述"></p>
<p>图8示出用于两个输出的并行解码的示例。由于两个输出共享相同的提示符，我们在提示阶段只为提示符状态的一个副本保留空间;两个序列的提示符的逻辑块映射到相同的物理块:两个序列的逻辑块0和1分别映射到物理块7和1。由于单个物理块可以映射到多个逻辑块，因此我们为每个物理块引入一个引用计数。在这种情况下，物理块7和1的引用计数都是2。在生成阶段，两个输出采样不同的输出令牌，需要单独存储KV缓存。对于需要通过多个序列修改的物理块，vLLM在块粒度上实现了一种写时复制机制，类似于操作系统虚拟内存中的写时复制技术(例如，当fork一个进程时)。具体来说，在图8中，当示例A1需要写入它的最后一个逻辑块(逻辑块1)时，vLLM识别到对应的物理块(物理块1)的引用计数大于1;它分配一个新的物理块(物理块3)，指示块引擎从物理块1复制信息，并将引用计数减少到1。接下来，当示例A2写入物理块1时，引用计数已经减少到1;因此A2直接将其新生成的KV缓存写入物理块1。</p>
<p>总之，<strong>vLLM支持跨多个输出样本共享用于存储提示的KV缓存的大部分空间，但最后的逻辑块除外，该逻辑块由写时复制机制管理。</strong>通过跨多个示例共享物理块，可以大大减少内存使用，特别是对于长输入提示。</p>
<p><strong>Beam search.</strong> 在机器翻译等LLM任务中[59]，用户期望LLM输出的top-𝑘最合适的翻译。Beam search 被广泛用于解码LLM最可能的输出序列，因为它降低了完全遍历样本空间的计算复杂度。该算法依赖于波束宽度参数𝑘，该参数决定了每一步保留的最佳候选数。在解码过程中，波束搜索通过考虑所有可能的标记来扩展波束中的每个候选序列，使用LLM计算它们各自的概率，并在候选序列(长度为$k\cdot|V|$)中保留最可能的𝑘个序列，其中 $|V|$ 是词汇表大小。</p>
<p> <img src="/2025/01/28/llm_1/5a259c8fbe4647e1aea8be7242a5d386.png" alt="在这里插入图片描述"></p>
<p>与并行解码不同，波束搜索工具不仅共享初始提示块，还共享不同候选块，并且共享模式随着解码过程的推进而动态变化，类似于复合分叉在操作系统中创建的进程树。图9显示了对于𝑘= 4的波束搜索示例，vLLM如何管理KV块。在用虚线表示的迭代之前，每个候选序列已经使用了4个完整的逻辑块。所有candidate共享第一个块0(即提示符)。candidate 3从第二部分开始离题。candidate0-2共用前3个block，并在第四个block分开。在随后的迭代中，前4个可能的候选项都来自候选项1和2。由于原来的候选0和3不再是最优候选，它们的逻辑块被释放，相应的物理块的引用计数被减少。vLLM释放所有引用计数达到0的物理块(block 2,4,5,8)，然后，vLLM分配新的物理块(block 9-12)来存储来自新候选对象的新KV缓存。现在，所有candidate共享0、1、3块;candidate0和1共享区块6，candidate2和3进一步共享block7。</p>
<p>以前的LLM服务系统需要在candidate上频繁地复制KV缓存。例如，在图9所示的情况下，在虚线之后，候选3将需要复制candidate2的KV缓存的大部分以继续生成。vLLM的物理块共享大大减少了这种频繁的内存复制开销。在vLLM中，不同candidate的大部分块可以共享。只有当新生成的令牌位于旧的共享块中时，才应用写时复制机制，就像并行解码一样。这只涉及复制一个数据块。</p>
<p> <img src="/2025/01/28/llm_1/cebed53b3256400aa2b1871479e9a69e.png" alt="在这里插入图片描述"></p>
<p><strong>Shared prefix.</strong> 通常，LLM用户提供任务的(长)描述，包括指令和示例输入输出，也称为系统提示符[36]。描述与实际任务输入连接起来，形成请求提示。LLM生成的输出基于完整的提示符。图10显示了一个示例。此外，可以通过提示工程进一步调整共享前缀，以提高下游任务的准确性。</p>
<p>对于这种类型的应用程序，许多用户提示共享一个前缀，因此LLM服务提供商可以提前存储前缀的KV缓存，以减少在前缀上花费的冗余计算。在vLLM中，可以通过LLM服务提供者为一组预定义的共享前缀保留一组物理块来方便地实现这一点，正如操作系统如何跨进程处理共享库一样。带有共享前缀的用户输入提示符可以简单地将其逻辑块映射到缓存的物理块(最后一个块标记为写时复制)。提示阶段的计算只需要在用户的任务输入上执行。</p>
<p><strong>Mixed decoding methods.</strong> 前面讨论的解码方法表现出不同的内存共享和访问模式。尽管如此，vLLM促进了具有不同解码偏好的请求的同时处理，这是现有系统无法有效做到的。这是因为vLLM通过将逻辑块转换为物理块的公共映射层隐藏了不同序列之间的复杂内存共享。LLM及其执行内核只看到每个序列的物理块id列表，不需要处理跨序列的共享模式。与现有系统相比，这种方法扩大了具有不同采样要求的请求的批处理机会，最终提高了系统的总体吞吐量。</p>
<h5 id="4-5-Scheduling-and-Preemption"><a href="#4-5-Scheduling-and-Preemption" class="headerlink" title="4.5 Scheduling and Preemption"></a>4.5 Scheduling and Preemption</h5><p>当请求流量超过系统容量时，vLLM必须优先考虑请求一个子集。在vLLM中，我们对所有请求采用先到先服务(FCFS)调度策略，确保公平性并防止饥饿。当vLLM需要抢占请求时，它确保首先服务最早到达的请求，并首先抢占最新的请求。</p>
<p>LLM服务面临着一个独特的挑战:LLM的输入提示的长度可能会有很大的不同，并且结果的输出长度是未知的，这取决于输入提示和模型。随着请求数量及其输出的增长，vLLM可能会耗尽GPU的物理块来存储新生成的KV缓存。在这种情况下，vLLM需要回答两个经典问题:(1)应该驱逐哪些街区? (2)如果再次需要，如何恢复被驱逐的块?通常，驱逐策略使用启发式方法来预测哪个块将在未来被访问得最远，并驱逐该块。在我们的例子中，我们知道序列的所有块都是一起被访问的，所以我们实现了一个全有或全无的驱逐策略，即，要么驱逐序列的所有块，要么不驱逐。此外，一个请求中的多个序列(例如，一个波束搜索请求中的波束候选序列)作为一个序列组进行组调度。一个序列组中的序列总是被抢占或重新调度在一起，因为这些序列之间可能存在内存共享。为了回答第二个问题，即如何恢复被驱逐的块，我们考虑两种技术:</p>
<p><strong>Swapping.</strong> 这是大多数虚拟内存实现使用的经典技术，它将被驱逐的页面复制到磁盘上的交换空间。在本例中，我们将被驱逐的块复制到CPU内存中。如图4所示，除了GPU块分配器之外，vLLM还包含一个CPU块分配器，用于管理交换到CPU RAM的物理块。当vLLM为新令牌耗尽空闲物理块时，它会选择一组序列来驱逐并将它们的KV缓存传输到CPU。一旦它抢占了一个序列并驱逐了它的块，vLLM就会停止接受新的请求，直到所有被抢占的序列都被完成。一旦请求完成，它的块就从内存中释放出来，并将被抢占序列的块带回来继续处理该序列。请注意，在这种设计中，交换到CPU RAM的块数量永远不会超过GPU RAM中的总物理块数量，因此CPU RAM上的交换空间受到分配给KV缓存的GPU内存的限制。</p>
<p><strong>Recomputation.</strong> 在这种情况下，当被抢占的序列被重新调度时，我们只需重新计算KV缓存。请注意，重新计算延迟可以显著低于原始延迟，因为在解码时生成的token可以与原始用户提示连接为一个新的token——它们在所有位置的KV缓存可以在一个提示阶段迭代中生成。</p>
<p>交换和重计算的性能取决于CPU RAM和GPU内存之间的带宽以及GPU的计算能力。我们将在§7.3中测试交换和重计算的速度。</p>
<h5 id="4-6-Distributed-Execution"><a href="#4-6-Distributed-Execution" class="headerlink" title="4.6 Distributed Execution"></a>4.6 Distributed Execution</h5><p>许多llm的参数大小超过了单个GPU的容量[5,9]。因此，有必要将它们划分到分布式gpu上，并以模型并行的方式执行[28,63]。这需要能够处理分布式内存的内存管理器。vLLM通过支持 transformer 上广泛使用的Megatron-LM风格张量模型并行策略，在分布式设置中是有效的。</p>
<p>该策略遵循SPMD(单程序多数据)调度，其中线性层是分区的，以执行逐块矩阵乘法，gpu通过allreduce操作不断同步中间结果。具体来说，注意力 算子在注意头维度上被分割，每个SPMD过程在多头注意中负责注意头的一个子集。我们观察到，即使模型并行执行，每个模型分片仍然处理相同的一组输入令牌，因此需要KV缓存用于相同的位置。因此，vLLM在集中式调度器中具有单个KV缓存管理器，如图4所示。不同的GPU工作者共享管理器，以及从逻辑块到物理块的映射。这种公共映射允许GPU worker使用调度程序为每个输入请求提供的物理块来执行模型。虽然每个GPU worker都有相同的物理块id，但一个工作线程只为其相应的注意头存储一部分KV缓存。</p>
<p>在每个步骤中，调度器首先为批处理中的每个请求准备带有输入token id的消息，并为每个请求准备块表。接下来，调度器将这个控制消息广播给GPU worker。然后，GPU worker 开始使用输入token id执行模型。在注意层，GPU工作人员根据控制消息中的块表读取KV缓存。在执行过程中，GPU worker 中间结果与all-reduce通信原语同步，而不需要调度程序的协调。最后，GPU工作器将这次迭代的采样token发送回调度器。总之，GPU工作人员不需要同步内存管理，因为他们只需要在每次解码迭代开始时接收所有内存管理信息以及step输入。</p>
<h4 id="5-Implementation"><a href="#5-Implementation" class="headerlink" title="5. Implementation"></a>5. Implementation</h4><p>vLLM是一个端到端服务系统，采用FastAPI[15]前端和基于gpu的推理引擎。前端扩展了OpenAI API[34]接口，允许用户为每个请求定制采样参数，如最大序列长度和beam width 𝑘。vLLM引擎是用8.5k行Python和2K行c++ /CUDA代码编写的。我们在Python中开发控制相关组件，包括调度器和块管理器，同时为关键操作(如PagedAttention)开发自定义CUDA内核。对于模型执行器，我们使用PyTorch和Transformer实现流行的llm，如GPT[5]、OPT[62]和LLaMA [52]我们使用NCCL[32]在分布式GPU worker之间进行张量通信。</p>
<h5 id="5-1-Kernel-level-Optimization"><a href="#5-1-Kernel-level-Optimization" class="headerlink" title="5.1 Kernel-level Optimization"></a>5.1 Kernel-level Optimization</h5><p>由于PagedAttention引入了现有系统无法有效支持的内存访问模式，我们开发了几个GPU内核来优化它。</p>
<p>(1)Fused reshape and block write. 在每个Transformer层中，新的KV缓存被分割成块，重新塑造为块读取优化的内存布局，然后保存在块表指定的位置。为了最小化内核启动开销，我们将它们融合到一个内核中。</p>
<p>(2)Fusing block read and attention.我们采用FasterTransformer[31]中的注意力内核，根据块表读取KV缓存，并动态执行注意力操作。为了确保合并内存访问，我们分配了一个GPU warp来读取每个块。此外，我们还增加了对请求批处理中可变序列长度的支持。</p>
<p>(3)Fused block copy.由写时拷贝机制发出的块拷贝操作可以在不连续的块上操作。如果我们使用cudamempyasync API，这可能导致大量的小数据移动调用。为了减少开销，我们实现了一个内核，它将不同块的复制操作批处理到单个内核启动中。</p>
<h5 id="5-2-Supporting-Various-Decoding-Algrithms"><a href="#5-2-Supporting-Various-Decoding-Algrithms" class="headerlink" title="5.2 Supporting Various Decoding Algrithms"></a>5.2 Supporting Various Decoding Algrithms</h5><p>vLLM使用三个关键方法实现各种解码算法:fork、append和free。fork方法从一个现有序列创建一个新序列。append方法向序列追加一个新标记。最后，free方法删除序列。例如，在并行采样中，vLLM使用fork方法从单个输入序列创建多个输出序列。然后在每次迭代中使用append向这些序列添加新的标记，并使用free删除满足停止条件的序列。vLLM在波束搜索和前缀共享中也采用了相同的策略。我们相信结合这些方法也可以支持未来的解码算法。</p>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
        <category>大语言模型</category>
      </categories>
      <tags>
        <tag>paper notes</tag>
        <tag>llm</tag>
      </tags>
  </entry>
  <entry>
    <title>使用Monkey进行软件测试（随机测试+脚本测试）</title>
    <url>/2025/02/09/monkeyTest/</url>
    <content><![CDATA[<p>使用Monkey测试进行软件测试，包括随机测试和脚本测试。</p>
<span id="more"></span>
<p>[toc]</p>
<h3 id="一、基础概念"><a href="#一、基础概念" class="headerlink" title="一、基础概念"></a>一、基础概念</h3><p>所谓的猴子测试（Money Test），也称搞怪测试，怪用测试，指在软件测试中﹐测试者可以进行各种稀奇古怪的操作模式，用以测试软件的稳定度。猴子测试，通俗来讲是一种系统对信号因子输入稳健性的测试方法。一般用于计算机软件程序这样的逻辑严密性要求高的系统。</p>
<p>猴子测试之所以会广泛用于软件BUG测试，是因为系统的“可重复性”以及系统输入因子“有限性”和“单纯性”。</p>
<p>通常情况下，复杂的测试会比简单的测试找到更多的bug。但是大部分的自动化测试都是简单的。我们期待一个输入后得到一个输出，然后程序回到一个已知的基本状态，然后我们再去执行另外一个简单的测试。我们设计测试用例一般都是按照一定的逻辑顺序，是经过深思熟虑的，但是这样的测试仍然是简单的测试。当我们回到程序的基本状态，则丢弃了前面测试的“历史”。而真正的用户不会这样操作，他们把一系列简单的动作串起来，形成一个复杂的动作流。</p>
<p>　　我们的简单测试不会模拟那些用户行为。因此如果一个简单的动作引起了另外一个动作的失败，我们的简单测试不会找到那个bug…因此，我们需要使用复杂序列的、以前未使用过的测试，在这方面猴子比人更有效。</p>
<p>　　猴子测试就是百般刁难，乱按一通，系統也不能宕机或者数据出现差错，这样才能称得上是经得起考验的程序。</p>
<h3 id="二、monkey测试的优缺点"><a href="#二、monkey测试的优缺点" class="headerlink" title="二、monkey测试的优缺点"></a>二、monkey测试的优缺点</h3><h4 id="2-1-优点"><a href="#2-1-优点" class="headerlink" title="2.1 优点"></a>2.1 优点</h4><p>简单易用，方便快捷，并且理论上可以测试到所有bug,因为理论上只要次数最够多，所有事件都会发生。</p>
<h4 id="2-2缺点"><a href="#2-2缺点" class="headerlink" title="2.2缺点"></a>2.2缺点</h4><ul>
<li><p><strong>遍历界面有限。</strong> 在monkey测试中，由于事件的随机性，使得monkey容易卡在某些简单页面，比如登陆页面这种可操作内容很少的页面。导致测试效果不佳。测试有效性大打折扣。</p>
</li>
<li><p><strong>无法得知Bug的复现步骤</strong>。由于Monkey的随机性，如果Bug是由于事件发生的特定序列产生的，往往很难复现Bug。</p>
</li>
<li><p><strong>路径回环</strong>。由于monkey太过随机，最后根本无法控制，很容易陷于一个页面无法出来，或者陷入某个无关紧要的地方无法出来，导致测试结果并不具有很好的意义。这也是导致遍历界面有限的原因。</p>
</li>
</ul>
<h4 id="2-3-解决方案"><a href="#2-3-解决方案" class="headerlink" title="2.3 解决方案"></a>2.3 解决方案</h4><h5 id="2-3-1-二次开发"><a href="#2-3-1-二次开发" class="headerlink" title="2.3.1 二次开发"></a>2.3.1 二次开发</h5><p>对monkey进行二次开发，例如maxin，可以通过一些黑白名单控制，或者输入指定事件流，或者指定不同的测试随机模式，深度优先或者控件识别等，同时加入一些熔断机制，在一个地方执行了太多次后可以自动触发熔断并拉起。但是这样还是会进入死循环，因为仍然不能解决路径回环的问题。</p>
<h5 id="2-3-2-指定测试页面"><a href="#2-3-2-指定测试页面" class="headerlink" title="2.3.2 指定测试页面"></a>2.3.2 指定测试页面</h5><p>我们可以指定测试哪些页面，但是发现如果指定某几个Activity，虽然不会陷入路径回环，但随机的意义又不是那么大了，如果在几个页面进行随机，并且页面深度不是很深，那一直在这些页面测试也没有多大意义</p>
<h5 id="2-3-3-调整各种事件的比例"><a href="#2-3-3-调整各种事件的比例" class="headerlink" title="2.3.3 调整各种事件的比例"></a>2.3.3 调整各种事件的比例</h5><p>我们可以根据应用的特点，适当调整各种事件的比例，adb也提供了这样的命令参数。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">--pct-trackball &lt;percent&gt;</span><br><span class="line"></span><br><span class="line">作用：调整滚动球事件百分比。（滚动球事件由一个或多个随机的移动事件组成，有时会伴随着点击事件）如不规则滑动解锁</span><br><span class="line"></span><br><span class="line">--pct-nav&lt;percent&gt; 导航，现在手机基本没有导航了</span><br><span class="line"></span><br><span class="line">---pct-syskeys&lt;percent&gt; 按键消息比例，主页、后退、音量增减</span><br><span class="line"></span><br><span class="line">--pct-anyevent&lt;percent&gt; 其他不常用的按键比例的设置，不常用</span><br></pre></td></tr></table></figure>
<h5 id="2-3-3-编写自定义测试脚本"><a href="#2-3-3-编写自定义测试脚本" class="headerlink" title="2.3.3 编写自定义测试脚本"></a>2.3.3 编写自定义测试脚本</h5><p>Monkey测试产生以上问题的原因就是太随机了，所以我们如果降低甚至抑制这种随机性，就可以避免上述的问题，调整各种事件的比例虽然降低了随机性，但仍可能存在上述问题。基于这个原因，我们可以编写特定的脚本，让事件依据我们设定的顺序发生，这样就可以解决上述问题：由于我们可以设定事件发生顺序，自然可以到达任意深度的页面，也可以定位出Bug。</p>
<p>但这种方式只适用于我们已经有明确的思路，特别想测试APP中某些功能。</p>
<h5 id="2-3-4-分析APP特性，选择合适的方案"><a href="#2-3-4-分析APP特性，选择合适的方案" class="headerlink" title="2.3.4 分析APP特性，选择合适的方案"></a>2.3.4 分析APP特性，选择合适的方案</h5><p>可以看出，测试的随机性和上述问题的解决是很难兼得的，我们必须根据我们应用的特性选择合适的方案。</p>
<p>对于<strong>页面功能不多，控件简单</strong>的应用，我们可以采用自定义脚本进行测试，因为在这种情况下测试所有功能也是可能的。</p>
<p>对于<strong>页面结构比较规律</strong>的应用，比如直播软件，每个页面结构相对固定（直播间都长一样），或者电商app(每种商品的页面大致相同)。由于随机性，所以点击不会一直在同一个地方进行点击，所以不容易一直卡在同一个页面。</p>
<h3 id="三、monkey测试的基本过程"><a href="#三、monkey测试的基本过程" class="headerlink" title="三、monkey测试的基本过程"></a>三、monkey测试的基本过程</h3><h4 id="3-1-环境准备"><a href="#3-1-环境准备" class="headerlink" title="3.1 环境准备"></a>3.1 环境准备</h4><h5 id="3-1-1环境"><a href="#3-1-1环境" class="headerlink" title="3.1.1环境"></a>3.1.1环境</h5><ul>
<li>电脑系统：window10</li>
<li>手机系统：Android 10</li>
<li>JDK版本：1.8</li>
<li>SDK版本：1.0.41</li>
</ul>
<h5 id="3-1-2-配置过程"><a href="#3-1-2-配置过程" class="headerlink" title="3.1.2 配置过程"></a>3.1.2 配置过程</h5><p> 每台android手机里都有Monkey工具，但是我们是看不到的，因为，Monkey不是一个可视化的工具。我们需要借助ADB才能与Monkey进行通讯。</p>
<p>ADB全名为Android Debug Bridge ,安卓调试桥，是实现电脑设备和手机沟通的桥梁。在开始Monkey测试之前，我们需要搭建环境。第一个前提条件就是安装Java JDK，第二个条件就是安装android SDK。</p>
<ul>
<li>下载jdk   具体安装流程参考：<a href="https://blog.csdn.net/write6/article/details/79136388">https://blog.csdn.net/write6/article/details/79136388</a></li>
<li>再下载sdk  具体安装流程参考：<a href="https://blog.csdn.net/u011541946/article/details/77142045">https://blog.csdn.net/u011541946/article/details/77142045</a></li>
</ul>
<p>配置成功后，打开cmd，输入以下命令：</p>
<p><code>adb</code></p>
<p>出现以下画面：<br><img src="/2025/02/09/monkeyTest/185e7f152230905f34ff718b101dda9f.png" alt="在这里插入图片描述"></p>
<p>表示配置成功。</p>
<h4 id="3-2-测试adb程序"><a href="#3-2-测试adb程序" class="headerlink" title="3.2 测试adb程序"></a>3.2 测试adb程序</h4><h5 id="3-2-1-连接设备"><a href="#3-2-1-连接设备" class="headerlink" title="3.2.1 连接设备"></a>3.2.1 连接设备</h5><p>配置成功后，打开手机的开发者模式，然后打开USB调试，并用数据线连接手机和电脑，然后在cmd中输入以下命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">adb devices</span><br></pre></td></tr></table></figure>
<p>出现以下画面，表示连接成功</p>
<p><img src="/2025/02/09/monkeyTest/8c9f5778443ccdc1e83f5c735df87f09.png" alt="在这里插入图片描述"></p>
<h5 id="3-2-2-获取包名"><a href="#3-2-2-获取包名" class="headerlink" title="3.2.2 获取包名"></a>3.2.2 获取包名</h5><p><strong>方法一：</strong></p>
<p>在cmd中输入以下命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">adb shell pm list packages</span><br></pre></td></tr></table></figure>
<p>可以查看手机所有的安装包（以下只显示部分）可以通过名称找出你要评测的包<br><img src="/2025/02/09/monkeyTest/88af5cfc35b5ffe9c343d6041f36f213.png" alt="在这里插入图片描述"></p>
<p><strong>方法二</strong></p>
<p>输入</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">adb shell pm list packages -3</span><br></pre></td></tr></table></figure>
<p>可以查看手机上所有的第三方安装包，可以通过名称找出你要评测的包</p>
<p><img src="/2025/02/09/monkeyTest/41dd5f6a411c148bd55f66d8092fe1cf.png" alt="在这里插入图片描述"></p>
<p><strong>方法三</strong></p>
<p>输入命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">adb shell logcat|findstr &quot;Displayed</span><br></pre></td></tr></table></figure>
<p>同时打开待测的APP，出现以下画面，这里以今日头条为例<br><img src="/2025/02/09/monkeyTest/710185dc577e87c7df0d89f6eb7bf7ee.png" alt="在这里插入图片描述"></p>
<p>其中<code>com.ss.android.article.news</code>表示包名之后是APPactivity，可用于自动化评测。</p>
<h5 id="3-2-3-使用monkey测试"><a href="#3-2-3-使用monkey测试" class="headerlink" title="3.2.3 使用monkey测试"></a>3.2.3 使用monkey测试</h5><p><strong>对整机进行测试</strong></p>
<p>使用<code>adb shell monkey -v 100</code>对整机进行测试</p>
<p><img src="/2025/02/09/monkeyTest/29cea00790089158dd8837dcaf3f7e65.png" alt="在这里插入图片描述"></p>
<p><strong>对指定应用进行测试</strong></p>
<p>找到包名然后使用一个简单的命令进行monkey测试，</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">adb shell monkey -p com.ss.android.article.news -v -v -v 100</span><br></pre></td></tr></table></figure>
<p>其中</p>
<p><code>-p</code>表示指定测试的程序</p>
<p><code>-v</code>表示查看monkey的执行日志，其中<code>-v</code>越多，表示信息越详细。</p>
<p>其分别对应三个等级：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Level 0(缺省值)</span><br><span class="line">除启动提示、测试完成和最终结果之外，提供较少信息。</span><br><span class="line">Level 1</span><br><span class="line">提供较为详细的测试信息，如逐个发送到Activity的事件。</span><br><span class="line">Level 2</span><br><span class="line">提供更加详细的设置信息，如测试中被选中的或未被选中的Activity</span><br></pre></td></tr></table></figure>
<p><code>100</code>表示执行的测试事件为100个</p>
<p>执行后会出现详细的信息，同时会发现，手机App界面自动进行了100次随机操作。</p>
<p><img src="/2025/02/09/monkeyTest/6bf7cad61e642c83fced0d0f8a5b071a.png" alt="在这里插入图片描述"></p>
<h5 id="3-2-4-日志内容解析"><a href="#3-2-4-日志内容解析" class="headerlink" title="3.2.4 日志内容解析"></a>3.2.4 日志内容解析</h5><p>Monkey运行时输出的日志一般包含四类信息，分别是测试命令信息、伪随机事件流信息、异常信息、Monkey执行结果信息。</p>
<p><strong>测试命令信息</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">-----随机种子，执行事件的数目-----</span><br><span class="line">:Monkey: seed=1639214805777 count=100</span><br><span class="line"></span><br><span class="line">-----可运行的应用列表-----</span><br><span class="line">:AllowPackage: com.ss.android.article.news</span><br><span class="line"></span><br><span class="line">-----表示启动的活动-----</span><br><span class="line">:IncludeCategory: android.intent.category.LAUNCHER</span><br><span class="line">:IncludeCategory: android.intent.category.MONKEY</span><br><span class="line"></span><br><span class="line">-----表示打开的活动-----</span><br><span class="line">// Selecting main activities from category android.intent.category.LAUNCHER</span><br><span class="line">// Seeded: 1639214805777</span><br><span class="line"></span><br><span class="line">-----各事件的百分比-----</span><br><span class="line">// Event percentages:</span><br><span class="line">//   0: 15.0%</span><br><span class="line">//   1: 10.0%</span><br><span class="line">//   2: 2.0%</span><br><span class="line">//   3: 15.0%</span><br><span class="line">//   4: -0.0%</span><br><span class="line">//   5: -0.0%</span><br><span class="line">//   6: 25.0%</span><br><span class="line">//   7: 15.0%</span><br><span class="line">//   8: 2.0%</span><br><span class="line">//   9: 2.0%</span><br><span class="line">//   10: 1.0%</span><br><span class="line">//   11: 13.0%</span><br></pre></td></tr></table></figure>
<p>日志中会显示进行各种操作的百分比：其含义会随着Android版本的不同而不同，为了了解其含义，首先使用如下命令查看Android版本</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">adb shell getprop ro.build.version.release</span><br></pre></td></tr></table></figure>
<p>我的查询结果为</p>
<p> <img src="/2025/02/09/monkeyTest/2cae79e3bbbf41746788345a740664ad.png" alt="在这里插入图片描述"></p>
<p>通过阅读对应版本的money源码中的MonkeySourceRandom.java文件，可以看到其Event序列是</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">FACTOR_TOUCH</span>        <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">FACTOR_MOTION</span>       <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">FACTOR_PINCHZOOM</span>    <span class="operator">=</span> <span class="number">2</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">FACTOR_TRACKBALL</span>    <span class="operator">=</span> <span class="number">3</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">FACTOR_ROTATION</span>     <span class="operator">=</span> <span class="number">4</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">FACTOR_NAV</span>          <span class="operator">=</span> <span class="number">5</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">FACTOR_MAJORNAV</span>     <span class="operator">=</span> <span class="number">6</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">FACTOR_SYSOPS</span>       <span class="operator">=</span> <span class="number">7</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">FACTOR_APPSWITCH</span>    <span class="operator">=</span> <span class="number">8</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">FACTOR_FLIP</span>         <span class="operator">=</span> <span class="number">9</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">FACTOR_ANYTHING</span>     <span class="operator">=</span> <span class="number">10</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">FACTORZ_COUNT</span>       <span class="operator">=</span> <span class="number">11</span>;    <span class="comment">// should be last+1</span></span><br></pre></td></tr></table></figure>
<p>所以对应的事件解释为</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">//   0: 15.0%   触摸事件TOUCH，--pct-touch</span><br><span class="line">//   1: 10.0%   手势事件MOTION, --pct-motion</span><br><span class="line">//   2: 2.0%    两指缩放事件PINCHZOOM,--pct-pinchzoom</span><br><span class="line">//   3: 15.0%   轨迹球事件TRACKBALL,--pct-trackball</span><br><span class="line">//   4: -0.0%   屏幕旋转事件ROTATION, --pct-rotation</span><br><span class="line">//   5: 25.0%   基本导航事件nav, --pct-nav</span><br><span class="line">//   6: 15.0%   主要导航事件majornav, --pct-majornav</span><br><span class="line">//   7: 2.0%    系统按钮事件sysops, --pct-syskeys</span><br><span class="line">//   8: 2.0%    启动activity事件appswitch, --pct-appswitch</span><br><span class="line">//   9: 1.0%    键盘轻弹事件flip, --pct-flip</span><br><span class="line">//   10: 13.0%   其它事件,包括按键和不常用的按键，--pct-anyevent</span><br></pre></td></tr></table></figure>
<p><strong>伪随机时间流信息</strong></p>
<p>Monkey开始执行测试后，会顺序输出执行的事件流信息。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">-----跳转到com.android.article.news里的activity.MainActivity这个活动-----</span><br><span class="line">:Switch: #Intent;action=android.intent.action.MAIN;category=android.intent.category.LAUNCHER;launchFlags=0x10200000;component=com.ss.android.article.news/.activity.MainActivity;end</span><br><span class="line"></span><br><span class="line">-----允许启动com.android.article.news里的activity.MainActivity这个活动-----</span><br><span class="line">    // Allowing start of Intent &#123; act=android.intent.action.MAIN cat=[android.intent.category.LAUNCHER] cmp=com.ss.android.article.news/.activity.MainActivity &#125; in package com.ss.android.article.news</span><br><span class="line">    </span><br><span class="line">-----延迟(可以在命令参数中设置延迟)-----</span><br><span class="line">Sleeping for 0 milliseconds</span><br><span class="line"></span><br><span class="line">-----触摸事件-----</span><br><span class="line">:Sending Touch (ACTION_DOWN): 0:(842.0,1801.0)</span><br><span class="line"></span><br><span class="line">-----基本导航事件-----</span><br><span class="line">C:\Users\86156&gt;adb shell monkey -p com.ss.android.article.news -v -v -v 100</span><br></pre></td></tr></table></figure>
<p><strong>异常信息</strong></p>
<p>当Monkey执行过程中遇到错误时，会输出对应的异常信息。由于本次实验的软件没有遇到错误，所以没有输出信息。</p>
<p><strong>执行结果信息</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">-----产生了100次事件-----Events injected: 100-----表示屏幕旋转信息-----:Sending rotation degree=0, persist=false-----表示丢弃的事件信息-----:Dropped: keys=1 pointers=0 trackballs=0 flips=0 rotations=0-----网络状态-----## Network stats: elapsed time=700ms (0ms mobile, 0ms wifi, 700ms not connected)----最终执行结果-----// Monkey finished</span><br></pre></td></tr></table></figure>
<h4 id="3-3脚本自动化测试"><a href="#3-3脚本自动化测试" class="headerlink" title="3.3脚本自动化测试"></a>3.3脚本自动化测试</h4><p>3.2中实现了使用monkey进行随机测试的过程，但有时候我们需要根据我们的需求进行测试，这时候就要设计特定的测试样例。</p>
<p>monkey提供了一系列脚本命令以便于我们根据需求设计测试样例，主要有</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">LaunchActivity(pkg_name, cl_name)： 启动应用，第一个参数DispatchPress(keycode)： 向系统发送一个固定的按键事件；例如home键，back键；参数是按键值 ，按键值可查看keycodeUserWait：让脚本的执行暂停一段时间，做一个等待操作RotateScreen(rotationDegree, persist)： 翻转屏幕，第一个参数是旋转角度，第二个是旋转后是否停在当前位置Tap(x, y) ：单击事件，点击屏幕，参数是点击坐标Drag(xStart, yStart, xEnd, yEnd) ：在屏幕上滑动，坐标是从哪一点滑到哪一点DispatchString(input)： 输入字符串RunCmd(cmd) ：执行shell命令，比如截图 screencap -p /data/local/tmp/tmp.pngDispatchFlip(true/false) ：打开或者关闭软键盘UserWait(sleepTime) ：睡眠指定时间</span><br></pre></td></tr></table></figure>
<h5 id="3-3-1需求分析"><a href="#3-3-1需求分析" class="headerlink" title="3.3.1需求分析"></a>3.3.1需求分析</h5><p>本次实验中主要实现以下<strong>测试案例</strong>:</p>
<p>打开今日头条，点击搜索框，输入”abc”，然后点击搜索。播放第一视频，并在播放过程中调整音量（增大和减小）。</p>
<p>通过资料中的脚本命令可知，输入字符串可以是使用<code>DispatchString(input)</code>函数，但我们还要点击搜索框和搜索按键，所以要获取对应按键的坐标。最后可以使用<code>RunCmd(Cmd)</code>进行截图。</p>
<h5 id="3-3-2-获取坐标"><a href="#3-3-2-获取坐标" class="headerlink" title="3.3.2 获取坐标"></a>3.3.2 获取坐标</h5><p>在保持设备连接的情况下。执行以下命令获取当前事件，</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">adb shellgetevent</span><br></pre></td></tr></table></figure>
<p><img src="/2025/02/09/monkeyTest/9bfd79be0c193bd45e56dd091858f80b.png" alt="在这里插入图片描述"></p>
<p>然后点击屏幕对应区域，可以看到如下信息</p>
<p><img src="https://i-blog.csdnimg.cn/blog_migrate/71b40115b83fc42b5016a11583700df3.png#pic_center" alt="在这里插入图片描述"></p>
<p>有四列信息，分别为：设备名称，Code，Type，Value.<br>Code是指：该事件是什么。<br>Type是指：事件的相关参数<br>Value是指：事件参数的值。</p>
<p>code值：</p>
<p>0003—&gt;绝对坐标</p>
<p>0000—&gt;同步事件。代表某一操作的完成。</p>
<p>0001—&gt;key_broad。</p>
<p>0002—&gt;相对坐标。</p>
<p>Type值，我们只关注两个值</p>
<p>0035—&gt;绝对坐标X<br>0036—&gt;绝对坐标Y</p>
<p>value：表示对应的值</p>
<p>例子中点击的坐标为<code>(0x1d2=466,0xad=173)</code></p>
<h5 id="3-3-3编写脚本"><a href="#3-3-3编写脚本" class="headerlink" title="3.3.3编写脚本"></a>3.3.3编写脚本</h5><p>根据需求已经获取的信息，编写以下脚本</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Start of Script   <span class="built_in">type</span>= user   count= 10   speed= 1.0   start data &gt;&gt;   <span class="comment">#open the appLaunchActivity(com.ss.android.article.news, com.ss.android.article.news.activity.MainActivity)   UserWait(8000)Tap(576, 239)UserWait(2000) DispatchString(abc)UserWait(2000) Tap(1298, 216)UserWait(2000) Tap(746,1277)UserWait(8000) #turn up and down the volumeDispatchPress(24)UserWait(1000) DispatchPress(24)UserWait(1000) DispatchPress(24)UserWait(1000) DispatchPress(24)UserWait(1000) DispatchPress(24)UserWait(3000) DispatchPress(25)UserWait(1000) DispatchPress(25)UserWait(1000) DispatchPress(25)UserWait(1000)</span></span> </span><br></pre></td></tr></table></figure>
<h5 id="3-3-4-执行脚本"><a href="#3-3-4-执行脚本" class="headerlink" title="3.3.4 执行脚本"></a>3.3.4 执行脚本</h5><p>因为Monkey是运行在设备上的，所以需要将脚本先传到设备上，</p>
<p>通过<code>adb push monkey.txt sdcard/monkey.txt</code>将文件推送到手机sd卡上</p>
<p>然后通过<code>adb shell monkey -f sdcard/monkey.txt -v 1</code> 执行脚本文件<br>执行日志如下：</p>
<p><img src="/2025/02/09/monkeyTest/89f6d67d9cce2664e60cfde1943cd2ad.png" alt="在这里插入图片描述"></p>
<p>可以看到，每个事件都成功执行了。</p>
]]></content>
      <categories>
        <category>配置</category>
      </categories>
      <tags>
        <tag>configure</tag>
      </tags>
  </entry>
  <entry>
    <title>markdown 常用符号大全</title>
    <url>/2025/02/03/markdown-note/</url>
    <content><![CDATA[<p>markdown 常用符号大全，包括数学符号，逻辑符号，运算符号，多行公式，希腊字母，矩阵，括号，表格，真值表、流程图</p>
<span id="more"></span>
<h4 id="标题符号"><a href="#标题符号" class="headerlink" title="标题符号"></a>标题符号</h4><div class="table-container">
<table>
<thead>
<tr>
<th>语法</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>#</td>
<td>一级标题</td>
</tr>
<tr>
<td>##</td>
<td>二级标题(# 越多，级数越多，依次类推)</td>
</tr>
<tr>
<td>+</td>
<td>无序列表(可嵌套)</td>
</tr>
</tbody>
</table>
</div>
<h4 id="数学符号"><a href="#数学符号" class="headerlink" title="数学符号"></a>数学符号</h4><div class="table-container">
<table>
<thead>
<tr>
<th>含义</th>
<th>语法</th>
<th>效果</th>
</tr>
</thead>
<tbody>
<tr>
<td>点乘</td>
<td>a \times b</td>
<td>$a\times b$</td>
</tr>
<tr>
<td>叉乘</td>
<td>a \cdot b</td>
<td>$a \cdot b$</td>
</tr>
<tr>
<td>内积</td>
<td>\langle a,b \rangle</td>
<td>$\langle a, b\rangle$</td>
</tr>
<tr>
<td>外积</td>
<td>a \otimes b</td>
<td>$a\otimes b$</td>
</tr>
<tr>
<td>任意</td>
<td>\forall</td>
<td>$\forall$</td>
</tr>
<tr>
<td>存在</td>
<td>\exists</td>
<td>$\exists$</td>
</tr>
</tbody>
</table>
</div>
<h4 id="花体字母"><a href="#花体字母" class="headerlink" title="花体字母"></a>花体字母</h4><p>以字母A为例，其他字母类似</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>语法</th>
<th>效果</th>
</tr>
</thead>
<tbody>
<tr>
<td>\mathbb{A}</td>
<td>$\mathbb{R}$</td>
</tr>
<tr>
<td>\mathcal{A}</td>
<td>$\mathcal{A}$</td>
</tr>
<tr>
<td>\mathscr{A}</td>
<td>$\mathscr{A}$</td>
</tr>
<tr>
<td>\mathrm{A}</td>
<td>$\mathrm{A}$</td>
</tr>
<tr>
<td>\mathbf{A}</td>
<td>$\mathbf{A}$</td>
</tr>
<tr>
<td>\mathit{A}</td>
<td>$\mathit{A}$</td>
</tr>
<tr>
<td>\mathtt{A}</td>
<td>$\mathtt{A}$</td>
</tr>
<tr>
<td>\mathfrak{A}</td>
<td>$\mathfrak{A}$</td>
</tr>
<tr>
<td>\mathsf{A}</td>
<td>$\mathsf{A}$</td>
</tr>
</tbody>
</table>
</div>
<h4 id="分段函数"><a href="#分段函数" class="headerlink" title="分段函数"></a>分段函数</h4><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">$f(x) = \begin&#123;cases&#125;  </span><br><span class="line">x^2 + 2x + 1, &amp; \text&#123;当 &#125; x &lt; 0 \text&#123; 时&#125; \\  </span><br><span class="line">3x + 2, &amp; \text&#123;当 &#125; 0 \leq x &lt; 1 \text&#123; 时&#125; \\  </span><br><span class="line">x + 3, &amp; \text&#123;当 &#125; x \geq 1 \text&#123; 时&#125;  </span><br><span class="line">\end&#123;cases&#125;$</span><br></pre></td></tr></table></figure>
<p>$f(x) = \begin{cases}<br>x^2 + 2x + 1, &amp; \text{当 } x &lt; 0 \text{ 时} \\<br>3x + 2, &amp; \text{当 } 0 \leq x &lt; 1 \text{ 时} \\<br>x + 3, &amp; \text{当 } x \geq 1 \text{ 时}<br>\end{cases}$</p>
<h4 id="多行公式"><a href="#多行公式" class="headerlink" title="多行公式"></a>多行公式</h4><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">$\begin&#123;aligned&#125;</span><br><span class="line">s&amp;=\frac&#123;1&#125;&#123;2&#125;(a+b)h\\</span><br><span class="line">&amp;=\frac&#123;1&#125;&#123;2&#125;(8+12)·6\\</span><br><span class="line">&amp;=10·6=60</span><br><span class="line">\end&#123;aligned&#125;$</span><br></pre></td></tr></table></figure>
<p>$\begin{aligned}<br>s&amp;=\frac{1}{2}(a+b)h\\<br>&amp;=\frac{1}{2}(8+12)·6\\<br>&amp;=10·6=60<br>\end{aligned}$</p>
<h4 id="数学符号-1"><a href="#数学符号-1" class="headerlink" title="数学符号"></a>数学符号</h4><div class="table-container">
<table>
<thead>
<tr>
<th>含义</th>
<th>语法</th>
<th>效果</th>
</tr>
</thead>
<tbody>
<tr>
<td>底端对齐省略号</td>
<td>1, 2, \ldots , n</td>
<td>$1, 2, \ldots , n$</td>
</tr>
<tr>
<td>中线对齐省略号</td>
<td>1, 2, \cdots, n</td>
<td>$1,2,\cdots,n$</td>
</tr>
<tr>
<td>竖直省略号</td>
<td>\vdots</td>
<td>$\vdots$</td>
</tr>
<tr>
<td>斜对齐省略号</td>
<td>\ddots</td>
<td>$\ddots$</td>
</tr>
<tr>
<td>虚数集合</td>
<td>\imath</td>
<td>$\imath$</td>
</tr>
<tr>
<td>实数集合</td>
<td>\mathbb{R}</td>
<td>$\mathbb{R}$</td>
</tr>
<tr>
<td>自然数集合</td>
<td>\mathbb{Z}</td>
<td>$\mathbb{Z}$</td>
</tr>
<tr>
<td>空集</td>
<td>$\emptyset</td>
<td>$</td>
<td></td>
</tr>
<tr>
<td>上标</td>
<td>\hat{x}</td>
<td>$\hat{x}$</td>
</tr>
<tr>
<td>上标</td>
<td>\check{x}</td>
<td>$\check{x}$</td>
</tr>
<tr>
<td>上标</td>
<td>\breve{x}</td>
<td>$\breve{x}$</td>
</tr>
<tr>
<td>上标</td>
<td>\tilde{x}</td>
<td>$\tilde{x}$</td>
</tr>
<tr>
<td>上标</td>
<td>\bar{x}</td>
<td>$\bar{a}$</td>
</tr>
<tr>
<td>向量</td>
<td>\vec{x}</td>
<td>$\vec{x}$</td>
</tr>
<tr>
<td>上标</td>
<td>\acute{x}</td>
<td>$\acute{x}$</td>
</tr>
<tr>
<td>上标</td>
<td>\mathring{x}</td>
<td>$\mathring{x}$</td>
</tr>
<tr>
<td>一阶导</td>
<td>\dot{a}</td>
<td>$\dot{a}$</td>
</tr>
<tr>
<td>二阶导</td>
<td>\ddot{a}</td>
<td>$\ddot{a}$</td>
</tr>
<tr>
<td>上箭头</td>
<td>\uparrow</td>
<td>$\uparrow$</td>
</tr>
<tr>
<td>上箭头</td>
<td>\Uparrow</td>
<td>$\Uparrow$</td>
</tr>
<tr>
<td>下箭头</td>
<td>\downarrow</td>
<td>$\downarrow$</td>
</tr>
<tr>
<td>下箭头</td>
<td>\Downarrow</td>
<td>$\Downarrow$</td>
</tr>
<tr>
<td>左箭头</td>
<td>\leftarrow</td>
<td>$\leftarrow$</td>
</tr>
<tr>
<td>左箭头</td>
<td>\Leftarrow</td>
<td>$\Leftarrow$</td>
</tr>
<tr>
<td>右箭头</td>
<td>\rightarrow</td>
<td>$\rightarrow$</td>
</tr>
<tr>
<td>右箭头</td>
<td>\Rightarrow</td>
<td>$\Rightarrow$</td>
</tr>
<tr>
<td>上划线</td>
<td>\overline{AB}</td>
<td>$\overline{AB}$</td>
</tr>
<tr>
<td>下划线</td>
<td>\underline{AB}</td>
<td>$\underline{AB}$</td>
</tr>
<tr>
<td>上括号</td>
<td>\overbrace{1+2+\cdots+n}</td>
<td>$\overbrace{1+2+\cdots+n}$</td>
</tr>
<tr>
<td>下括号</td>
<td>\underbrace{1+2+\cdots+n}</td>
<td>$\underbrace{1+2+\cdots+n}$</td>
</tr>
<tr>
<td>在字母下添加字母</td>
<td>\mathop{Max}\limits_{x \in X}</td>
<td>$\mathop{Max}\limits_{x \in X}$</td>
</tr>
<tr>
<td>上取整</td>
<td>\lceil x \rceil</td>
<td>$\lceil x \rceil$</td>
</tr>
<tr>
<td>下取整</td>
<td>\lfloor x \rfloor</td>
<td>$\lfloor x \rfloor$</td>
</tr>
</tbody>
</table>
</div>
<h4 id="常用微分符号"><a href="#常用微分符号" class="headerlink" title="常用微分符号"></a>常用微分符号</h4><div class="table-container">
<table>
<thead>
<tr>
<th>语法</th>
<th>效果</th>
<th>语法</th>
<th>效果</th>
</tr>
</thead>
<tbody>
<tr>
<td>\nabla</td>
<td>$\nabla$</td>
<td>\partial x</td>
<td>$\partial x$</td>
</tr>
<tr>
<td>\mathrm{d}x</td>
<td>$\mathrm{d}x$</td>
<td>\dot x</td>
<td>$\dot x$</td>
</tr>
<tr>
<td>\ddot y</td>
<td>$\ddot y$</td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h4 id="希腊字母"><a href="#希腊字母" class="headerlink" title="希腊字母"></a>希腊字母</h4><div class="table-container">
<table>
<thead>
<tr>
<th>字母</th>
<th>实现</th>
<th>字母</th>
<th>实现</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>A</td>
<td>α</td>
<td>\alhpa</td>
</tr>
<tr>
<td>B</td>
<td>B</td>
<td>β</td>
<td>\beta</td>
</tr>
<tr>
<td>Γ</td>
<td>\Gamma</td>
<td>γ</td>
<td>\gamma</td>
</tr>
<tr>
<td>Δ</td>
<td>\Delta</td>
<td>δ</td>
<td>\delta</td>
</tr>
<tr>
<td>E</td>
<td>E</td>
<td>ϵ</td>
<td>\epsilon</td>
</tr>
<tr>
<td>Z</td>
<td>Z</td>
<td>ζ</td>
<td>\zet</td>
</tr>
<tr>
<td>H</td>
<td>H</td>
<td>η</td>
<td>\eta</td>
</tr>
<tr>
<td>Θ</td>
<td>\Theta</td>
<td>θ</td>
<td>\theta</td>
</tr>
<tr>
<td>I</td>
<td>I</td>
<td>ι</td>
<td>\iota</td>
</tr>
<tr>
<td>K</td>
<td>K</td>
<td>κ</td>
<td>\kappa</td>
</tr>
<tr>
<td>Λ</td>
<td>\Lambda</td>
<td>λ</td>
<td>\lambda</td>
</tr>
<tr>
<td>M</td>
<td>M</td>
<td>μ</td>
<td>\mu</td>
</tr>
<tr>
<td>N</td>
<td>N</td>
<td>ν</td>
<td>\nu</td>
</tr>
<tr>
<td>Ξ</td>
<td>\X</td>
<td>ξ</td>
<td>\xi</td>
</tr>
<tr>
<td>O</td>
<td>O</td>
<td>ο</td>
<td>\omicron</td>
</tr>
<tr>
<td>Π</td>
<td>\Pi</td>
<td>π</td>
<td>\pi</td>
</tr>
<tr>
<td>P</td>
<td>P</td>
<td>ρ</td>
<td>\rho</td>
</tr>
<tr>
<td>Σ</td>
<td>\Sigma</td>
<td>σ</td>
<td>\sigma</td>
</tr>
<tr>
<td>T</td>
<td>T</td>
<td>τ</td>
<td>\tau</td>
</tr>
<tr>
<td>Υ</td>
<td>\Upsilon</td>
<td>υ</td>
<td>\upsilon</td>
</tr>
<tr>
<td>Φ</td>
<td>\Phi</td>
<td>ϕ</td>
<td>\phi</td>
</tr>
<tr>
<td>X</td>
<td>X</td>
<td>χ</td>
<td>\chi</td>
</tr>
<tr>
<td>Ψ</td>
<td>\Psi</td>
<td>ψ</td>
<td>\psi</td>
</tr>
<tr>
<td>Ω</td>
<td>\v</td>
<td>ω</td>
<td>\omega</td>
</tr>
</tbody>
</table>
</div>
<h4 id="运算符号"><a href="#运算符号" class="headerlink" title="运算符号"></a>运算符号</h4><div class="table-container">
<table>
<thead>
<tr>
<th>含义</th>
<th>语法</th>
<th>效果</th>
</tr>
</thead>
<tbody>
<tr>
<td>求和</td>
<td>\sum_{x&gt;1}f(x)</td>
<td>$\sum_{x&gt;1}f(x)$</td>
</tr>
<tr>
<td>求和</td>
<td>\sum\limits_{n=1}^{n&lt;10}n</td>
<td>$\sum\limits_{n=1}^{n&lt;10}n$</td>
</tr>
<tr>
<td>乘积</td>
<td>\prod_{i=1}^N x_i</td>
<td>$\prod_{i=1}^N x_i$</td>
</tr>
<tr>
<td>乘积</td>
<td>\prod\limits_{i=1}^N x_i</td>
<td>$\prod\limits_{i=1}^N x_i$</td>
</tr>
<tr>
<td>极限</td>
<td>\lim^{x \to \infty}_{y \to 0}{\frac{x}{y}}</td>
<td>$\lim^{x \to \infty}_{y \to 0}{\frac{x}{y}}$</td>
</tr>
<tr>
<td>极限</td>
<td>\lim\limits_{x \to \infty}^{y\to 0} \frac{x}{y}</td>
<td>$\lim\limits_{x \to \infty}^{y\to 0} \frac{x}{y}$</td>
</tr>
<tr>
<td>一重积分</td>
<td>\int_{a}^{b}f(x)dx</td>
<td>$\int_{a}^{b}f(x)dx$</td>
</tr>
<tr>
<td>二重积分</td>
<td>\iint_\Omega f(x,y)dS</td>
<td>$\iint_\Omega f(x,y)dS$</td>
</tr>
<tr>
<td>三重积分</td>
<td>\iiint_\Omega f(x,y,z)dV</td>
<td>$\iiint_\Omega f(x,y,z)dV$</td>
</tr>
<tr>
<td>加减</td>
<td>a \pm b</td>
<td>$a\pm b$</td>
</tr>
<tr>
<td>减加</td>
<td>a \mp b</td>
<td>$a\mp b$</td>
</tr>
<tr>
<td>乘法</td>
<td>a \times b</td>
<td>$a \times b$</td>
</tr>
<tr>
<td>除法</td>
<td>a \div b</td>
<td>$a\div b$</td>
</tr>
<tr>
<td>对数</td>
<td>\log{a+b}</td>
<td>$\log{a+b}$</td>
</tr>
<tr>
<td>开方</td>
<td>\sqrt{a + b}</td>
<td>$\sqrt{a+b}$</td>
</tr>
<tr>
<td>n次根号</td>
<td>\sqrt[n]{3}</td>
<td>$\sqrt[n]{3}$</td>
</tr>
<tr>
<td>点乘</td>
<td>a \cdot b</td>
<td>$a \cdot b$</td>
</tr>
<tr>
<td>叉乘</td>
<td>a \times b</td>
<td>$a \times b $</td>
</tr>
<tr>
<td>内积</td>
<td>\langle x, b \rangle</td>
<td>$\langle x, b \rangle$</td>
</tr>
<tr>
<td>外积</td>
<td>a \otimes b</td>
<td>$a \otimes b$</td>
</tr>
<tr>
<td>分式</td>
<td>\frac{a}{b}</td>
<td>$\frac{a}{b}$</td>
</tr>
<tr>
<td>小于等于</td>
<td>\leq</td>
<td>$\leq$</td>
</tr>
<tr>
<td>大于等于</td>
<td>\geq</td>
<td>$\geq$</td>
</tr>
<tr>
<td>不等于</td>
<td>\neq</td>
<td>$\neq$</td>
</tr>
<tr>
<td>不小于等于</td>
<td>\nleq</td>
<td>$\nleq$</td>
</tr>
<tr>
<td>不大于等于</td>
<td>\ngeq</td>
<td>$\ngeq$</td>
</tr>
<tr>
<td>约等于</td>
<td>\approx</td>
<td>$\approx$</td>
</tr>
<tr>
<td>恒等于</td>
<td>\equiv</td>
<td>$\equiv$</td>
</tr>
<tr>
<td>无穷</td>
<td>\infty</td>
<td>$\infty$</td>
</tr>
<tr>
<td>属于</td>
<td>a \in A</td>
<td>$a\in A$</td>
</tr>
<tr>
<td>不属于</td>
<td>a \notin A</td>
<td>$a \notin A$</td>
</tr>
<tr>
<td>子集</td>
<td>x \subset y</td>
<td>$x \subset y$</td>
</tr>
<tr>
<td>非子集</td>
<td>x \not\subset y</td>
<td>$x \not\subset y$</td>
</tr>
<tr>
<td>真子集</td>
<td>x \subseteq y</td>
<td>$x \subseteq y$</td>
</tr>
<tr>
<td>非真子集</td>
<td>x \subsetneq y</td>
<td>$x \subsetneq y$</td>
</tr>
<tr>
<td>并集</td>
<td>x \cup y</td>
<td>$x \cup y$</td>
</tr>
<tr>
<td>交集</td>
<td>x \cap y</td>
<td>$x \cap y$</td>
</tr>
<tr>
<td>差集</td>
<td>x \setminus y</td>
<td>$x \setminus y$</td>
</tr>
<tr>
<td>同或</td>
<td>$x \bigodot y</td>
<td>$x \bigodot y$</td>
<td></td>
</tr>
<tr>
<td>同与</td>
<td>x \bigotimes y</td>
<td>$x \bigotimes y$</td>
</tr>
<tr>
<td>因为</td>
<td>\beacuse</td>
<td>$\because$</td>
</tr>
<tr>
<td>所以</td>
<td>\therefore</td>
<td>$\therefore$</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h4 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a>矩阵</h4><p>最后的<code>\tag&#123;1&#125;</code>表示序号</p>
<h5 id="1-简单矩阵"><a href="#1-简单矩阵" class="headerlink" title="1. 简单矩阵"></a>1. 简单矩阵</h5><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\begin&#123;matrix&#125;</span><br><span class="line">1 &amp; 2 &amp; 3 \\</span><br><span class="line">4 &amp; 5 &amp; 6 \\</span><br><span class="line">7 &amp; 8 &amp; 9</span><br><span class="line">\end&#123;matrix&#125; \tag&#123;1&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure>
<script type="math/tex; mode=display">
\begin{matrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9
\end{matrix} \tag{1}</script><h5 id="2-带括号的矩阵"><a href="#2-带括号的矩阵" class="headerlink" title="2. 带括号的矩阵"></a>2. 带括号的矩阵</h5><p><strong>中括号</strong></p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\begin&#123;bmatrix&#125;</span><br><span class="line">1 &amp; 2 &amp; 3 \\</span><br><span class="line">4 &amp; 5 &amp; 6 \\</span><br><span class="line">7 &amp; 8 &amp; 9</span><br><span class="line">\end&#123;bmatrix&#125; \tag&#123;2&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure>
<script type="math/tex; mode=display">
\begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9
\end{bmatrix} \tag{2}</script><p><strong>大括号</strong></p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\begin&#123;Bmatrix&#125;</span><br><span class="line">1 &amp; 2 &amp; 3 \\</span><br><span class="line">4 &amp; 5 &amp; 6 \\</span><br><span class="line">7 &amp; 8 &amp; 9</span><br><span class="line">\end&#123;Bmatrix&#125; \tag&#123;3&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure>
<script type="math/tex; mode=display">
\begin{Bmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9
\end{Bmatrix} \tag{3}</script><h5 id="3-包含省略号"><a href="#3-包含省略号" class="headerlink" title="3. 包含省略号"></a>3. 包含省略号</h5><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\begin&#123;bmatrix&#125;</span><br><span class="line">1 &amp; 2 &amp; \cdots &amp; n \\</span><br><span class="line">1 &amp; 2 &amp; \cdots &amp; n \\</span><br><span class="line">\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\</span><br><span class="line">1 &amp; 2 &amp; \cdots &amp; n</span><br><span class="line">\end&#123;bmatrix&#125; \tag&#123;4&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure>
<script type="math/tex; mode=display">
\begin{bmatrix}
1 & 2 & \cdots & n \\
1 & 2 & \cdots & n \\
\vdots & \vdots & \ddots & \vdots \\
1 & 2 & \cdots & n
\end{bmatrix} \tag{4}</script><h5 id="4-行列式"><a href="#4-行列式" class="headerlink" title="4. 行列式"></a>4. 行列式</h5><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line"> \begin&#123;vmatrix&#125;</span><br><span class="line">   1 &amp; 2 &amp; 3 \\</span><br><span class="line">   4 &amp; 5 &amp; 6 \\</span><br><span class="line">   7 &amp; 8 &amp; 9</span><br><span class="line">  \end&#123;vmatrix&#125;</span><br><span class="line">\tag&#123;7&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure>
<script type="math/tex; mode=display">
\begin{vmatrix}
   1 & 2 & 3 \\
   4 & 5 & 6 \\
   7 & 8 & 9
  \end{vmatrix}
\tag{7}</script><h4 id="表格"><a href="#表格" class="headerlink" title="表格"></a>表格</h4><h5 id="1-普通表格"><a href="#1-普通表格" class="headerlink" title="1. 普通表格"></a>1. 普通表格</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\begin&#123;array&#125;&#123;|c|c|c|&#125;</span><br><span class="line">	\hline 2&amp;9&amp;4\\</span><br><span class="line">	\hline 7&amp;5&amp;3\\</span><br><span class="line">	\hline 6&amp;1&amp;8\\</span><br><span class="line">	\hline</span><br><span class="line">\end&#123;array&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure>
<script type="math/tex; mode=display">
\begin{array}{|c|c|c|}
    \hline 2&9&4\\
    \hline 7&5&3\\
    \hline 6&1&8\\
    \hline
\end{array}</script><p><strong>说明:</strong> <code>&#123;c|c|c&#125;</code>中，每个字母代表一列，<code>c</code>表示居中，<code>l</code> 表示左对齐，<code>r</code>表示右对齐。</p>
<p><code>|</code>表示竖直分隔线，<code>||</code> 表示两条竖直分隔线。</p>
<p><code>\hline</code> 表示在下一行输入前插入水平分割线。</p>
<h4 id="2-真值表"><a href="#2-真值表" class="headerlink" title="2. 真值表"></a>2. 真值表</h4><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\begin&#123;array&#125;&#123;cc|c&#125;</span><br><span class="line"><span class="code">	       A&amp;B&amp;F\\</span></span><br><span class="line"><span class="code">	\hline 0&amp;0&amp;0\\</span></span><br><span class="line"><span class="code">	       0&amp;1&amp;1\\</span></span><br><span class="line"><span class="code">	       1&amp;0&amp;1\\</span></span><br><span class="line"><span class="code">	       1&amp;1&amp;1\\</span></span><br><span class="line"><span class="code">\end&#123;array&#125;</span></span><br><span class="line"><span class="code">$$</span></span><br></pre></td></tr></table></figure>
<script type="math/tex; mode=display">
\begin{array}{cc|c}
           A&B&F\\
    \hline 0&0&0\\
           0&1&1\\
           1&0&1\\
           1&1&1\\
\end{array}</script><h4 id="流程图"><a href="#流程图" class="headerlink" title="流程图"></a>流程图</h4><p>输入代码块<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">···mermaid</span><br><span class="line">    ……流程图代码……</span><br><span class="line">···</span><br></pre></td></tr></table></figure><br>流程图代码示例如下:<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">graph TB</span><br><span class="line">    A(开始)</span><br><span class="line">    B[打开冰箱门]</span><br><span class="line">    C&#123;&quot;冰箱小不小？&quot;&#125;</span><br><span class="line">    D((连接))</span><br></pre></td></tr></table></figure><br>效果为<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">graph TB</span><br><span class="line">    A(开始)</span><br><span class="line">    B[打开冰箱门]</span><br><span class="line">    C&#123;&quot;冰箱小不小？&quot;&#125;</span><br><span class="line">    D((连接))</span><br></pre></td></tr></table></figure><br>连线时用<code>--&gt;</code><br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">graph TD</span><br><span class="line">A[方形] --&gt; B(圆角)</span><br><span class="line">    B --&gt; C&#123;条件a&#125;</span><br><span class="line">    C --&gt; |a=1| D[结果1]</span><br><span class="line">    C --&gt; |a=2| E[结果2]</span><br><span class="line">    F[竖向流程图]</span><br></pre></td></tr></table></figure><br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">graph TD</span><br><span class="line">A[方形] --&gt; B(圆角)</span><br><span class="line">    B --&gt; C&#123;条件a&#125;</span><br><span class="line">    C --&gt; |a=1| D[结果1]--&gt;F(结束)</span><br><span class="line">    C --&gt; |a=2| E[结果2]--&gt;F</span><br><span class="line">   </span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>notes</tag>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title>Energy Efficient Real-time Task Scheduling on CPU-GPU Hybrid Clusters</title>
    <url>/2025/02/03/scheduler-2/</url>
    <content><![CDATA[<p>通过动态电压和频率缩放研究了新兴CPU-GPU混合集群的节能问题。</p>
<span id="more"></span>
<h4 id="Energy-Efficient-Real-time-Task-Scheduling-on-CPU-GPU-Hybrid-Clusters"><a href="#Energy-Efficient-Real-time-Task-Scheduling-on-CPU-GPU-Hybrid-Clusters" class="headerlink" title="Energy Efficient Real-time Task Scheduling on CPU-GPU Hybrid Clusters"></a>Energy Efficient Real-time Task Scheduling on CPU-GPU Hybrid Clusters</h4><ul>
<li><p>出处：2017IEEE Xplore    <a href="https://ieeexplore.ieee.org/document/8057205">基于CPU-GPU混合集群的高效实时任务调度</a></p>
</li>
<li><p>主要工作：通过动态电压和频率缩放研究了新兴CPU-GPU混合集群的节能问题。</p>
<ul>
<li><p>首次分析GPU特定的DVFS模型。</p>
</li>
<li><p>设计了一种新的调度算法：1)利用GPU DVFS来节省能源而不违反任务期限；2)有效将一组任务打包到多个服务器上，以减少动态能耗；3)智能调节DVFS设定，更有效地节省能源。</p>
</li>
<li>仿真测试，可以节省多达36%的能耗。</li>
</ul>
</li>
<li><p>做出的假设：<strong>集群中只有一种GPU/CPU，但不同服务器可能有不同数量的GPU-CPU对，且每个任务只能分配给一个CPU-GPU对，每个CPU-GPU对一次只能执行一个任务。</strong></p>
</li>
<li><p>目标：最小化在截止日期限制下处理一系列实时任务的总能耗。考虑了对任务执行时间和功耗有显著影响的三个缩放变量：GPU核心电压、CPU核心频率、GPU内存频率。</p>
</li>
<li><p>方法：通过数学优化计算每个任务的合适电压/频率设置，并使用启发式调度算法将多个任务分配给集群。</p>
</li>
<li><p>数据中心常用的两种节能技术：</p>
<ul>
<li>DVFS：dynamic voltage and frequency scaling(动态电压和频率缩放)</li>
<li>DRS：dynamic resource sleep(动态资源休眠)</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
        <category>调度</category>
      </categories>
      <tags>
        <tag>scheduler</tag>
      </tags>
  </entry>
  <entry>
    <title>BIOS中断介绍</title>
    <url>/2025/02/09/osHomework-1/</url>
    <content><![CDATA[<p>BIOS中断介绍，包括中断号以及一些常见操作如字符显示，清屏</p>
<span id="more"></span>
<h3 id="一-BIOS中断号"><a href="#一-BIOS中断号" class="headerlink" title="一.BIOS中断号"></a>一.BIOS中断号</h3><div class="table-container">
<table>
<thead>
<tr>
<th>中断号</th>
<th>功能</th>
<th style="text-align:left">入口参数</th>
<th>返回值</th>
</tr>
</thead>
<tbody>
<tr>
<td>13H</td>
<td>磁盘读取</td>
<td style="text-align:left">AH:2表示读扇区，3表示写扇区AL=读/写的扇区数  ；CH=磁道号；CL=扇区号；DH=磁头号；DL=驱动器号 0：软驱A，1：软驱B， 80H：硬盘C，81H：硬盘D ES:BX 指向接收从扇区读入数据的内存区/写入磁盘的数据</td>
<td>成功：AH=0       AL=读取/写入的扇区数                        失败：AH=出错码</td>
</tr>
<tr>
<td>16H</td>
<td>键盘操作</td>
<td style="text-align:left">AH=0 从键盘读入字符送入AL，                                                               AH=1 查询键盘缓冲区，但不阻塞                                                                 AH=2 检查键盘上各特殊功能键的状态。执行后，各种特殊功能键的状态放入AL寄存器中，这个状态字记录在内存0040H：0017H单元中，若对应位为“1”，表示该键状态为“ON”，处于按下状态；若对应位为“0”，表示该键状态为“OFF”，处于断开状态。</td>
<td></td>
</tr>
<tr>
<td>21H</td>
<td></td>
<td style="text-align:left">AH=00h 程序终止</td>
<td></td>
</tr>
<tr>
<td>10H</td>
<td>清屏</td>
<td style="text-align:left">AX=0003h</td>
</tr>
</tbody>
</table>
</div>
<h3 id="二-字符显示"><a href="#二-字符显示" class="headerlink" title="二.字符显示"></a>二.字符显示</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mov ax,0B800h</span><br><span class="line">mov es,ax</span><br><span class="line">mov byte[es:off],&#x27;a&#x27;</span><br><span class="line">mov byte[es:off+1],b&#x27;KRGBIRGB</span><br></pre></td></tr></table></figure>
<p>在这里插入图片描述</p>
<h3 id="三-清屏"><a href="#三-清屏" class="headerlink" title="三.清屏"></a>三.清屏</h3><p>功能号：06H/07H<br>用　途：窗口内容向上/向下滚动<br>参　数：AL＝要滚动的行数（若是0将清窗口）<br>BH＝填入新行的属性<br>CH＝滚动窗口的左上角行号<br>CL＝滚动窗口的左上角列号<br>DH＝滚动窗口的右下角行号<br>DL＝滚动窗口的右下角列号<br>调　用：INT 10H<br>返　回：无<br>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mov ah,0x06</span><br><span class="line">mov al,0</span><br><span class="line">mov ch,0  ;(0,0)</span><br><span class="line">mov cl,0</span><br><span class="line">mov dh,24  ;(24,79)</span><br><span class="line">mov dl,79</span><br><span class="line">mov bh,0x07 ;黑底白字</span><br><span class="line">int 10h</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>作业</category>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>os</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文阅读】MICCO:An Enhanced Multi-GPU Scheduling Framework for Many-Body Correlation Functions</title>
    <url>/2025/02/03/scheduler-3/</url>
    <content><![CDATA[<p>首先对数据重用和负载平衡的相互作用进行了全面的研究，并提出了局部重用模式和重用边界两个新概念，研究两者之间实现最佳权衡的机会。在此基础上，MICCO提出了一种启发式调度算法和一种基于机器学习的回归模型来生成重用边界的最优设置。</p>
<span id="more"></span>
<h4 id="MICCO-An-Enhanced-Multi-GPU-Scheduling-Framework-for-Many-Body-Correlation-Functions"><a href="#MICCO-An-Enhanced-Multi-GPU-Scheduling-Framework-for-Many-Body-Correlation-Functions" class="headerlink" title="MICCO: An Enhanced Multi-GPU Scheduling Framework for Many-Body Correlation Functions"></a>MICCO: An Enhanced Multi-GPU Scheduling Framework for Many-Body Correlation Functions</h4><ul>
<li><p>出处：2022IEEE    <a href="https://ieeexplore.ieee.org/document/9820666">MICCO:多体关联函数的增强型多GPU调度框架</a></p>
</li>
<li><p>主要工作：首先对数据重用和负载平衡的相互作用进行了全面的研究，并提出了局部重用模式和重用边界两个新概念，研究两者之间实现最佳权衡的机会。在此基础上，MICCO提出了一种启发式调度算法和一种基于机器学习的回归模型来生成重用边界的最优设置。</p>
</li>
<li><p>动机：</p>
<ul>
<li>多体相关函数是计算和内存密集型，现有的基于图的多GPU调度器无法捕获这些以数据为中心的特征，从而导致多体相关函数计算性能不佳。</li>
<li>数据重用和负载均衡形成了一种权衡关系。</li>
</ul>
</li>
<li><p>影响负载均衡和数据重用均衡的因素：多体相关函数计算的执行主要包括三个部分：内核计算、内存分配和数据通信，后两者称为内存操作。数据重用主要降低内存操作成本。</p>
<ul>
<li>局部重用的影响：如果调度程序能够捕获所有的数据重用，并以最佳的数据重用-负载均衡组合为目标进行穷举搜索，则可以找到最优的任务调度方案。但是实际存在两个问题：许多情况下可能全局信息不可用(特别是动态生成收缩图时)；搜索空间太大，NP难问题。本文提出<strong>利用局部重用模式信息动态搜索局部最优调度方案</strong>。</li>
<li>重用边界：数据重用时允许的负载不均衡的级别。</li>
</ul>
</li>
<li><p>多GPU调度框架</p>
<p>MICCO的设计侧重于这些方面：</p>
<ul>
<li>探索重复张量的数据重用机会。</li>
<li>改善负载均衡以保持GPU繁忙。</li>
<li>在考虑内存调出的情况下实现最佳的数据重用-负载均衡权衡。</li>
</ul>
<p>主要由两部分组成</p>
<p> <img src="/2025/02/03/scheduler-3/7e5dfcc212b47c4b6a4ea85225cc1abf.png" alt="在这里插入图片描述"></p>
</li>
</ul>
<p>  MICCO将每个向量的数据特征提取到回归模型中(①)。回归模型生成最优复用边界(②)。启发式算法对张量对进行分类(③)，共同管理三条策略</p>
<ul>
<li>启发式调度算法</li>
<li>预训练的轻量级回归模型</li>
</ul>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
        <category>调度</category>
      </categories>
      <tags>
        <tag>scheduler</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文阅读】Predictive Scheduling for GPU Based Cloud Servers Using Machine Learning</title>
    <url>/2025/02/03/scheduler-4/</url>
    <content><![CDATA[<p>提出一个干扰感知调度器Mystic，用于在基于GPU 的集群和云服务器上高效地协同执行应用程序。Mystic识别新应用程序和正在执行的应用程序之间的相似之处，并指导调度器将干扰最小化并提高系统吞吐量。</p>
<span id="more"></span>
<h4 id="7-Mystic-Predictive-Scheduling-for-GPU-Based-Cloud-Servers-Using-Machine-Learning"><a href="#7-Mystic-Predictive-Scheduling-for-GPU-Based-Cloud-Servers-Using-Machine-Learning" class="headerlink" title="7.Mystic: Predictive Scheduling for GPU Based Cloud Servers Using Machine Learning"></a>7.Mystic: Predictive Scheduling for GPU Based Cloud Servers Using Machine Learning</h4><ul>
<li><p>出处：2016IEEE IPDPS  <a href="https://ieeexplore.ieee.org/document/7516031">使用机器学习的基于GPU的云服务器的预测调度</a></p>
</li>
<li><p>背景：</p>
<ul>
<li>在同一GPU上共同执行的应用程序之间的资源争用会产生干扰，从而导致性能下降，影响应用程序的QoS要求，降低系统整体吞吐率。现有解决方案要么是为了CPU集群开发的，要么是使用静态分析方法，这可能是计算密集型的，并且不能很好地扩展。</li>
</ul>
</li>
<li><p>主要工作：提出一个干扰感知调度器Mystic，用于在基于GPU 的集群和云服务器上高效地协同执行应用程序。Mystic识别新应用程序和正在执行的应用程序之间的相似之处，并指导调度器将干扰最小化并提高系统吞吐量。</p>
<ul>
<li>使用<strong>协同过滤</strong>来识别新任务和其他正在执行的任务的相似性。协同过滤广泛应用于推荐系统，通过分析用户的购买历史，并根据用户的兴趣提供个性化的推荐。协同过滤中有两种常用方法——基于邻域的方法和潜在因素模型。</li>
<li>争用的影响：多上下文GPU上的资源争用可能导致多个协同执行的应用程序之间的干扰，如流多处理器SMS、内存资源、纹理缓存、全局DRAM和互联网络。</li>
</ul>
</li>
<li><p>模型：</p>
<ul>
<li><p>Stage I: 初始化器和配置文件生成器。</p>
<ul>
<li>初始化程序：查询集群状态表MAST，获取每个计算节点的IP地址、CPU核数、GPU核数和系统内存量，同时为每个传入的应用程序创建状态项。</li>
<li>Mysitc为每个传入的应用程序启动连个简短的分析，已获得两个随机选择的COI(干扰的原因：流多处理器SMS、内存资源、纹理缓存、全局DRAM和互联网络。)，并存在PIT(概要信息表)中。PIT用PID(进程ID)索引。</li>
<li>TRM(评级矩阵)：维护离线分析阶段的几个应用程序的完整配置文件。</li>
</ul>
</li>
<li><p>Stage II: 基于协同过滤(CF)的预测</p>
<ul>
<li>CF将PIT和TRM作为输入。首先根据PID返回一个稀疏的PIT向量v(不包含所有COI信息)并附加到TRM上，然后执行基于svd的协同过滤填充TRM中所有缺失的值(只有一行缺少值),补全后将该行添加到PRT(预测表)中。知道调度。</li>
</ul>
</li>
<li><p>Stage III: 干扰感知的调度程序</p>
<p>以MAST和PRT作为输入，生成应用程序对之间的相似度，决定传入的程序A0是否不会与同一GPU上正在执行的应用程序A1并发执行。首先检查是否有空闲GPU，如果有，则分配到最靠近头节点的GPU，如果没有，获取正在执行的PRT条目，干扰分数并分配到干扰分数最低的GPU。</p>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
        <category>调度</category>
      </categories>
      <tags>
        <tag>scheduler</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文阅读】Analysis of Large-Scale Multi-Tenant GPU Clusters  for DNN Training Workloads</title>
    <url>/2025/02/03/scheduler-5/</url>
    <content><![CDATA[<p>Analysis of Large-Scale Multi-Tenant GPU Clusters  for DNN Training Workloads 论文阅读笔记，主要工作是描述了Microsoft中一个多租户GPU集群两个月的工作负载特征，研究影响多租户集群上DNN训练工作负载的集群利用率的三个问题</p>
<span id="more"></span>
<h5 id="Analysis-of-Large-Scale-Multi-Tenant-GPU-Clusters-for-DNN-Training-Workloads"><a href="#Analysis-of-Large-Scale-Multi-Tenant-GPU-Clusters-for-DNN-Training-Workloads" class="headerlink" title="Analysis of Large-Scale Multi-Tenant GPU Clusters  for DNN Training Workloads"></a>Analysis of Large-Scale Multi-Tenant GPU Clusters  for DNN Training Workloads</h5><ul>
<li><p>出处：2019 USENIX-TAC <a href="https://www.usenix.org/conference/atc19/presentation/jeon">大规模多租户GPU集群对DNN训练工作负载的分析</a>  </p>
</li>
<li><p>主要工作：描述了Microsoft中一个多租户GPU集群两个月的工作负载特征，研究影响多租户集群上DNN训练工作负载的集群利用率的三个问题：</p>
<ul>
<li>队列调度和局部性约束对队列的影响。</li>
<li>局部性对GPU利用率的影响。</li>
<li>训练期间的故障问题。</li>
</ul>
</li>
<li><p>介绍GPU集群Philly：</p>
<p><img src="/2025/02/03/scheduler-5/5c166e5ac015bc30f6f80e5b3cde922e.png" alt="请添加图片描述"></p>
</li>
</ul>
<p>① 传入作业和排队：用户可指定GPU数量，调度器跟踪集群中所有空闲GPU，调度时首先考虑机架，然后考虑机架中可用GPU最多的服务器。</p>
<p>②作业放置和利用：将小作业打包到更少的服务器中来避免资源碎片。一旦作业被安排运行，它的GPU就不会与其他作业共享。</p>
<p>③训练进度和完成情况：有三种可能情况：passed：已完成；killed：被用户终止；unsuccessful：不成功。</p>
<ul>
<li>工具：<strong>Apache-Yarn</strong>， 是一种新的Hadoop资源管理器，是一个通用资源管理系统 和调度平台，可以为上层应用提供统一的资源管理和调度。</li>
<li>展望：<ul>
<li>局部性优先：缺乏局部性会影响利用率和作业运行时间。等待有限的时间来查看是否可以实现局部性，如果不能，则使用局部性宽松的可用资源来调度作业( 从而减少用户的排队时间)。</li>
<li>减轻干扰：将不同小作业放在专用服务器上，而不是打包到单个服务器，从而减少这些作业之间的干扰，但会增加碎片化。所以要支持作业迁移以对集群进行碎片整理。</li>
<li>改进故障处理：大量作业失败是由于代码或配置中的用户错误造成，语法检查可以放置许多错误，并且可以通过运行训练的第一次迭代来捕获一些运行时错误。</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
        <category>调度</category>
      </categories>
      <tags>
        <tag>scheduler</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文阅读】THEMIS:Fair and Efficient GPU Cluster Scheduling</title>
    <url>/2025/02/03/scheduler-6/</url>
    <content><![CDATA[<p>调度架构分为两个级别，包括多个应用程序调度器和一个称为ARBITER的跨应用程序调度程序：PPf∈01。</p>
<span id="more"></span>
<h5 id="THEMIS-Fair-and-Efficient-GPU-Cluster-Scheduling"><a href="#THEMIS-Fair-and-Efficient-GPU-Cluster-Scheduling" class="headerlink" title="THEMIS: Fair and Efficient GPU Cluster Scheduling"></a>THEMIS: Fair and Efficient GPU Cluster Scheduling</h5><ul>
<li>出处: 2020 USENIX             <a href="https://www.usenix.org/conference/nsdi20/presentation/mahajan">Themis：公平高效的 GPU 集群调度 |USENIX</a></li>
<li>主要工作：使用拍卖机制，针对长时间运行、位置敏感的ML应用程序。任务以短期的效率公平来赢取投标但确保长期是完成时间公平性。<ul>
<li>对每个ML应用程序$A_i$引入在GPU分配$\vec{G_i}$最大完成时间公平性$P_i(\cdot)$ </li>
<li>单轮拍卖：<ul>
<li>输入：资源和投标，$\vec{R}$表示要拍卖的GPU资源总量，其中每个元素为1，长度为要拍卖的GPU数量。每个ML应用程序都竞标这些资源，每个ML程序出价是对几个GPU分配方案$\vec{G}$($\vec{G_i}$的值为$\{0,1\}$)的最大时间公平性$P_i$</li>
</ul>
</li>
<li>多轮拍卖：<ul>
<li>逐轮拍卖：在每一轮拍卖开始时，向应用程序请求新的$P_i(\cdot)$</li>
<li>逐轮过滤：为了最大化$P\le 1$应用数量，每轮都过滤掉$P$最大的$1-f$部分，$f\in(0,1)$。过滤使具有SI的应用数量最大化：一个输掉一轮拍卖的不公平应用$i$，更容易打赢一个赢得一轮拍卖的不那么公平的应用$k$（$k$被分配了GPU，它的$P_K$会增大，$i$由于等待也会增大$P_i$，会一直出现，输掉太多轮的应用最终会失去所有资源的租约）</li>
<li>剩余分配：在每轮的最后，有剩余GPU，将这些GPU分配给没有参加拍卖的应用程序，这在违反了SI的风险下提高了效率。</li>
</ul>
</li>
</ul>
</li>
<li>目标：在有效利用集群GPU的同时最小化所有ML应用程序的最大完成时间公平性。<strong>最大完成时间公平性</strong>(共享激励)：在一个有N个应用程序的共享集群中运行时间与在一个有$\frac{1}{N}$个应用程序的集群中单独运行的比率。通过两个思想来做到这点。<ul>
<li>扩大ML应用程序和调度器之间的API，以允许应用程序指定放置偏好。通过一轮轮的拍卖达到，THEMIS使用租约来处理长时间运行的ML任务，并在租约到期时开始拍卖，新一轮开始时有个应用程序出价，价高者得。出价反映应用程序从获取的不同GPU子集的完成时间公平指标。</li>
<li>提出两级调度设计。顶层包含一个集中的应用程序见调度程序，底层包含一个集成现有超参数调优框架的API。</li>
</ul>
</li>
<li>动机：<ul>
<li>现有方案无法提供公平的共享保证，因为他们不知道ML应用程序的特征。(主要是位置偏好)</li>
<li>简单的方案是获取所有应用的 $P_i$ 然后进行排序并分配，但应用程序请求的资源往往比所需的更多，以获得更多的分配机会。</li>
</ul>
</li>
<li>几个概念：<ul>
<li>sharing incentive(SI): 如果N个用户共用集群C，每个用户的性能应该不弱于 $\frac{C}{N}$</li>
<li>Pareto efficiency(PE): 帕累托最优。帕累托改善：从一种分配状态到另一种状态的变化中，在没有使任何人情况变化的前提下，使得至少一个人变得更好。帕累托最优是指集群分配无法再进行帕累托改善()。</li>
<li>envy-freedoms(EF): 无羡嫉</li>
</ul>
</li>
<li>新指标：$P=\frac{T_{sh}}{T_{id}}$ $T_{id}$ 是独立完成时间，$T_{sh}$ 是共享完成时间。如果$P\le 1$ 即可实现ML应用的共享激励。(此时共享完成时间小于独立完成时间)。</li>
<li>调度器的类型：<ul>
<li>悲观调度器：可见性和分配在单个应用的粒度上是密切相关的。</li>
<li>全乐观调度器：可见性和分配在多个应用的粒度上是密切相关的，有完整的多应用可见性，因为所有集群资源及其状态对所有应用都是可见的，所有应用程序都争夺资源，资源分配决策是由多个应用程序同时使用事务作出的。</li>
<li>半乐观调度器：<strong>本文所用方案</strong>，当跨应用调度器同时为多个应用提供资源时，它具有多应用可见性，而资源分配时是无冲突的，保证每个应用程序对资源的独占访问。</li>
</ul>
</li>
<li><p>模型：</p>
<p><img src="/2025/02/03/scheduler-6/72c286902cd978db3f7838bbba8cb1c8.png" alt="在这里插入图片描述"></p>
</li>
</ul>
<p>调度架构分为两个级别，包括多个应用程序调度器和一个称为ARBITER的跨应用程序调度程序：</p>
<ul>
<li>第一阶段(1~3):<ul>
<li>ARBITER 要求所有应用程序提供当前完成时间公平性$P$的估计值。</li>
<li>ARBITER 启动拍卖，将可用资源的相同非绑定资源分配给$P$最小的$f\in[0,1]$部分应用(根据逐轮过滤)。为了最小化拍卖中ML调度器与参与者的变化，THEMIS 引入一个AGENT ，它和每个ML程序调度器放在一起。AGENT作为ML程序和ARBITER之间的媒介。</li>
<li>这些应用程序并行地检查提供的资源，每个应用程序的AGENT返回一个标价，该标价反映了对资源分配的偏好。</li>
</ul>
</li>
<li>第二阶段(4~5):<ul>
<li>ARBITER 在收到这一轮的所有投标后，根据部分分配算法和剩余分配方案选择中标。然后通知每个AGENT获胜的分配(如果有的话)。</li>
<li>AGENT将分配传播给ML应用程序调度器，然后ML应用程序调度器可以决定组成作业之间的分配。</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
        <category>调度</category>
      </categories>
      <tags>
        <tag>scheduler</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文阅读】Efficient Memory Management for Large Language Model Serving with PagedAttention</title>
    <url>/2025/02/05/scheduler-8/</url>
    <content><![CDATA[<p>高吞吐量的LLM服务需同时处理多个请求。但是现有系统非常困难，因为KV cache非常巨大并且是动态伸缩的，因为显存管理不善，导致碎片和重复，造成显存的巨大浪费，从而限制了batch的大小和吞吐量。为了解决这个问题，本文借鉴操作系统的分页内存管理方法，提出了PagedAttention。基于这个方法，实现了vLLM，它能够实现：1) 接近零的KV cache浪费；2) 同一请求内和不同请求间KV cache的灵活共享。实验证明本方法的吞吐量是SOTA系统的2-4倍。</p>
<span id="more"></span>
<h3 id="Efficient-Memory-Management-for-Large-Language-Model-Serving-with-PagedAttention"><a href="#Efficient-Memory-Management-for-Large-Language-Model-Serving-with-PagedAttention" class="headerlink" title="Efficient Memory Management for Large Language Model Serving with PagedAttention"></a>Efficient Memory Management for Large Language Model Serving with PagedAttention</h3><ul>
<li><p>出处: ACM-2023   <a href="https://arxiv.org/abs/2309.06180">[2309.06180] Efficient Memory Management for Large Language Model Serving with PagedAttention (arxiv.org)</a></p>
</li>
<li><p>源码: <a href="https://github.com/vllm-project/vllm.git">项目地址</a></p>
</li>
</ul>
<h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><p>高吞吐量的LLM服务需同时处理多个请求。但是现有系统非常困难，因为KV cache非常巨大并且是动态伸缩的，因为显存管理不善，导致碎片和重复，造成显存的巨大浪费，从而限制了batch的大小和吞吐量。为了解决这个问题，本文借鉴操作系统的分页内存管理方法，提出了PagedAttention。基于这个方法，实现了vLLM，它能够实现：1) 接近零的KV cache浪费；2) 同一请求内和不同请求间KV cache的灵活共享。实验证明本方法的吞吐量是SOTA系统的2-4倍。</p>
<h4 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h4><p>像GPT和PaLM这样的大型语言模型(llm)的出现使编程助理和通用聊天机器人等新应用成为可能，它们开始深刻地影响我们的工作和日常生活。许多云计算公司[34,44]正在竞相提供这些应用程序作为托管服务。然而，运行这些应用程序是非常昂贵的，需要大量的硬件加速器，如gpu。</p>
<p>llm的核心是自回归的Transformer模型。该模型基于输入(提示)和迄今为止生成的输出标记的前一个序列，一次生成一个单词(token)。对于每个请求，重复这个昂贵的过程，直到模型输出一个终止令牌。这种顺序生成过程使工作负载受到内存限制，使gpu的计算能力得不到充分利用，并限制了服务吞吐量。</p>
<p> <img src="/2025/02/05/scheduler-8/90f34ab726a84868a23baf38716f1415.png" alt="在这里插入图片描述"></p>
<p>通过将多个请求批处理在一起，可以提高吞吐量。但是，为了批量处理许多请求，应该有效地管理每个请求的内存空间。例如，图1(左)说明了在具有40GB RAM的NVIDIA A100 GPU上13B参数LLM的内存分布。大约65%的内存分配给模型参数，这些权重在服务期间保持静态。接近30%的内存用于存储请求的动态状态。对于transformer，这些状态由与注意力机制相关的key值和value值组成，通常被称为KV cache，它表示来自早期令牌的上下文，以按顺序生成新的输出令牌。剩下的一小部分用于其他数据，包括激活——在评估LLM时产生的短暂张量。由于模型权重是恒定的，并且激活仅占用GPU内存的一小部分，因此管理KV缓存的方式对于确定最大批大小至关重要。当管理效率低下时，KV高速缓存会显著限制批处理大小，从而限制LLM的吞吐量，如图1(右)所示。</p>
<p>在本文中，我们观察到现有的LLM服务系统无法有效地管理KV缓存。这主要是因为它们将请求的KV缓存存储在连续内存空间中，因为大多数深度学习框架要求将张量存储在连续内存中。然而，与传统深度学习工作负载中的张量不同，KV cache 具有独特的特征:随着模型生成新的token，它会随着时间的推移动态增长和缩小，并且它的生命周期和长度是未知的。这些特点使现有系统的方法在两个方面显着效率低下：</p>
<p> <img src="/2025/02/05/scheduler-8/5b4d27afa1d44d4ba1fe56329043ad0d.png" alt="在这里插入图片描述"></p>
<p>首先，现有系统存在内部和外部内存碎片。为了在连续空间中存储请求的KV cache，它们预先分配了一个具有请求最大长度的连续内存块(例如，2048个token)。这可能导致严重的内部碎片，因为请求的实际长度可能比它的最大长度短得多(例如，图11)。此外，即使预先知道实际长度，预分配仍然是低效的:由于在请求的生命周期内保留了整个块，其他较短的请求不能利用当前未使用的块的任何部分。此外，外部内存碎片也很重要，因为每个请求的预分配大小可能不同。事实上，我们在图2中的分析结果显示，在现有系统中，只有20.4% - 38.2%的KV cache内存用于存储实际 token 状态。</p>
<p>其次，现有系统无法利用内存共享的机会。LLM服务通常使用高级解码算法，例如parallel sampling 和 beam search，每个请求生成多个输出。在这些场景中，请求由多个序列组成，这些序列可以部分共享它们的KV cache。然而，在现有系统中，内存共享是不可能的，因为序列的KV cache存储在单独的连续空间中。</p>
<p>为了解决上述限制，我们提出了PagedAttention，这是一种注意力算法，灵感来自于操作系统(OS)对内存碎片和共享的解决方案:带分页的虚拟内存。PagedAttention将请求的KV缓存划分为块，每个块可以包含固定数量的token的key和value。在PagedAttention中，KV cache的块不一定存储在连续的空间中。因此，我们可以像在OS的虚拟内存中那样更灵活地管理KV cache:可以将块视为页，token视为字节，request视为进程。这种设计通过使用相对较小的块并按需分配来减轻内部碎片。此外，它消除了外部碎片，因为所有块都具有相同的大小。最后，它支持以块粒度、跨与相同请求关联的不同序列甚至跨不同请求共享内存。</p>
<p>在这项工作中，我们在PagedAttention的基础上构建了一个高吞吐量的分布式LLM服务引擎vLLM，它在KV高速缓存中实现了接近零的浪费。vLLM使用与PagedAttention共同设计的块级内存管理和抢占式请求调度。vLLM支持GPT[5]、OPT[62]、LLaMA[52]等流行的llm，支持不同大小的llm，包括超过单个GPU内存容量的llm。我们对各种模型和工作负载的评估表明，与最先进的系统相比，vLLM将LLM服务吞吐量提高了2-4倍，而完全不影响模型的准确性。对于更长的序列、更大的模型和更复杂的解码算法，改进更加明显。</p>
<p>综上所述，我们做出了以下贡献:</p>
<ul>
<li>我们确定了在服务llm时内存分配方面的挑战，并量化了它们对服务性能的影响。</li>
<li>受操作系统中虚拟内存和分页的启发，提出了一种基于非连续分页内存中KV cache 的注意力算法PagedAttention。</li>
<li>我们设计并实现了vLLM，一个基于PagedAttention的分布式LLM服务引擎。</li>
<li>我们在各种情况下评估了vLLM，并证明它大大优于以前的核心解决方案，如FasterTransformer[31]和Orca[60]。</li>
</ul>
<h4 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h4><p>在本节中，我们描述了典型LLM的生成和服务过程，以及LLM服务中使用的iteration 调度。</p>
<h5 id="2-1-Transformer-Based-Large-Language-Models"><a href="#2-1-Transformer-Based-Large-Language-Models" class="headerlink" title="2.1 Transformer-Based Large Language Models"></a>2.1 Transformer-Based Large Language Models</h5><p>语言建模的任务是对标记列表 $(x_1, \dots, x_n)$ 的概率进行建模。由于语言具有自然的顺序排序，因此通常将整个序列的联合概率分解为条件概率的乘积(也称为自回归分解[3]):</p>
<script type="math/tex; mode=display">
P(x)=P(x_1) \cdot P(x_2|x_1)\cdot P(x_n|x_1,\cdots,x_{n-1})</script><p>Transformer[53]已经成为在大范围内对上述概率进行建模的事实上的标准架构。基于transformer的语言模型最重要的组件是它的自注意力层。对于输入隐藏层状态 $(x_1,\cdots,x_n) \in \mathbb{R}^{n\times d}$ ，自注意力层首先对每个位置向量进行线性变换，得到Query, key 和 value 向量:</p>
<script type="math/tex; mode=display">
q_i=W_qx_i,k_i=W_kx_i,v_i=W_vx_i</script><p>然后，自注意层通过将某一位置的 Query 向量与其之前的所有 key 向量相乘来计算关注分数 $a_{ij}$，并计算输出 $o_{i}$ 作为 value 向量的加权平均值:</p>
<script type="math/tex; mode=display">
a_{ij}=\frac{exp(q_i^Tk_j/\sqrt{d})}{\sum_{t=1}^i exp(q_i^Tk_t/\sqrt{d})}, o_i=\sum\limits_{j=1}^{i}a_{ij}v_{j}.</script><p>除Eq. 4中的计算外，Transformer模型中的所有其他组成部分，包括嵌入层、前馈层、层归一化[2]、剩余连接[22]、输出logit计算以及Eq. 2中的Query, key, value 值转换，都是按照位置独立应用的，形式为: $y_i=f(x_i)$。</p>
<h5 id="2-2-LLM-Service-amp-Autoregressive-Generation"><a href="#2-2-LLM-Service-amp-Autoregressive-Generation" class="headerlink" title="2.2 LLM Service &amp; Autoregressive Generation"></a>2.2 LLM Service &amp; Autoregressive Generation</h5><p>经过训练后，llm通常被部署为生成服务(例如，生成API[34]或聊天机器人[19,35])。对LLM服务的请求提供了一个输入提示token序列$(x_1,\cdots,x_n)$，LLM服务根据式(1)生成一个输出令牌列表$(x_{n+1},\cdots,x{n+T})$。我们将提示符列表和输出列表的连接称为序列。</p>
<p>LLM只能逐个采样并生成新的令牌，并且每个新token的生成过程取决于该序列中所有先前的tonken，特别是它们的key和value。在这个顺序生成过程中，通常缓存现有token的key和value向量，以生成未来的token，称为KV cache。注意，一个token的KV cache依赖于它之前的所有token。这意味着同一token在序列中出现在不同位置的KV cache 将是不同的。</p>
<p>给定一个请求prompt，LLM服务中的生成计算可以分解为两个阶段:</p>
<p><strong>The prompt phase: </strong> 将用户的整个prompt  $(x_1,\cdots,x_n)$ 作为输入计算第一个token的概率$P(x_{n+1}|x_1,\cdots,x_n)$ ，在此过程中，还生成了key向量$k_1,\cdots,k_n$ 和 value 向量$v_1,\cdots,v_n$，由于token $(x_1,\cdots,x_n)$ 都是已知的，提示阶段的计算可以使用矩阵乘法运算并行化。因此，这一阶段可以有效地利用gpu固有的并行性。</p>
<p><strong>The autoregressive generation phase: </strong> 依次生成剩余的新token，在第 t 个迭代，模型需要token $x_{n+t}$ 作为输入并使用key向量 $k_1,\cdots,k_{n+t}$ 和value 向量 $v_1,\cdots,v_{n+t}$ 计算概率 $P(x_{n+t+1}|x_1,\cdots,x_{n+t})$ ,注意，$1\to n + t-1$ 的key和value向量在之前的迭代已经缓存了，这个迭代值生成新的token的key 和 value 。当序列达到最大长度(由用户或者llm限制)或发出序列结束($<eos>$)token时，自回归阶段完成。由于数据依赖性，不同迭代的计算不能并行化，通常采用矩阵-向量乘法，效率较低。因此，这一阶段严重未充分利用GPU计算并成为内存限制，带来单个请求的大部分延迟。</eos></p>
<h5 id="2-3-Batching-Techniques-for-LLMs"><a href="#2-3-Batching-Techniques-for-LLMs" class="headerlink" title="2.3 Batching Techniques for LLMs"></a>2.3 Batching Techniques for LLMs</h5><p>通过批量处理多个请求，可以提高llm服务的计算利用率。由于请求共享相同的模型权重，因此移动权重的开销在批处理请求中平摊，并且当批处理大小足够大时，可能会被计算开销所抵消(相比变得很小)。但是，由于两个原因，将请求批处理到LLM服务是非常重要的。首先，请求可能在不同的时间到达。直接的批处理策略要么让较早的请求等待较晚的请求，要么将之后的请求延迟到较早的请求完成，从而导致严重的排队延迟。其次，请求可能具有不同的输入和输出长度(图11)。直接的批处理技术填充请求的输入和输出，以平衡它们的长度，但会浪费GPU计算和内存。</p>
<p>为了解决这个问题，人们提出了细粒度的批处理机制，如cellular batching[16]和iteration-level scheduling[60]。与在请求级别工作的传统方法不同，这些技术在迭代级别操作。在每次迭代之后，完成的请求将从批处理中删除，并添加新的请求。因此，可以在等待单个迭代后处理新请求，而不是等待整个批处理完成。</p>
<p>此外，使用特殊的GPU内核，这些技术消除了填充输入和输出的需要。通过减少排队延迟和填充带来的低效率，细粒度批处理机制显著提高了LLM服务的吞吐量。</p>
<h4 id="3-Memory-Callenge-in-LLM-Serving"><a href="#3-Memory-Callenge-in-LLM-Serving" class="headerlink" title="3. Memory Callenge in LLM Serving"></a>3. Memory Callenge in LLM Serving</h4><p>尽管细粒度批处理减少了计算浪费，并使请求能够以更灵活的方式进行批处理，但可以批处理的请求数量仍然受到GPU内存容量的限制，特别是分配给存储KV cache 的空间。换句话说，服务系统的吞吐量受内存限制。克服这种内存限制需要解决内存管理中的以下挑战:</p>
<p><strong>Large KV cache</strong> 。KV缓存大小随着请求数量的增加而快速增长。例如，对于13B参数的OPT模型[62]，单个令牌的KV缓存需要800 KB的空间，计算为2(key 和 value)× 5120(hidden state size)× 40(number of layers)× 2(bytes per FP16)。由于OPT可以生成多达2048个token的序列，因此存储一个请求的KV cache所需的内存可能高达1.6 GB。并发GPU的内存容量为几十GB。即使将所有可用内存分配给KV cache，也只能容纳几十个请求。此外，低效的内存管理会进一步减小批处理大小，如图2所示。此外，从目前的趋势来看，GPU的计算速度的增长速度超过了内存容量的增长速度。例如，从NVIDIA A100到H100, FLOPS提高了2倍以上，但GPU内存最大保持在80GB。因此，我们相信内存将成为越来越重要的瓶颈。</p>
<p><strong>Complex decoding algorithms.</strong> LLM服务提供了一系列解码算法供用户选择，每种算法对内存管理复杂性的影响各不相同。例如，当用户从单个输入提示请求多个随机样本(程序建议中的典型用例)时，提示部分的KV cache 可以共享，在我们的实验中(§6.3)，它占总KV缓存的12%，以最小化内存使用。另一方面，在自回归生成阶段，由于不同的样本结果及其对环境和位置的依赖，KV cache 应该保持不共享。<strong>KV cache 共享的程度取决于所采用的具体解码算法。</strong> 在beam search [49]等更复杂的算法中，不同的请求可以共享其KV cache 的更大部分(高达55%的内存节省，参见§6.3)，并且共享模式随着解码过程的推进而变化。</p>
<p><strong>Scheduling for unknown input &amp; output lengths.</strong> LLM服务的请求在其输入和输出长度方面表现出可变性。这就要求内存管理系统能够适应各种提示长度。此外，随着解码时请求的输出长度增加，其KV cache 所需的内存也会扩展，并且可能耗尽用于传入请求或正在生成的现有提示的内存。系统需要做出调度决策，例如从GPU内存中删除或交换某些请求的KV缓存。</p>
<h5 id="3-1-Memory-Management-in-Existing-Systems"><a href="#3-1-Memory-Management-in-Existing-Systems" class="headerlink" title="3.1 Memory Management in Existing Systems"></a>3.1 Memory Management in Existing Systems</h5><p>由于当前深度学习框架中的大多数运算符要求将张量存储在连续内存中，以前的LLM服务系统也将一个请求的KV缓存存储为跨不同位置的连续张量。由于LLM的输出长度不可预测，因此它们根据请求的最大可能序列长度静态地为请求分配一块内存，而不考虑请求的实际输入或最终输出长度。</p>
<p> <img src="/2025/02/05/scheduler-8/c86039995d1b47249a4346281689a445.png" alt="在这里插入图片描述"></p>
<p>图3显示了两个请求:请求A的最大可能序列长度为2048，请求B的最大可能序列长度为512。现有系统中的块预分配方案有三个主要的内存浪费来源:为未来token 保留的内存、由于过度供应潜在的最大序列长度而导致的内部碎片，以及来自内存分配器(如buddy分配器)的外部碎片。外部碎片永远不会用于生成的令牌，这在服务请求之前是已知的。内部碎片也未被使用，但这只有在请求完成采样后才会实现。它们都是纯粹的内存浪费。虽然保留的内存最终会被使用，但是在整个请求期间保留这个空间，特别是当保留的空间很大时，会占用本来可以用来处理其他请求的空间。我们在图2中可视化了我们的实验中内存浪费的平均百分比，揭示了以前系统中的实际有效内存可以低至20.4%。</p>
<p>虽然compaction[54]已经被提出作为一种潜在的碎片解决方案，但由于大量KV缓存，在性能敏感的LLM服务系统中执行压缩是不切实际的。即使使用了压缩，为每个请求预先分配的块空间也会阻止现有内存管理系统中特定于解码算法的内存共享。</p>
<h4 id="4-Method"><a href="#4-Method" class="headerlink" title="4. Method"></a>4. Method</h4><p> <img src="/2025/02/05/scheduler-8/062049b20dcc430d999bc89bf80af483.png" alt="在这里插入图片描述"></p>
<p>在这项工作中，我们开发了一种新的注意力算法PagedAttention，并构建了一个LLM服务引擎vLLM，以解决§3中概述的挑战。vLLM的架构如图4所示。vLLM采用集中式调度程序来协调分布式GPU工作线程的执行。KV cache 管理器以分页方式有效地管理KV cache，由PagedAttention启用。具体来说，KV cache 管理器通过集中调度程序发送的指令来管理GPU工作线程上的物理KV cache。</p>
<p>接下来，我们在§4.1中描述PagedAttention算法。我们分别在§4.2中展示KV cache 管理器的设计以及它如何促进§4.3中的PagedAttention。然后，我们展示了这种设计如何促进各种解码方法(§4.4)的有效内存管理，并处理可变长度的输入和输出序列(§4.5)。最后，我们展示了vLLM的系统设计如何在分布式设置中工作(第4.6节)。</p>
<h5 id="4-1-PageAttention"><a href="#4-1-PageAttention" class="headerlink" title="4.1 PageAttention"></a>4.1 PageAttention</h5><p>为了解决§3中的内存挑战，我们引入了PagedAttention，这是一种受操作系统中分页的经典思想启发的注意力算法[25]。与传统的注意力算法不同，PagedAttention允许在非连续的内存空间中存储连续的键和值。具体来说，PagedAttention将每个序列的KV cache 划分为KV block。每个块包含固定数量的token的key和value向量，我们将其记为KV block size。记key block为 $K_j=(k_{(j-1)B+1},\cdots,k_{jB})$ ，记value block为$V_j=(v_{(j-1)B+1},\cdots,v_{jB})$ ，注意力可以逐块计算为:</p>
<script type="math/tex; mode=display">
A_{ij}=\frac{exp(q_i^TK_j/\sqrt{d})}{\sum_{t=1}^{\lceil /B\rceil }exp(q_i^TK_t/\sqrt{d})}, o_i = \sum\limits_{j=1}^{\lceil i / B\rceil} V_j A_{ij}^T</script><p>其中$A_{ij}=(a_{i,(j-1)B+1},\cdots,a_{i,j}B)$表示第j个KV block中的行向量。在注意力计算过程中，PagedAttention内核分别识别和提取不同的KV块，我们在图5中展示了一个PagedAttention的例子:键和值向量分布在三个块上，并且这三个块在物理内存上不是连续的。每次，内核将查询tokend 的Query向量和block中的key向量$K_j$ (例如，block 0中的key向量”Four score and seven”)相乘，计算出注意力分数 $A_{ij}$， 然后将变量 $A_{ij}$ 和block中的value向量$V_j$相乘得到最终的注意力分数。 </p>
<p> <img src="/2025/02/05/scheduler-8/d64723720b434d668211104cb6a749cc.png" alt="在这里插入图片描述"></p>
<p>总之，PagedAttention算法允许将KV块存储在非连续的物理内存中，从而在vLLM中实现更灵活的分页内存管理。</p>
<h5 id="4-2-KV-Cache-Manager"><a href="#4-2-KV-Cache-Manager" class="headerlink" title="4.2 KV Cache Manager"></a>4.2 KV Cache Manager</h5><p>vLLM内存管理器背后的关键思想类似于操作系统中的虚拟内存[25]。操作系统将内存划分为固定大小的页面，并将用户程序的逻辑页面映射到物理页面。连续的逻辑页可以对应于非连续的物理内存页，允许用户程序访问内存，就好像它是连续的一样。此外，物理内存空间不需要提前完全预留，使操作系统可以根据需要动态分配物理页面。vLLM使用虚拟内存背后的思想来管理LLM服务中的KV缓存。通过PagedAttention，我们将KV缓存组织为固定大小的KV块，就像虚拟内存中的页面一样。</p>
<p>请求的KV cache 表示为一系列逻辑KV块，从左到右填充为生成的新token及其KV cache。最后一个KV区块的未填充位置保留给未来生成的token使用。在GPU节点上，块引擎分配一个连续的GPU DRAM块，并将其划分为物理KV块(这也在CPU RAM上完成，用于交换 ,§4.5)。KV块管理器还维护块表——每个请求的逻辑和物理KV块之间的映射。每个块表项记录一个逻辑块对应的物理块和填充位置的数量。分离逻辑和物理KV块允许vLLM动态增长KV缓存，而无需提前为所有位置保留它，这消除了现有系统中的大部分内存浪费，如图2所示。</p>
<h5 id="4-3-Decoding-with-PagedAttention-and-vLLM"><a href="#4-3-Decoding-with-PagedAttention-and-vLLM" class="headerlink" title="4.3 Decoding with PagedAttention and vLLM"></a>4.3 Decoding with PagedAttention and vLLM</h5><p> <img src="/2025/02/05/scheduler-8/973ddad656ae4831b5623c6cf729b0da.png" alt="在这里插入图片描述"></p>
<p>接下来，我们通过一个示例，如图6所示，来演示vLLM如何在单个输入序列的解码过程中执行PagedAttention并管理内存:</p>
<p>①就像在OS的虚拟内存中一样，vLLM不需要为最初可能生成的最大序列长度保留内存。相反，它只保留必要的KV块，以容纳在提示计算期间生成的KV缓存。在本例中，提示符有7个令牌，因此vLLM将前2个逻辑KV块(0和1)映射到2个物理KV块(分别为7和1)。在预填充步骤中，vLLM使用传统的自关注算法(例如[13])生成提示符和第一个输出令牌的KV缓存。然后，vLLM将前4个令牌的KV缓存存储在逻辑块0中，并将随后的3个令牌存储在逻辑块1中。剩余的槽保留给后续的自回归生成阶段。</p>
<p>②在第一个自回归解码步骤中，vLLM使用物理块7和1上的PagedAttention算法生成新的令牌。由于在最后一个逻辑块中仍然有一个槽可用，因此新生成的KV缓存存储在那里，并且块表的#filled记录被更新。</p>
<p>③在第二步解码时，由于最后一个逻辑块已满，vLLM将新生成的KV缓存存储在新的逻辑块中;vLLM为它分配一个新的物理块(物理块3)，并将这个映射存储在块表中。</p>
<p>全局而言，对于每次解码迭代，vLLM首先选择一组候选序列进行批处理(参见§4.5)，并为新需要的逻辑块分配物理块。然后,vLLM连接所有当前迭代的输入token作为一个序列，并将其输入LLM。在LLM的计算过程中，vLLM使用PagedAttention内核访问之前以逻辑KV块形式存储的KV缓存，并将新生成的KV缓存保存到物理KV块中。在KV块中存储多个令牌(块大小&gt; 1)使PagedAttention内核能够跨多个位置并行处理KV缓存，从而增加硬件利用率并减少延迟。然而，更大的块大小也会增加内存碎片。我们在§7.2中研究了块大小的影响。</p>
<p>同样，当生成更多令牌及其KV缓存时，vLLM会动态地将新的物理块分配给逻辑块。由于所有的块都是从左到右填充的，并且只有在之前的所有块都已满时才分配新的物理块，因此vLLM将请求的所有内存浪费限制在一个块内，因此它可以有效地利用所有内存，如图2所示。这允许将更多请求放入内存中进行批处理，从而提高吞吐量。一旦一个请求完成了它的生成，它的KV块可以被释放来存储其他请求的KV缓存。在图7中，我们展示了一个vLLM管理两个序列的内存的示例。两个序列的逻辑块映射到GPU worker中块引擎保留的空间内的不同物理块。两个序列的相邻逻辑块不需要在物理GPU内存中连续，两个序列可以有效地利用物理块的空间。</p>
<p> <img src="/2025/02/05/scheduler-8/e507ff7cc78044088cd9ae2ce3fff6a6.png" alt="在这里插入图片描述"></p>
<h5 id="4-4-Application-to-Other-Decoding-Scenarios"><a href="#4-4-Application-to-Other-Decoding-Scenarios" class="headerlink" title="4.4 Application to Other Decoding Scenarios"></a>4.4 Application to Other Decoding Scenarios</h5><p>§4.3展示了PagedAttention和vLLM如何处理基本的解码算法，例如贪婪解码和采样，将一个用户提示作为输入并生成单个输出序列。在许多成功的LLM应用程序中[18,34]，LLM服务必须提供更复杂的解码场景，表现出复杂的访问模式和更多的内存共享机会。在本节中，我们将展示vLLM对它们的一般适用性。</p>
<p><strong>Parallel sampling.</strong> 并行采样。在基于LLM的程序助手中[6,18]，LLM为单个输入提示生成多个采样输出;用户可以从各种候选输出中选择自己喜欢的输出。到目前为止，我们已经隐含地假设了一个请求生成单个序列。在本文的其余部分中，我们假设一个请求生成多个序列的更一般的情况。在并行采样中，一个请求包含多个共享相同输入提示的样本，从而允许共享提示的KV缓存。通过它的PagedAttention和分页内存管理，vLLM可以很容易地实现这种共享并节省内存。在本文的其余部分中，我们假设一个请求生成多个序列的更一般的情况。在并行采样中，一个请求包含多个共享相同输入提示的样本，从而允许共享提示的KV缓存。通过它的PagedAttention和分页内存管理，vLLM可以很容易地实现这种共享并节省内存。</p>
<p> <img src="/2025/02/05/scheduler-8/136cfecb0ff74f36abe81a6ce50bd312.png" alt="在这里插入图片描述"></p>
<p>图8示出用于两个输出的并行解码的示例。由于两个输出共享相同的提示符，我们在提示阶段只为提示符状态的一个副本保留空间;两个序列的提示符的逻辑块映射到相同的物理块:两个序列的逻辑块0和1分别映射到物理块7和1。由于单个物理块可以映射到多个逻辑块，因此我们为每个物理块引入一个引用计数。在这种情况下，物理块7和1的引用计数都是2。在生成阶段，两个输出采样不同的输出令牌，需要单独存储KV缓存。对于需要通过多个序列修改的物理块，vLLM在块粒度上实现了一种写时复制机制，类似于操作系统虚拟内存中的写时复制技术(例如，当fork一个进程时)。具体来说，在图8中，当示例A1需要写入它的最后一个逻辑块(逻辑块1)时，vLLM识别到对应的物理块(物理块1)的引用计数大于1;它分配一个新的物理块(物理块3)，指示块引擎从物理块1复制信息，并将引用计数减少到1。接下来，当示例A2写入物理块1时，引用计数已经减少到1;因此A2直接将其新生成的KV缓存写入物理块1。</p>
<p>总之，<strong>vLLM支持跨多个输出样本共享用于存储提示的KV缓存的大部分空间，但最后的逻辑块除外，该逻辑块由写时复制机制管理。</strong>通过跨多个示例共享物理块，可以大大减少内存使用，特别是对于长输入提示。</p>
<p><strong>Beam search.</strong> 在机器翻译等LLM任务中[59]，用户期望LLM输出的top-𝑘最合适的翻译。Beam search 被广泛用于解码LLM最可能的输出序列，因为它降低了完全遍历样本空间的计算复杂度。该算法依赖于波束宽度参数𝑘，该参数决定了每一步保留的最佳候选数。在解码过程中，波束搜索通过考虑所有可能的标记来扩展波束中的每个候选序列，使用LLM计算它们各自的概率，并在候选序列(长度为$k\cdot|V|$)中保留最可能的𝑘个序列，其中 $|V|$ 是词汇表大小。</p>
<p> <img src="/2025/02/05/scheduler-8/5a259c8fbe4647e1aea8be7242a5d386.png" alt="在这里插入图片描述"></p>
<p>与并行解码不同，波束搜索工具不仅共享初始提示块，还共享不同候选块，并且共享模式随着解码过程的推进而动态变化，类似于复合分叉在操作系统中创建的进程树。图9显示了对于𝑘= 4的波束搜索示例，vLLM如何管理KV块。在用虚线表示的迭代之前，每个候选序列已经使用了4个完整的逻辑块。所有candidate共享第一个块0(即提示符)。candidate 3从第二部分开始离题。candidate0-2共用前3个block，并在第四个block分开。在随后的迭代中，前4个可能的候选项都来自候选项1和2。由于原来的候选0和3不再是最优候选，它们的逻辑块被释放，相应的物理块的引用计数被减少。vLLM释放所有引用计数达到0的物理块(block 2,4,5,8)，然后，vLLM分配新的物理块(block 9-12)来存储来自新候选对象的新KV缓存。现在，所有candidate共享0、1、3块;candidate0和1共享区块6，candidate2和3进一步共享block7。</p>
<p>以前的LLM服务系统需要在candidate上频繁地复制KV缓存。例如，在图9所示的情况下，在虚线之后，候选3将需要复制candidate2的KV缓存的大部分以继续生成。vLLM的物理块共享大大减少了这种频繁的内存复制开销。在vLLM中，不同candidate的大部分块可以共享。只有当新生成的令牌位于旧的共享块中时，才应用写时复制机制，就像并行解码一样。这只涉及复制一个数据块。</p>
<p> <img src="/2025/02/05/scheduler-8/cebed53b3256400aa2b1871479e9a69e.png" alt="在这里插入图片描述"></p>
<p><strong>Shared prefix.</strong> 通常，LLM用户提供任务的(长)描述，包括指令和示例输入输出，也称为系统提示符[36]。描述与实际任务输入连接起来，形成请求提示。LLM生成的输出基于完整的提示符。图10显示了一个示例。此外，可以通过提示工程进一步调整共享前缀，以提高下游任务的准确性。</p>
<p>对于这种类型的应用程序，许多用户提示共享一个前缀，因此LLM服务提供商可以提前存储前缀的KV缓存，以减少在前缀上花费的冗余计算。在vLLM中，可以通过LLM服务提供者为一组预定义的共享前缀保留一组物理块来方便地实现这一点，正如操作系统如何跨进程处理共享库一样。带有共享前缀的用户输入提示符可以简单地将其逻辑块映射到缓存的物理块(最后一个块标记为写时复制)。提示阶段的计算只需要在用户的任务输入上执行。</p>
<p><strong>Mixed decoding methods.</strong> 前面讨论的解码方法表现出不同的内存共享和访问模式。尽管如此，vLLM促进了具有不同解码偏好的请求的同时处理，这是现有系统无法有效做到的。这是因为vLLM通过将逻辑块转换为物理块的公共映射层隐藏了不同序列之间的复杂内存共享。LLM及其执行内核只看到每个序列的物理块id列表，不需要处理跨序列的共享模式。与现有系统相比，这种方法扩大了具有不同采样要求的请求的批处理机会，最终提高了系统的总体吞吐量。</p>
<h5 id="4-5-Scheduling-and-Preemption"><a href="#4-5-Scheduling-and-Preemption" class="headerlink" title="4.5 Scheduling and Preemption"></a>4.5 Scheduling and Preemption</h5><p>当请求流量超过系统容量时，vLLM必须优先考虑请求一个子集。在vLLM中，我们对所有请求采用先到先服务(FCFS)调度策略，确保公平性并防止饥饿。当vLLM需要抢占请求时，它确保首先服务最早到达的请求，并首先抢占最新的请求。</p>
<p>LLM服务面临着一个独特的挑战:LLM的输入提示的长度可能会有很大的不同，并且结果的输出长度是未知的，这取决于输入提示和模型。随着请求数量及其输出的增长，vLLM可能会耗尽GPU的物理块来存储新生成的KV缓存。在这种情况下，vLLM需要回答两个经典问题:(1)应该驱逐哪些街区? (2)如果再次需要，如何恢复被驱逐的块?通常，驱逐策略使用启发式方法来预测哪个块将在未来被访问得最远，并驱逐该块。在我们的例子中，我们知道序列的所有块都是一起被访问的，所以我们实现了一个全有或全无的驱逐策略，即，要么驱逐序列的所有块，要么不驱逐。此外，一个请求中的多个序列(例如，一个波束搜索请求中的波束候选序列)作为一个序列组进行组调度。一个序列组中的序列总是被抢占或重新调度在一起，因为这些序列之间可能存在内存共享。为了回答第二个问题，即如何恢复被驱逐的块，我们考虑两种技术:</p>
<p><strong>Swapping.</strong> 这是大多数虚拟内存实现使用的经典技术，它将被驱逐的页面复制到磁盘上的交换空间。在本例中，我们将被驱逐的块复制到CPU内存中。如图4所示，除了GPU块分配器之外，vLLM还包含一个CPU块分配器，用于管理交换到CPU RAM的物理块。当vLLM为新令牌耗尽空闲物理块时，它会选择一组序列来驱逐并将它们的KV缓存传输到CPU。一旦它抢占了一个序列并驱逐了它的块，vLLM就会停止接受新的请求，直到所有被抢占的序列都被完成。一旦请求完成，它的块就从内存中释放出来，并将被抢占序列的块带回来继续处理该序列。请注意，在这种设计中，交换到CPU RAM的块数量永远不会超过GPU RAM中的总物理块数量，因此CPU RAM上的交换空间受到分配给KV缓存的GPU内存的限制。</p>
<p><strong>Recomputation.</strong> 在这种情况下，当被抢占的序列被重新调度时，我们只需重新计算KV缓存。请注意，重新计算延迟可以显著低于原始延迟，因为在解码时生成的token可以与原始用户提示连接为一个新的token——它们在所有位置的KV缓存可以在一个提示阶段迭代中生成。</p>
<p>交换和重计算的性能取决于CPU RAM和GPU内存之间的带宽以及GPU的计算能力。我们将在§7.3中测试交换和重计算的速度。</p>
<h5 id="4-6-Distributed-Execution"><a href="#4-6-Distributed-Execution" class="headerlink" title="4.6 Distributed Execution"></a>4.6 Distributed Execution</h5><p>许多llm的参数大小超过了单个GPU的容量[5,9]。因此，有必要将它们划分到分布式gpu上，并以模型并行的方式执行[28,63]。这需要能够处理分布式内存的内存管理器。vLLM通过支持 transformer 上广泛使用的Megatron-LM风格张量模型并行策略，在分布式设置中是有效的。</p>
<p>该策略遵循SPMD(单程序多数据)调度，其中线性层是分区的，以执行逐块矩阵乘法，gpu通过allreduce操作不断同步中间结果。具体来说，注意力 算子在注意头维度上被分割，每个SPMD过程在多头注意中负责注意头的一个子集。我们观察到，即使模型并行执行，每个模型分片仍然处理相同的一组输入令牌，因此需要KV缓存用于相同的位置。因此，vLLM在集中式调度器中具有单个KV缓存管理器，如图4所示。不同的GPU工作者共享管理器，以及从逻辑块到物理块的映射。这种公共映射允许GPU worker使用调度程序为每个输入请求提供的物理块来执行模型。虽然每个GPU worker都有相同的物理块id，但一个工作线程只为其相应的注意头存储一部分KV缓存。</p>
<p>在每个步骤中，调度器首先为批处理中的每个请求准备带有输入token id的消息，并为每个请求准备块表。接下来，调度器将这个控制消息广播给GPU worker。然后，GPU worker 开始使用输入token id执行模型。在注意层，GPU工作人员根据控制消息中的块表读取KV缓存。在执行过程中，GPU worker 中间结果与all-reduce通信原语同步，而不需要调度程序的协调。最后，GPU工作器将这次迭代的采样token发送回调度器。总之，GPU工作人员不需要同步内存管理，因为他们只需要在每次解码迭代开始时接收所有内存管理信息以及step输入。</p>
<h4 id="5-Implementation"><a href="#5-Implementation" class="headerlink" title="5. Implementation"></a>5. Implementation</h4><p>vLLM是一个端到端服务系统，采用FastAPI[15]前端和基于gpu的推理引擎。前端扩展了OpenAI API[34]接口，允许用户为每个请求定制采样参数，如最大序列长度和beam width 𝑘。vLLM引擎是用8.5k行Python和2K行c++ /CUDA代码编写的。我们在Python中开发控制相关组件，包括调度器和块管理器，同时为关键操作(如PagedAttention)开发自定义CUDA内核。对于模型执行器，我们使用PyTorch和Transformer实现流行的llm，如GPT[5]、OPT[62]和LLaMA [52]我们使用NCCL[32]在分布式GPU worker之间进行张量通信。</p>
<h5 id="5-1-Kernel-level-Optimization"><a href="#5-1-Kernel-level-Optimization" class="headerlink" title="5.1 Kernel-level Optimization"></a>5.1 Kernel-level Optimization</h5><p>由于PagedAttention引入了现有系统无法有效支持的内存访问模式，我们开发了几个GPU内核来优化它。</p>
<p>(1)Fused reshape and block write. 在每个Transformer层中，新的KV缓存被分割成块，重新塑造为块读取优化的内存布局，然后保存在块表指定的位置。为了最小化内核启动开销，我们将它们融合到一个内核中。</p>
<p>(2)Fusing block read and attention.我们采用FasterTransformer[31]中的注意力内核，根据块表读取KV缓存，并动态执行注意力操作。为了确保合并内存访问，我们分配了一个GPU warp来读取每个块。此外，我们还增加了对请求批处理中可变序列长度的支持。</p>
<p>(3)Fused block copy.由写时拷贝机制发出的块拷贝操作可以在不连续的块上操作。如果我们使用cudamempyasync API，这可能导致大量的小数据移动调用。为了减少开销，我们实现了一个内核，它将不同块的复制操作批处理到单个内核启动中。</p>
<h5 id="5-2-Supporting-Various-Decoding-Algrithms"><a href="#5-2-Supporting-Various-Decoding-Algrithms" class="headerlink" title="5.2 Supporting Various Decoding Algrithms"></a>5.2 Supporting Various Decoding Algrithms</h5><p>vLLM使用三个关键方法实现各种解码算法:fork、append和free。fork方法从一个现有序列创建一个新序列。append方法向序列追加一个新标记。最后，free方法删除序列。例如，在并行采样中，vLLM使用fork方法从单个输入序列创建多个输出序列。然后在每次迭代中使用append向这些序列添加新的标记，并使用free删除满足停止条件的序列。vLLM在波束搜索和前缀共享中也采用了相同的策略。我们相信结合这些方法也可以支持未来的解码算法。</p>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
        <category>调度</category>
      </categories>
      <tags>
        <tag>scheduler</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文阅读】Deep Learning Workload Scheduling in GPU Datacenters:Taxonomy, Challenges and Vision</title>
    <url>/2025/02/03/scheduler-7/</url>
    <content><![CDATA[<p>论文阅读笔记Gao W, Hu Q, Ye Z, et al. Deep Learning Workload Scheduling in GPU Datacenters: Taxonomy, Challenges and Vision[J]. 讨论了数据中心负载作业的特征以及相关工作</p>
<span id="more"></span>
<h4 id="【论文阅读笔记】Deep-Learning-Workload-Scheduling-in-GPU-Datacenters-Taxonomy-Challenges-and-Vision"><a href="#【论文阅读笔记】Deep-Learning-Workload-Scheduling-in-GPU-Datacenters-Taxonomy-Challenges-and-Vision" class="headerlink" title="【论文阅读笔记】Deep Learning Workload Scheduling in GPU Datacenters:Taxonomy, Challenges and Vision"></a><center>【论文阅读笔记】Deep Learning Workload Scheduling in GPU Datacenters:Taxonomy, Challenges and Vision</center></h4><p><a href="https://arxiv.org/abs/2205.11913">论文链接</a><br>GPU数据中心的DL工作负载调度：分类、挑战、展望</p>
<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>Deep learning (DL) shows its prosperity in a wide variety of fields. The development of a DL model is a time-consuming and resource-intensive procedure. Hence, dedicated GPU accelerators have been collectively constructed into a GPU datacenter. An efficient scheduler design for such GPU datacenter is crucially important to reduce the operational cost and improve resource utilization. However, traditional approaches designed for big data or high performance computing workloads can not support DL workloads to fully utilize the GPU  resources. Recently, substantial schedulers are proposed to tailor for DL workloads in GPU datacenters. This paper surveys existing research efforts for both training and inference workloads. We primarily present how existing schedulers facilitate the respective workloads from the scheduling objectives and resource consumption features. Finally, we prospect several promising future research directions. </p>
<p>深度学习在很多领域取得成功。DL模型的发展是一个费时和资源密集的过程。因此，专用GPU已经被共同建造到数据中心。一个高效的调度器对于数据中心来说在减少操作开销和提高资源利用方面非常重要。然而，针对大数据中心和高性能计算设计的传统方法不能支持DL工作负载来有效利用GPU资源。最近，大量的调度器被设计来适应GPU数据中心的DL工作负载。本文调查了现有的针对训练和推理工作负载的研究。我们主要说明现有调度器如何从调度目标和资源消耗特性来促进各自的工作负载。最后，我们展望了几个有前景的研究方向。</p>
<h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h3><p>Recent decades have witnessed a dramatic increase in deep learning (DL) research, development, and application in many fields, including Go [130], medical analysis [125], robotics [48], etc. A standard DL development pipeline consists of model training and model inference. Each stage requires high-grade hardware resources (GPU and other compute systems) to produce and serve production-level DL models [62, 71, 106, 149]. Therefore it becomes prevalent for IT industries [62, 149] and research institutes [18, 19, 71] to set up GPU datacenters to meet their ever-growing DL development demands. A GPU datacenter possesses large amounts of heterogeneous compute resources to host large amounts of DL workloads. An effective scheduler system is urgently required to orchestrate these resources and workloads to guarantee the efficiency of DL workload execution, hardware utilization, and other scheduling objectives.</p>
<p>近几十年DL研究，发展和在各领域的应用取得了重大进步，包括 Go[130]、医学分析[125]、机器学[48]等等，一个标准的DL开发路线包括模型训练和模型推理。每个阶段需要大量硬件资源（GPU和其他计算系统）来生成和提供工业级的DL模型[62,71,106,149]。因此，it行业 [62, 149]和研究机构[18, 19, 71]建造GPU数据中心来满足不断增长的DL开发需求变得普遍。GPU数据中心拥有大量的异构计算资源来承载大量的DL工作负载。急需一个高效的调度器来协调这些资源和工作负载，以保证DL工作负载的执行效率、硬件利用率和其他调度目标。</p>
<p>The scheduler is responsible for determining the resource utilization of the entire datacenter and the performance of each job, which further affects the operation cost and user experience [42]. Specifically, (1) for model training, the scheduler allocates resources requested by the users to support the long-running offline training workloads. The scheduler needs to achieve high performance for each individual workload, high resource utilization for the entire datacenter, and high fairness among different users. Due to the unique and complicated features of DL training jobs, conventional scheduling algorithms for high performance computing (HPC) and big data workloads could cause unbalanced resource utilization and exorbitant infrastructure expense [157], and new solutions tailored for GPU datacenters are required. (2) For model inference, DL applications often serve as online services to answer users’ requests. They often have a higher expectation on the response latency and inference accuracy [25, 172]. Applications that fail to be completed within the specified time (Service Level Agreement) or have lower accuracy than expected may have little or no commercial values. Therefore, it is critical for the scheduler to balance the inference latency, accuracy and cost.</p>
<p>调度器负责决定整个数据中心的资源调度和每个任务的执行，这进一步影响运行开销和用户体验。具体来说，（1）对于模型训练，调度器分配用于请求的资源来支持长期运行的离线训练工作负载。调度器需要为每个单独的工作负载实现高性能，为这个数据中心实现高资源利用率，为每个用户实现高协调性。由于DL训练工作的独特性和负载性，用于高性能计算（HPC）和大数据工作负载的传统调度算法可能会导致不平衡的资源利用率和高昂的基础设施费用，所以GPU数据中心需要合适的新方法。（2）对于模型推理，DL应用通常作为线上服务来回答用户的请求，他们通常被期望有低响应延迟和高的推理准确率[25,172],那些不能在规定时间内做出响应或者精度低于预期的应用可能几乎或根本没有商业价值。因此，调度器需要平衡推理延迟、准确率和开销。</p>
<p>Over the years a variety of DL schedulers have been proposed for GPU datacenters [25, 46,<br>106, 117, 121, 152, 172]. However, most of these systems are designed in an ad-hoc way for some specific objectives. There is still a lack of comprehensive exploration towards efficient scheduling of DL workloads. We are interested in the following questions: (1) what are the main challenges for designing a satisfactory scheduler to manage DL workloads and resources? (2) Do existing solutions share common strategies to achieve their scheduling objectives? (3) How do we need to refine the schedulers to adapt to the rapid development of DL technology? Those questions are important for system researchers and practitioners to understand the fundamental principles of DL workload scheduling and management, and design innovative schedulers for more complex scenarios and objectives. Unfortunately, there are currently no such works to summarize and answer these questions from a systematic point of view.</p>
<p>多年来，大量的针对GPU数据中心的DL调度器已经被设计。然而，这些系统中的大多数都是以特定的方式为某些特定的任务设计。仍缺少对高效调度器的全面探索。我们关注以下几个问题：（1）设计一个令人满意的调度器来管理DL工作负载和资源的主要挑战是什么？（2）现有的方法是否在实现他们的调度目标方面是否存在共同策略？（3）我们如何改进调度器来适应急速发展的DL技术？这些问题对于系统研究这从业者理解DL工作负载调度和管理，并为更负载场景和目标设计新的调度器来说至关重要。不幸的是，目前还没有工作从系统的角度来总结和回答这些问题。</p>
<p>To the best of our knowledge, this paper presents the first survey for scheduling both DL training and inference workloads in research and production GPU datacenters. We make the following contributions. First, we perform an in-depth analysis about the characteristics of DL workloads and identify the inherent challenges to manage various DL workloads in GPU datacenters. Second, we comprehensively review and summarize existing DL scheduling works. We categorize these solutions based on the scheduling objectives and resource consumption features. We also analyze their mechanisms to address the scheduling challenges. Such summary can disclose the common and important considerations for existing DL scheduler designs. Third, we conclude the limitations and implications from existing designs, which can shed new light on possible directions of scheduler designs in GPU datacenters. We expect this survey can help the community understand the development of DL schedulers and facilitate future designs.</p>
<p>据我们所知，本文首次对研究和工业GPU数据中心的DL训练和推理工作负载进行了调研。我们做出了如下贡献。首先，我们对DL工作负载进行深入分析，并确定GPU数据中心中管理不同DL工作负载的挑战。其次，我们全面回顾和总结了现有的DL调度工作，我们根据调度目标和资源消耗特征对这些方法进行分类。我们也分析了他们解决调度挑战的原理。这样的总结可以说明现有DL调度设计常见和重要的考虑因素。第三，我们总结了现有设计的局限性和联系，这可以为GPU数据中心调度器设计你的可能方向提供新的线索。我们希望这项工作可以帮助大家了解DL调度器的发展并为未来设计提供思路。</p>
<p><strong>Existing surveys.</strong> Past works also presented some surveys, which are relevant to but distinct<br>from ours. (1) Some works summarized the optimization techniques for DL applications, such as distributed training acceleration [112, 139], efficient model inference [44, 90], etc. These surveys primarily focused on the acceleration of individual jobs, while we consider the global optimization of the entire datacenter with plenty of workloads for various objectives. (2) Some works surveyed the scheduler designs for conventional cloud big data [128, 171] and HPC [109, 120] workloads. As discussed in Sec. 2.1, DL workloads have significantly distinct characteristics from these traditional jobs, and their scheduling mechanisms are not quite adaptable for DL training or inference. (3) Very few surveys conducted investigations on DL workload scheduling. Mayer and Jacobsen [98] summarized early designs of DL training job schedulers before 2019. This summary is outdated due to the emerging scheduling algorithms in recent years. Yu et al. [164] proposed a taxonomy for DL inference system optimization based on the computing paradigm. However, it mainly investigated the single node scenario instead of the datacenter scale. A recent work [163] considered the inference scheduling by colocating multiple workloads on the same GPU from both the cluster level and workload level. Different from those works, we provide a very comprehensive and up-to-date survey for scheduling techniques of both DL training and inference in the entire GPU datacenters</p>
<p><strong>现有研究。</strong> 过去的工作也进行了一些调查，他们和我们的研究相关但不同。（1）一些工作总结了DL应用的优化技术，比如分布式训练加速[112,39],高效模型推理等等。这些调查主要关注当个工作的加速，而我们考虑具有各种工作负载的数据中心的全局优化。（2）一些工作研究为传统云大数据和HPC工作负载设计的调度器。如2.1节所示，DL工作负载和这些任务具有显著的区别，他们的调度原理不适用于DL训练和推理。（3）很少研究关注DL工作负载调度。[98]总结2019年之前的DL训练任务调度，由于今年来产生的大量调度器，这个工作已经过时了。[164]提出了一种基于计算范式的DL推理系统优化分类法，然而，它主要研究的是单节点场景，而不是数据中心规模。[163]考虑了通过从集群级别和工作负载级别将多个工作负载集中在同一GPU上进行推理调度。与这些工作不同的是，我们提供了一个对于整个GPU数据中心的DL训练和推理的调度全面和最新的研究。</p>
<p><strong>Paper organization.</strong> The paper is organized as follows: Sec. 2 describes the unique characteristics of DL workloads and challenges for scheduling in GPU datacenters. It also illustrates the scope of this survey. The main body of this survey is presented in Fig 1.Concretely, Sec. 3 and Sec.4 present detailed categorizations of training and inference workloads based on the scheduling objectives and resource consumption features, respectively. Sec. 5 discusses the other workloads, e.g., hyperparameter optimization, mixed training and inference workloads. Implications from these works are also given at the end of each section. Sec. 6 concludes this survey paper and identifies the future directions of scheduler designs.</p>
<p><strong>论文结构。</strong>本文组织如下：第二节描述了DL工作负载独特的特征和GPU数据中心调度的挑战。他也说明了本次调查的范围。本次调查的主体结构在图1中展现。具体而言，第三节和第四节基于调度目标和资源消耗特征对训练和推理工作负载进行了详细的分类。第五节讨论了其他工作，诸如超参数优化，混合训练和推理工作负载。这些工作的实现也在每个部分的最后给出。第六节总结了本次调查并指出未来的研究方向。</p>
<p><img src="/2025/02/03/scheduler-7/756baf0cd3430783e1812db1fd260eb3.png" alt="在这里插入图片描述"></p>
<h3 id="2-Background"><a href="#2-Background" class="headerlink" title="2.Background"></a>2.Background</h3><h4 id="2-1-DL工作负载及他们的特征"><a href="#2-1-DL工作负载及他们的特征" class="headerlink" title="2.1 DL工作负载及他们的特征"></a>2.1 DL工作负载及他们的特征</h4><p>A DL development pipeline typically consists of three stages: data processing, model training and model inference. In this survey, we narrow down our focus to training and inference workloads which account for the most computation and consume the majority of resources in the datacenter.</p>
<p>一个DL开发流程通常有三个阶段组成：数据处理、模型训练和模型推理。在这个调查中，我们将关注点缩小到训练和推理工作负载，它们占用了数据中心的大部分计算并消耗大部分资源。</p>
<h5 id="2-1-1-DL-Traning"><a href="#2-1-1-DL-Traning" class="headerlink" title="2.1.1 DL Traning"></a>2.1.1 DL Traning</h5><p>A DL training workload builds models by extracting features from existing data. A DL framework (e.g., PyTorch [114], TensorFlow [6]) is commonly adopted to fully utilize heterogeneous compute resources to accelerate the training process. To further reduce the training time, the workload is deployed across multiple GPUs with a data-parallel training scheme, which is implemented via distributed training libraries (e.g., Horovod [124], DistributedDataParallel in Pytorch, MultiWorkerMirroredStrategy in Tensorflow).</p>
<p>DL训练工作负载从现有数据中提取特征来建立模型。DL框架通常充分利用异构计算资源来加快训练过程。为了进一步缩短训练时间，工作负载通过数据并行训练方案部署在多个GPU上，该方案通过分布式训练库实现。</p>
<p>DL training workloads exhibit some unique features compared to traditional big data or HPC<br>jobs, which need to be particularly considered for GPU datacenter scheduling. A series of studies have characterized training workloads from the production GPU datacenters, including Microsoft[71], SenseTime [62] and Alibaba [144, 149]. The characteristics are summarized as below.</p>
<p>DL训练工作负载具有和传统大数据或HPC不同的独特的特征，GPU数据中心调度需要特别关注这些特征。很多工作总结了工业GPU数据中心的训练负载工作的特征征，包括微软、SenseTime和阿里巴巴，这些特征总结如下：</p>
<p><strong>T1: Inherent heterogeneity [71, 152].</strong> GPU resources play a dominant role in DL training.<br>However, CPUs and memory might interfere with the input processing and then delay the training execution. A GPU datacenter generally offers an ample pool of CPU and memory resources compared to GPUs. Arbitrary selection of heterogeneous resource combinations by users may lead to imperfect training progress. Figure 2 (f) shows the training performance speedups of common DL models with various generations of GPUs. Different models have diverse affinities to GPU types.</p>
<p><strong>T1：Inherent heterogeneity(内在异质性)</strong>：GPU资源在DL训练中发挥着主要作用，然而GPUs和内存可能会干扰输入处理，进而推迟训练执行，和GPU相比，GPU数据中心通常提供充足的GPU和内存资源池。用户任意选择异构资源组合可能导致训练进度不完美。图2（f）显示了具有不同代GPU的常见DL模型的训练性能加速。不同的模型对GPU类型具有不同的偏向。</p>
<p> <img src="/2025/02/03/scheduler-7/9a0cfc40d9aa52995d13e1093d584c77.png" alt="在这里插入图片描述"></p>
<p>(a)：独占分配和GPU共享<br>(b)：集体调度和弹性训练<br>(c) : 合并布局和拓扑不可知布局<br>(e): 迭代过程：使用torch.profiler分析分配和保留的GPU内存跟踪<br>(f):异构亲和性：各代GPU的加速幅度在不同任务中差异很大。</p>
<p><strong>T2: Placement sensitivity [97, 152].</strong> Distributed DL jobs are sensitive to the locality of allocated GPU resources. Specifically, the runtime speed of some distributed DL jobs are bounded by device-to-device communication. Figure 2 (c) shows two types of placement, where a consolidated placement can efficiently reduce the communication overhead compared with topology-agnostic placement. The communication sensitivity of training jobs depends on the inherent property of the model structure. Advanced interconnect link (e.g., NVlink) can offer an order of magnitude higher bandwidth than PCIe. Therefore, distributed training jobs tend to request advanced interconnect to further obtain communication time reduction. Besides, jobs colocated in one server may suffer from PCIe bandwidth contention.</p>
<p><strong>T2:放置位置敏感</strong>：分布式DL任务对分配的GPU资源的位置敏感。具体而言，一些DL分布式任务的运行速度受到设备之间通信的限制。图2(c)表示两种放置方式，与拓扑不可知的布局相比，合并布局可以有效减少通信开销。训练任务的通信敏感性取决于模型结构的固有特性。高级互连链路可以提供比PCIe高一个数量级的带宽。因此，分布式训练任务倾向于请求高级互连，以进一步减少通信时间。此外，位于一台服务器的作业可能会受到PCIe带宽争用的影响。</p>
<p><strong>T3: Iterative process [42, 115]</strong>. DL training repeats a similar iterative pattern for up to thousands of times, as shown in Figure 2 (e). Each iteration consists of forward propagation, backward propagation and parameter update. It motivates that profiling a small number of iterations suffices to predict the pattern of future GPU memory usage and job completion time.</p>
<p><strong>T3:迭代过程</strong>:  如图2(e)所示，DL训练任务重复一个类似的迭代模式多达上千次，每次的迭代包括前向传播、后向传播和参数更新。这促使分析少量迭代就可以预测未来GPU内存使用模式和作业完成时间。</p>
<p><strong>T4: Feedback-driven exploration [152, 183].</strong> Training a DL model is a typical trial-and-error process. Users may explore a number of trial configurations and terminate unpromising trials by the early feedback. Such early feedback can further motivate to launch new trial configurations. Hence, a GPU datacenter hosts abundant repetitive training trials and short duration trials.</p>
<p><strong>T4：反馈驱动的探索。</strong>训练一个DL模型是一个典型的试错过程。用户可以探索很多种配置并在早期反馈终止。这种早期反馈可以进一步激励出新的使用配置。因此，GPU数据中心拥有大量重复训练试验和短时间试验。</p>
<p><strong>T5: Exclusive allocation [62] versus GPU sharing [149]. </strong>Figure 2 (a) depicts the difference between exclusive allocation and GPU. Exclusive allocation refers to that a DL job exclusively has the resource usage ownership. On the contray, GPU sharing allows multiple jobs to co-locate in the same GPU device and take advantage of resources in a time-/space- sharing manner. Unlike CPUs, GPUs basically do not have the intrinsic hardware-level support for fine-grained sharing across users and thus they are allocated to DL training jobs exclusively. Due to the increasing hardware compute capability, plenty of DL training jobs can not fully utilize recent generations of GPU chips. To address this issue, datacenters enable GPU sharing through various technologies, e.g., NVIDIA Multi-Instance GPU (MIG) [3], Multi-Process Service (MPS) [4], GPU virtualization [61].</p>
<p><strong>T5：独占分配与GPU共享：</strong>图2(a)描述了独占分配和GPU之间的区别。独占分配是指一个DL任务拥有资源使用所有权。相反，GPU共享运行多个作业共同加载到同一GPU上，并且以时间/空间共享的方式利用GPU资源。不像CPUs，GPUs基本上没有内在硬件级别支持跨用户的细粒度共享，因此它们被专门分配到DL训练任务上，由于硬件计算能力的逐步提升，大量的DL训练工作可以无法充分利用近几代GPU芯片。为了解决这个问题，数据中心通过各种技术实现GPU共享，比如NVIDA 多实例GPU，多进程服务，GPU虚拟化。</p>
<p><strong>T6: Gang scheduling [62] versus elastic training [117].</strong> Figure 2 (b) illustrates two scheduling mechanisms for data-parallel DL jobs. In particular, gang scheduling is that DL training requires all the GPUs to be allocated simultaneously in an all-or-nothing manner [35]. The requirement of gang scheduling results from the native support of DL frameworks and runtime speed performance guarantee. In contrast, elastic training removes the strict GPU request constraint, and allows a dynamic number of GPUs to run training jobs. Many scheduling systems support elastic training in order to improve GPU utilization and accelerate the training process. They take advantage of the elasticity of DL training workloads: a DL training job can adapt to a wide range of GPU counts<br>and the training processes can be suspended and resumed via checkpoints [62].</p>
<p><strong>T6：集体调度和弹性训练。</strong> 图2(b)说明了数据并行DL作业的两种调度机制。特别地，集体调度是指DL训练任务以要么全有要么全无的方式分配所有GPU。集体调度的需求源于DL框架的本地支持和运行时速度性能保证。相反，弹性训练移除了严格的GPU请求限制，允许动态数量的GPUs运行训练任务。许多调度系统支持弹性训练以提高GPU利用率和加快训练过程。他们充分利用DL训练工作负载的灵活性；一个DL训练任务可以适应于广泛的GPU数量，并且训练过程可以通过检查点暂停和恢复。</p>
<h5 id="2-1-2-DL-inference"><a href="#2-1-2-DL-inference" class="headerlink" title="2.1.2 DL inference"></a>2.1.2 DL inference</h5><p>Model inference is the process of making predictions to users’ inputs. It is commonly applied as online services (e.g., personalized recommendations, face recognition, language translation). DL frameworks also make efforts to support inference workloads, like TensorFlow Serving [111], MXNet Model Server [1], etc. The inference jobs must be performed in a real-time manner, facing dynamic queries with strict latency requirements [172]. They may process each inference request individually, or batch multiple requests concurrently to balance the resource usage and latency. Since many inference systems are deployed in the public cloud alternative to on-premise clusters, there exist many works emphasizing how to exploit cloud resources at scale to handle inference requests. According to the report from A WS [63], the cost of DL inference has already taken up the majority (more than 90%) of the total infrastructure cost for machine learning as a service. A DL inference workload also gives unique characteristics that can affect the scheduling system designs. They are summarized as follows.</p>
<p>模型推理是根据用于输入进行预测的过程，通常作为在线服务(例如个性化推荐、人脸识别、语言翻译) DL框架也努力支持推理工作负载，如TensorFlow Serving,MXNet Model Server等，推理作业以实时的方式进行，具有严格的延迟要求的动态请求。他们可以一次处理一个请求，也可以同时批处理多个请求，来平衡资源使用和延迟。由于许多推理系统部署在公共云中，而不是部署在本地集群中，因此存在许多工作强调如何大规模利用云资源来处理推理请求。根据A WS[63]的报告，DL推理的成本已经占据了机器学习服务基础设施总成本的大部分（超过90%）。DL推理工作负载还提供了可以影响调度系统设计的特征。总结如下：</p>
<p><strong>I1: Deterministic online execution [28, 51]</strong>. Different from offline training which could be resource-intensive and last for days or weeks, the inference for each query is often completed with sub-second response time and consumes much less resources. Moreover, many inference jobs reveal deterministic execution flows and duration under fixed-size query input. This gives predictable resource usage and execution speed, offering the opportunities of fine-grained optimization.</p>
<p><strong>I1：确定性在线执行：</strong> 与资源密集且可以持续数天或数周的离线训练不同，每个请求的推理通常在几秒内完成并且消耗的资源要少的多。此外，在固定大小的查询输入下，很多推理作业表现出确定性的执行流和持续时间，这使得资源使用率和执行时间可预测，提供了细粒度优化的机会。</p>
<p><strong>I2: High demands on latency and accuracy [25, 172]. </strong> First, the inference service is expected to respond to the incoming queries promptly. Delays of inference responses can cause bad user experience. For example, an online recommend service is required to provide recommendations at interactive latencies (&lt;100ms) to prevent user losses [25]. Other kinds of inference services also have strong latency requirements (e.g., &lt;200ms [172]). Second, the prediction accuracy is also critical for building a reliable inference service. Inference workloads in some critical domains, e.g., healthcare and finance, may have stronger accuracy requirements [55]. The tight latency and accuracy demands pose great difficulty in managing inference jobs on GPUs, and there exist a trade-off between high accuracy and low latency. The datacenter managers need to carefully balance the latency overhead and prediction performance of the inference workloads.</p>
<p><strong>I2：对延迟和准确率的高要求：</strong> 首先，推理任务需要迅速响应输入的请求，推理响应的延迟可能会导致用户糟糕的体验。例如，一个在线推荐服务需要在交互延迟(&lt;100ms)内提供推荐，来防止用户流失。其他推理服务对延迟要求也很高。第二，准确率对建立可靠的推理服务也至关重要。一些关键领域的推理工作负载，例如医疗保健和金融可能具有更强的准确性要求。严格的延迟和准确性要求给在GPU上管理推理任务带来很大困难，并且在高精度和低延迟之间存在权衡。数据中心的管理者需要仔细平衡推理工作的延迟和准确性。</p>
<h4 id="2-2-Scheduler-in-GPU-Datacenters-and-Design-Challenges"><a href="#2-2-Scheduler-in-GPU-Datacenters-and-Design-Challenges" class="headerlink" title="2.2 Scheduler in GPU Datacenters and Design Challenges"></a>2.2 Scheduler in GPU Datacenters and Design Challenges</h4><p>Scheduling has continuously drawn public attention for several decades [34, 36, 37]. Similar to scheduling at the level of the operating system, networking system or applications, parallel job scheduling at the datacenter level makes decisions about the allocation of computing resources to competing jobs for specific scheduling objectives [36], which forms an NP-hard problem. In<br>particular, it matches available resources with pending jobs and decides the optimal moment and amount of resources to be allocated to each job. Modern datacenters have introduced a number of schedulers to manage conventional workloads. For instance, HPC schedulers (e.g., Slurm [162], OpenPBS [5]) are used to support HPC applications and scientific computing; cloud schedulers (e.g., Mesos [59], Kubernetes [14], Yarn [138]) help allocate heterogeneous compute resources for big data applications at scale.</p>
<p>过去几十年调度受到大众的持续关注。类似在操作系统、网络系统或者应用级别的调度，数据中心级别的并行任务调度决定计算资源的分配给具有竞争关系的任务，来达到特定的调度目标，这是一个NP难问题。特别地，它将可用资源和未决任务进行匹配，并决定分配给每个作业的最佳时机和资源量。现代数据中心已经引入了许多调度器来管理传统的工作负载。例如，HPC调度器（例如，Slurm[162]、OpenPBS[5]）用于支持HPC应用和科学计算；云调度器（例如，Mesos[59]、Kubernetes[14]、Yarn[138]）有助于大规模为大数据应用程序分配异构计算资源。</p>
<p>As a special case, DL workload scheduling in GPU datacenters shares many similar features as conventional parallel job scheduling. Figure 3 shows the general workflow of DL schedulers in a GPU datacenter. The scheduler works on top of the DN frameworks, and assigns appropriate resources to satisfy a variety of DL workloads. It receives different types of workloads from the users. By monitoring the usages of existing compute resources in the datacenter, it delivers an efficient scheduling solution for these workloads to optimize the predetermined scheduling objective, e.g, JCT, fairness. Then it allocates the jobs to a set of hardware resources for execution. The schedulers for model training and model inference share similar logic flows but have totally different scheduling objectives, workload types, and target users. So our survey will investigate them separately (Sec. 3<br>and 4), and consider the mix of them in Sec. 5.</p>
<p>作为一种特殊情况，GPU数据中心中的DL工作负载调度与传统的并行作业调度具有许多相似的特性。图3显示了GPU数据中心中DL调度器的一般工作流程。调度器在DN框架之上工作，并分配适当的资源来满足各种DL工作负载。它接收来自用户的不同类型的工作负载。通过监控数据中心中现有计算资源的使用情况，它为这些工作负载提供了一个高效的调度解决方案，以优化预定的调度目标，例如JCT、公平性。然后，它将作业分配给一组硬件资源以供执行。用于模型训练和模型推理的调度器具有相似的逻辑流，但具有完全不同的调度目标、工作负载类型和目标用户。因此，我们将分别对它们进行调查（第3节和第4节），并在第5节中考虑它们的组合。</p>
<p><img src="/2025/02/03/scheduler-7/ea4dab8b04cbc53ce4d631751b12e88a.png" alt="在这里插入图片描述"></p>
<h5 id="2-2-1-Scheduling-Techniques-调度技术"><a href="#2-2-1-Scheduling-Techniques-调度技术" class="headerlink" title="2.2.1 Scheduling Techniques 调度技术"></a>2.2.1 Scheduling Techniques 调度技术</h5><p>Some techniques and mechanisms of conventional parallel job scheduling may also apply to DL workloads scheduling in GPU datacenters. For example, to manage computing resources more efficiently and provide guaranteed service for users, it is common to   divide computing resources into separate partitions and set up different queues for different users or jobs with different characteristics [38, 46, 157]. Queues may also have different priorities and be equipped with different queuing policies, e.g., First-Come-First-Served and Shortest-Remaining- Time-First. Schedulers also pursue a better comprehension of affinities between workloads and resources to make wiser decisions. Therefore, mechanisms like performance modeling of workloads (e.g., online profiling [42] and performance prediction [46]) and trace analysis for characterizing the cluster-level workload distribution [62, 149] are widely adopted. Other traditional scheduling techniques (e.g., backfilling [46, 103]) and mechanisms (e.g., time-slicing [152], checkpointing [157], and migration [16, 152]) are also adopted for more flexible job arrangements and better resource utilization in DL workloads scheduling.</p>
<p>传统并行作业调度的一些技术和机制也可以应用于GPU数据中心中的DL工作负载调度。例如，为了更有效地管理计算资源并为用户提供有保障的服务，通常将计算资源划分为单独的分区，并为具有不同特征的不同用户或作业设置不同的队列[38，46157]。队列也可能具有不同的优先级，并配备有不同的排队策略，例如，先到先得和最短剩余时间优先。调度器还追求更好地理解工作负载和资源之间的相关性，以做出更明智的决策。因此，工作负载的性能建模（例如，在线评测[42]和性能预测[46]）和用于表征集群级工作负载分布的跟踪分析[62149]等机制被广泛采用。其他传统的调度技术（例如，回填[46103]）和机制（例如，时间切片[152]、检查点[157]和迁移[16152]）也被采用，以在DL工作负载调度中实现更灵活的作业安排和更好的资源利用率。</p>
<p>However, due to the distinct characteristics of DL jobs (Sec. 2.1), simply adopting these techniques can cause a series of issues, e.g., serving job blocking, resource under-utilization, high operation cost. Below we summarize the challenges of scheduler designs caused by DL workload features.</p>
<p>然而，由于DL作业分布式的特点，简单地采用这些技术可能会导致一系列的问题，比如，服务作业阻塞、资源利用不足、运营成本高。下面我们总结了DL工作负载特性对调度器设计带来的挑战。</p>
<h5 id="2-2-2-Challenge-for-Scheduling-Training-Jobs"><a href="#2-2-2-Challenge-for-Scheduling-Training-Jobs" class="headerlink" title="2.2.2 Challenge for Scheduling Training Jobs"></a>2.2.2 Challenge for Scheduling Training Jobs</h5><p> As discussed in Sec. 2.1.1, DL training workloads have some unique requirements compared to HPC or cloud jobs, which raises some challenges for scheduling them. We discuss these challenges with a workload trace analysis from four private clusters (Venus, Earth, Saturn and Uranus) in SenseTime GPU datacenter Helios [62]. These clusters contain over 6000 GPUs and 1.5 million GPU jobs in total, spanning 6 months in 2020.</p>
<p>如第2.1.1节所述，与HPC或云作业相比，DL培训工作负载有一些独特的要求，这给调度它们带来了一些挑战。我们通过对SenseTime GPU数据中心Helios[62]中四个私人集群（金星、地球、土星和天王星）的工作负载跟踪分析来讨论这些挑战。这些集群总共包含6000多个GPU和150万个GPU作业，在2020年占据了6个月。</p>
<p><img src="/2025/02/03/scheduler-7/5b965863e750d9ae7cfe1cf6947d5033.png" alt="在这里插入图片描述"></p>
<p><strong>C1: Intensive resource consumption.</strong> The adoption of distributed training aims to reduce the training time yet prompts users to overclaim GPU resources for their jobs. Figures 4 (a) and (b) depict the distributions of requested GPUs pertaining to job and GPU resource occupation respectively. We observe that large-size jobs (≤8 GPUs) account for 10% of the entire trace, and they consume over half of computing resources. Such intensive resource requests can aggravate the job pending issue due to the shortage of GPU resources. If the scheduler prioritizes those large-scale jobs, the situation becomes worse as subsequent jobs have to compete for much less resources. Existing solutions often favor small jobs or treat large and small jobs equally. How to balance the trade-off between intensive and light-weight resource consumption remains a challenging problem.</p>
<p><strong>C1：资源消耗密集。</strong> 分布式训练的采用旨在减少训练时间，但也会促使用户为自己的工作过度索取GPU资源。图4（a）和（b）分别描述了与作业和GPU资源占用有关的请求的GPU的分布。我们观察到，大型作业（≤8个GPU）占整个跟踪的10%，它们消耗了一半以上的计算资源。由于GPU资源短缺，这种密集的资源请求可能会加剧作业未决问题。如果调度器优先考虑那些大规模作业，情况会变得更糟，因为后续作业必须争夺更少的资源。现有的解决方案往往有利于小型工作，或者平等对待大型和小型工作。如何平衡大型和轻量级资源消耗之间的权衡仍然是一个具有挑战性的问题。</p>
<p><strong>C2: Unbalanced runtime distribution. </strong>Recent trace analysis works [62, 71, 144, 149] presented the long-tail runtime distribution of DL training workloads in production GPU datacenters. Figure 4 (c) compares the GPU job duration distribution of each cluster. We observe it is common that job runtime varies from seconds to weeks even months among different production-level GPU clusters. The majority of workloads only finish within a short period of time, while the minority part consume many orders magnitudes of GPU time. Prioritizing short jobs is an effective way to reduce average job completion time but incurs low GPU utilization. More research efforts should be devoted to balance between short jobs and time-consuming jobs.</p>
<p><strong>C2：运行时分布不平衡</strong>。最近的跟踪分析工作[62，71，144，149]介绍了工业GPU数据中心中DL训练工作负载的长尾运行时分布。图4（c）比较了每个集群的GPU作业持续时间分布。我们观察到，在不同的工业级GPU集群中，作业运行时间从几秒到几周甚至几个月不等，这是很常见的。大多数工作负载只在短时间内完成，而少数工作负载消耗许多数量级的GPU时间。优先考虑短作业是减少平均作业完成时间的有效方法，但GPU利用率较低。应该投入更多的研究工作来平衡短期工作和耗时工作。</p>
<p><strong>C3: Heterogeneous resource affinity.</strong>  The runtime speed of a DL training job is affected by a variety of hardware factors, among which GPU heterogeneity and network link heterogeneity are the most important ones. For the impact of GPUs, DL training can benefit from newer generations of GPUs. However, the marginal benefit brought by new GPU versions varies significantly (Figure<br>2 (f)). Also, the speedup ratio is unpredictable, which complicates the heterogeneous GPU resource allocation. For the impact of network links, the recently released high-end GPU interconnect including NVlink and NVswitch can significantly reduce the communication overhead across GPUs in the same sever. Along with PCI Express, InfiniBand, Ethernet and QPI, distributed training has several alternatives for cross-GPU communications. As these links differ considerably in bandwidths, and different jobs have different data sizes for exchange, it is non-trivial to allocate these network resources to the jobs to maximize the benefits and minimize the bandwidth contention.</p>
<p><strong>C3：异构资源亲和性。</strong> DL训练作业的运行速度受多种硬件因素的影响，其中<strong>GPU异构性</strong>和<strong>网络链路异构性</strong>是最重要的因素。对于GPU的影响，DL训练可以从新一代GPU中受益。然而，新GPU版本带来的边际效益差异很大（图2（f））。此外，加速比是不可预测的，这使异构GPU资源分配变得复杂。对于网络链路的影响，最近发布的包括NVlink和NVswitch在内的高端GPU互连可以显著降低同一服务器中GPU之间的通信开销。除了PCI Express、InfiniBand、以太网和QPI之外，分布式训练还有多种跨GPU通信的替代方案。由于这些链路的带宽差异很大，而且不同的作业有不同的数据大小可供交换，因此将这些网络资源分配给作业以最大化效益和最小化带宽争用是非常重要的。</p>
<p><strong>C4: Preemption overhead. </strong> DL frameworks usually provide functions to pause/resume the training jobs at any time for better fault-tolerance. The overhead of such processes primarily depends upon the job scale, which ranges from seconds to minutes. In this paper, the preemption overhead is considered as the addition of the costs of pausing and resuming the job. For time consuming jobs, the preemption overhead is relatively small with the benefit of higher scheduling flexibility. But for short jobs, the preemption overhead is non-negligible, and frequent preemption will delay their progress. Designing an appropriate preemptive mechanism requires meticulous considerations of both short and time-consuming jobs as well as their preemption overheads.</p>
<p><strong>C4：抢占开销。</strong> DL框架通常提供在任何时候暂停/恢复训练工作的功能，以获得更好的容错性。这些过程的开销主要取决于作业规模，从几秒钟到几分钟不等。在本文中，抢占开销被认为是暂停和恢复作业成本的增加。对于耗时的作业，抢占开销相对于更高的调度灵活性带来的收益来说较小。但对于短作业，抢占开销是不可忽略的，并且频繁抢占将推迟他们的进度。设计适当的先发制人机制需要一丝不苟考虑到短期和耗时的工作以及它们的抢占性管理费用。</p>
<h5 id="2-2-3-调度推理任务的挑战"><a href="#2-2-3-调度推理任务的挑战" class="headerlink" title="2.2.3 调度推理任务的挑战"></a>2.2.3 调度推理任务的挑战</h5><p>The online execution fashion and high latency requirement of inference workloads also give the following challenges for designing a scheduler.</p>
<p>推理工作负载的在线执行方式和高延迟要求也给设计调度器带来了以下挑战。</p>
<p><strong>C5: Low GPU utilization for each request. </strong>Compared to training jobs, the inference service mainly involves small convolutional operations (e.g., 1x1, 3x3), and consumes small amounts of GPU resources. Besides, the peak performance of new GPUs are increasing rapidly [32]. This often leads to low GPU utilization for inference workloads [83, 173]. A common practice to improve the GPU utilization is to batch multiple inference requests and execute them at the same time [25].</p>
<p><strong>C5：每个请求的GPU利用率低。</strong>与训练作业相比，推理服务主要涉及小的卷积运算（例如，1x1、3x3），并且消耗少量的GPU资源。此外，新GPU的峰值性能正在迅速提高[32]。这通常会导致推理工作负载的GPU利用率较低[83173]。提高GPU利用率的一种常见做法是批量处理多个推理请求并同时执行它们[25]。</p>
<p><strong>C6: Latency-accuracy-cost tradeoff. </strong>The inference jobs are relatively malleable in terms of latency, accuracy and cost. To improve the resource utilization and cluster-wide job throughput, we can colocate multiple inference jobs or increase the batch size. However, this can increase the inference latency. To increase the accuracy, effective ways include model ensemble or augmentation evaluation, which can also incur latency delay [52]. The adoption of high-class hardware resources can accelerate the inference execution, but charges more for online services. Different users or inference jobs may have different demands towards latency, accuracy and cost. The scheduler needs to figure out a sweet spot for each job over an assortment of algorithms and hardware units.</p>
<p><strong>C6：延迟精度成本权衡。</strong> 推理作业在延迟、准确性和成本方面具有相对的可塑性。为了提高资源利用率和集群范围内的作业吞吐量，我们可以将多个推理作业并置或增加批处理大小。然而，这会增加推理延迟。为了提高准确性，有效的方法包括模型集成或增强评估，这也可能导致延迟[52]。采用高级硬件资源可以加速推理执行，但在线服务的费用更高。不同的用户或推理作业可能对延迟、准确性和成本有不同的要求。调度器需要通过各种算法和硬件单元为每个作业找出一个最佳点。</p>
<p><strong>C7: Bursty and fluctuating requests.</strong> As an online service, it is common for the inference application to receive bursty and fluctuating requests, which are unpredictable. This must be considered when determining the resources for the workload. How to guarantee the latency with the minimal operational cost even in extremely overloading scenarios raises a new challenge. In<br>practice, resources are often over-provisioned for inference workloads to guarantee their latency during the rush hours. Then an efficient scheduler needs to consider how to exploit the unused resources of these workloads when there are less queries.</p>
<p><strong>C7：突发性和波动性请求。</strong>作为一种在线服务，推理应用程序通常会接收突发和波动的请求，这些请求是不可预测的。在确定工作负载的资源时，必须考虑到这一点。即使在极端过载的情况下，如何以最小的操作成本保证延迟也提出了新的挑战。在实践中，通常会为推理工作负载过度提供资源，以保证其在高峰时段的延迟。然后，一个高效的调度器需要考虑如何在查询较少的情况下利用这些工作负载的未使用资源。</p>
<h4 id="2-3-Relevant-Studies-Not-Included-in-This-Survey-本次调查未包括的相关研究"><a href="#2-3-Relevant-Studies-Not-Included-in-This-Survey-本次调查未包括的相关研究" class="headerlink" title="2.3 Relevant Studies Not Included in This Survey 本次调查未包括的相关研究"></a>2.3 Relevant Studies Not Included in This Survey 本次调查未包括的相关研究</h4><p>This survey mainly focuses on the scheduling of DL training and inference workloads in GPU datacenters. Other relevant works beyond the scope of this paper will not be summarized in the following sections. Here we briefly discuss these directions. Readers who are interested in these works can refer to relevant surveys [44, 90, 109, 112, 120, 128, 139, 171].</p>
<p>本调查主要关注GPU数据中心中DL训练和推理工作负载的调度。本文范围之外的其他相关工作将不会在以下章节中进行总结。在这里，我们简要讨论一下这些方向。对这些作品感兴趣的读者可以参考相关调查[44、90、109、112、120、128、139、171]。</p>
<p>First, we do not consider the optimization solutions for individual training or inference jobs. Training job optimization mainly contains distributed training acceleration [17, 74, 175] and job placement optimization [84, 94, 161]. Inference job optimization techniques include workload characterization [15], pipeline execution [75], etc. Their objectives are to achieve high performance<br>for a single job instead of an entire datacenter. It is worth noting that scheduling hyperparameter optimization jobs will not be considered as single job optimization, because it involves a collection of training tasks (e.g., RubberBand [101], HyperSched [91]). They will be summarized in Sec. 5.</p>
<p>首先，我们不考虑单个训练或推理作业的优化解决方案。训练作业优化主要包括分布式训练加速[17,74175]和作业布局优化[84,94161]。推理作业优化技术包括工作负载表征[15]、流水线执行[75]等。它们的目标是为单个作业而不是整个数据中心实现高性能。值得注意的是，调度超参数优化作业不会被视为单作业优化，因为它涉及训练任务的集合（例如，RubberBand[101]、HyperSched[91]）。它们将在第5节中进行总结。</p>
<p>Second, we consider the scheduling at the job level, and do not cover the scheduling approaches at the hardware resource level (e.g., network I/O, power). For instance, HIRE [13] proposed a novel in- network computing scheduling algorithm for datacenter switches. A number of works [49, 99, 145] utilized the DVFS mechanism on CPU and GPU chips to achieve cluster energy conservation. These works are not included in this survey.</p>
<p>其次，我们考虑了作业级别的调度，而没有涵盖硬件资源级别的调度方法（例如，网络I/O、电源）。例如，HIRE[13]提出了一种用于数据中心交换机的新型网络内计算调度算法。许多工作[4919145]利用CPU和GPU芯片上的DVFS机制来实现集群节能。这些工作不包括在本次调查中。</p>
<p>Third, we focus on the GPU datacenters where GPUs are the primary resources for the DL workloads. Those datacenters can also exploit other resources (e.g., CPU, FPGA, ASIC) as subsidiary. This can reflect the current status of mainstream DL infrastructures. Some scheduling systems mainly utilize the CPU [11, 53, 66, 156], FPGA [65, 72], or hybrid resources [73] where GPUs are not dominant. Some papers consider the DL services on mobile devices [110] or edge computing [123,177] other than datacenters. Those works are also out of the scope of this survey.</p>
<p>第三，我们关注GPU数据中心，其中GPU是DL工作负载的主要资源。这些数据中心还可以利用其他资源（例如CPU、FPGA、ASIC）作为辅助资源。这可以反映主流DL基础设施的现状。一些调度系统主要利用CPU[11，53，66156]、FPGA[65，72]或GPU不占主导地位的混合资源[73]。一些论文考虑了移动设备[110]或边缘计算[123，177]，而不是数据中心。这些工作也不在本次调查的范围内。</p>
<p>Fourth, we target the scheduling of general DL training and inference workloads. Some works studied other types of DL jobs, e.g., data processing, model re-training, model validation. Some papers considered the optimization of specific DL applications based on their unique behaviors, including RNN-based service [41, 60], recommendation systems [54, 72, 73, 93] and video analytics [126, 174]. These works are not summarized in this paper. Besides, our aim is to enhance the system and workloads in terms of performance, efficiency and user experience. Other objectives like privacy protection [81, 96] is not considered either.</p>
<p>第四，我们针对一般DL训练和推理工作负载的调度。一些工作研究了其他类型的DL任务，例如数据处理、模型再训练、模型验证。一些论文考虑了基于特定DL应用程序的特定行为的优化，包括基于RNN的服务[41，60]、推荐系统[54，72，73，93]和视频分析[126174]。本文未对这些工作进行总结。此外，我们的目标是在性能、效率和用户体验方面增强系统和工作负载。其他目标，如隐私保护[81，96]也没有考虑在内。</p>
<h3 id="3-SCHEDULING-DL-TRAINING-WORKLOADS-DL训练工作负载调度"><a href="#3-SCHEDULING-DL-TRAINING-WORKLOADS-DL训练工作负载调度" class="headerlink" title="3.SCHEDULING DL TRAINING WORKLOADS DL训练工作负载调度"></a>3.SCHEDULING DL TRAINING WORKLOADS DL训练工作负载调度</h3><p>DL training jobs consume a majority of compute resources in GPU datacenters. Therefore an effective and efficient scheduler for training workloads is of critical importance. Existing scheduling systems can be generally categorized from two dimensions: scheduling objective and resource consumption feature. Table 1 summarizes the past works for DL training scheduling. We detail them in the rest of this section.</p>
<p>DL训练作业消耗GPU数据中心中的大部分计算资源。因此，一个有效和高效的调度程序对训练工作量至关重要。现有的调度系统通常可以从两个维度进行分类：调度目标和资源消耗特征。表1总结了DL训练调度的过去工作。我们将在本节的其余部分详细介绍它们。</p>
<h4 id="3-1-Scheduling-Objectives"><a href="#3-1-Scheduling-Objectives" class="headerlink" title="3.1 Scheduling Objectives"></a>3.1 Scheduling Objectives</h4><p>Different schedulers are designed to achieve different objectives, including efficiency, fairness and deadline guarantee. We first review past works from this perspective.</p>
<p>不同的调度器被设计来实现不同的目标，包括效率、公平性和最后期限保证。我们首先从这个角度来回顾过去的工作。</p>
<h5 id="3-1-1-Efficiency"><a href="#3-1-1-Efficiency" class="headerlink" title="3.1.1 Efficiency."></a>3.1.1 Efficiency.</h5><p>Efficiency is a main objective to pursue when designing the workload schedulers. The GPU datacenter manager can consider different types of efficiency. We classify the efficiency- aware schedulers into three categories, as discussed below.</p>
<p>效率是设计工作负载调度器时要追求的主要目标。GPU数据中心管理器可以考虑不同类型的效率。我们将效率感知调度器分为三类，如下所述。</p>
<p>1) <strong>Timing efficiency.</strong>  This scheduling goal is to reduce the average queuing and execution time of training workloads in a datacenter. Some advanced strategies with special training configurations (e.g., sharing training, elastic training, heterogeneous training) can help improve the timing efficiency [64, 79, 85, 117, 151–154], which will be elaborated in Sec. 3.2. Here we mainly discuss the techniques over common training configurations that support gang scheduling, resource exclusive usage and preemptive operations.</p>
<p><strong>1） 定时效率</strong>。此调度目标是减少数据中心中训练工作负载的平均排队和执行时间。一些具有特殊训练配置的高级策略（例如，共享训练、弹性训练、异构训练）可以帮助提高计时效率[64，79，851117151–154]，这将在第3.2节中详细阐述。在这里，我们主要讨论支持集群调度、资源独占使用和抢先操作的常见训练配置上的技术。</p>
<p>One of the most common and effective ways for guaranteeing timing efficiency is to adopt some heuristic functions to determine the job scheduling priority . For instance, Tiresias [46] designs the Least Attained Service (LAS) algorithm to prioritize jobs based on their service, a metric defined as the multiplication of requested GPU resources and execution time. It devises the priority discretization to mitigate the frequent preemption issue , which is inspired by the classic Multi-Level Feedback Queue (MLFQ) algorithm [8, 22, 23]. These techniques enable Tiresias to beat the classical YARN-CS [138] significantly. E-LAS [132] improves over Tiresias by prioritizing jobs with the real-time epoch progress rate, which is computed as the proportion of the current training epoch over the total number of training epochs. With such improvement, E-LAS outperforms Tiresias in terms of average job timing efficiency. FfDL [70] is an open-sourced scheduler platform developed by IBM. It uses the operating lessons from the industry practice to guide the management of DL training workloads in the cloud.</p>
<p>保证定时效率的最常见和最有效的方法之一是采用一些启发式函数来确定作业调度优先级。例如，Tiresias[46]设计了最小可达服务（LAS）算法，以根据作业的服务对作业进行优先级排序，该度量定义为请求的GPU资源和执行时间的乘积。它设计了优先级离散化来缓解频繁抢占问题，这受到了经典的多级反馈队列（MLFQ）算法[8，22，23]的启发。这些技术使Tiresias能够击败经典的YARN-CS[138]。E-LAS[132]通过使用实时epoch进度率对作业进行优先级排序来改进Tiresias，该进度率计算为当前训练epoch占训练epoch总数的比例。有了这样的改进，E-LAS在平均工作时间效率方面优于Tiresias。FfDL[70]是由IBM开发的一个开源调度器平台。它使用行业实践中的操作经验教训来指导云中DL训练工作负载的管理。</p>
<p>An alternative strategy is to use machine learning techniques for job scheduling.  [95] is a scheduler based on reinforcement learning (RL). It utilizes a Q-network which takes the job state and GPU datacenter state as input, and outputs the optimal job to be scheduled. MLFS [141] also leverages RL to determine the job priority and resource allocation. The RL model takes as input the job time information, resource demand, and accuracy requirements. It can effectively improve the average latency of a mix of data-parallel and model-parallel training jobs. Helios [62] characterizes the production training jobs from a shared GPU datacenter in SenseTime, and then adopts a variety of machine learning algorithms to predict the job priority from the history job information. The prediction result suffices to minimize the cluster-wide job latency. JPAS [183] is a scheduler based on the accuracy curve fitting technique to expedite the feedback-driven exploration of general training workloads. The feedback-driven exploration readily expects the scheduler to allocate more resources for more accurate models. JPAS leverages the accuracy curve fitting to predict the potential maximal accuracy improvement of each job, and then prioritize the jobs in a time interval. With this technique, JPAS can facilitate the early-stage progress of the training workloads and satisfy the needs for the feedback-driven exploration.</p>
<p>另一种策略是使用机器学习技术进行作业调度。[95]是一种基于强化学习（RL）的调度器。它利用Q-network，以作业状态和GPU数据中心状态为输入，并输出要调度的最佳作业。MLFS[141]还利用RL来确定作业优先级和资源分配。RL模型将工作时间信息、资源需求和准确性要求作为输入。它可以有效地提高数据并行和模型并行训练作业的平均延迟。Helios[62]在SenseTime中对来自共享GPU数据中心的生产训练作业进行表征，然后采用各种机器学习算法从历史作业信息中预测作业优先级。预测结果足以最小化集群范围内的作业延迟。JPAS[183]是一种基于精度曲线拟合技术的调度器，用于加快一般训练工作的反馈驱动探索。反馈驱动的探索很容易期望调度器为更准确的模型分配更多的资源。JPAS利用精度曲线拟合来预测每个作业潜在的最大精度改进，然后在一个时间间隔内对作业进行优先级排序。有了这项技术，JPAS可以促进训练工作的早期进展，并满足反馈驱动探索的需求。</p>
<p>The timing efficiency of DL training jobs is highly dependent on the job placement C3, where different placement policies can lead to different communication overheads. Users prefer the strict placement locality to maintain the DL training speed T2. Amaral et al. [7] found that packing jobs on the same CPU socket could bring up to 1.3× speedup compared to spreading jobs across different sockets. Then they designed the Topology-Aware scheduler, which uses a profiler to measure the placement sensitivity of each job, and thus performs a best-effort approach to schedule locality-sensitive jobs in a packing manner. Similarly, Tiresias [46] and E-LAS [132] also adopt the profiling strategy to identify the optimal job placement solutions. SMD [167] is a scheduler for parameter-server (PS) training jobs, which allows multiple jobs to contend the communication bandwidth. It models the scheduling problem as a non-convex integer non-linear program with the bin-packing constraints, and then develops an 휖-approximation algorithm called sum-of-ratio multi-dimensional-knapsack decomposition to solve it. The effectiveness of the SMD scheduler is validated both theoretically and empirically. Philly [71] investigates a production workload trace from Microsoft and conducts a thorough analysis about the impact of gang scheduling and locality constraints on the queuing delay and job runtime. Motivated by this, it proposes to relax locality constraints to improve the job timing efficiency.</p>
<p>DL训练作业的定时效率高度依赖于作业布置C3，因为不同的布置策略可能导致不同的通信开销。用户更喜欢紧密的放置位置以保持DL训练速度T2。Amaral等人[7]发现，与在不同socket中分配作业相比，在同一CPUsocket上打包作业可以带来1.3倍的加速。然后，他们设计了拓扑感知调度器，该调度器使用探查器来测量每个作业的位置敏感度，从而以打包的方式执行对位置敏感作业的最佳调度方法。同样，Tiresias[46]和E-LAS[132]也采用了分析策略来确定最佳的作业放置解决方案。SMD[167]是一种用于参数服务器（PS）训练作业的调度器，它允许多个作业竞争通信带宽。它将调度问题建模为具有装箱约束的非凸整数非线性规划，然后开发了一个采用比例和多维背包分解的近似算法进行求解，从理论和实验两方面验证了SMD调度器的有效性。Philly[71]调查了微软的生产工作负载跟踪，并对集群调度和位置约束对排队延迟和作业运行时间的影响进行了深入分析。基于此，提出放宽局部约束，提高作业时间效率。</p>
<p>Sometimes the scheduler can satisfy the GPU capacity request but fail to meet the placement locality. This will lead to the cluster fragmentation issue, which is often caused by the scattered GPU resource allocation. HiveD [181] emphasizes that sharing the GPU datacenter without the consideration of cluster fragmentation will cause significant job queuing delay. Therefore it develops<br>a buddy cell allocation mechanism to ensure sharing safety. HiveD can be easily incorporated with Tiresias [46] to reduce the queuing delay and further improve the job latency.  [95] addresses the cluster fragmentation problem with an RL model, which is able to satisfy the locality constraint as much as possible. SPIN [57] observes that delay scheduling [170] can bring reward to the GPU datacenter in the long term for satisfying the placement locality in the near future. It requires the job runtime information to determine the delay scheduling policy. SPIN proposes a rounding-based randomized approximation method to achieve this goal, which has strong robustness even with inaccurate job runtime estimation.</p>
<p>有时调度器可以满足GPU容量请求，但不能满足放置位置。这将导致集群碎片化问题，这通常是由分散的GPU资源分配引起。HiveD[181]强调，在不考虑集群碎片的情况下共享GPU数据中心将导致显著的作业排队延迟。因此，它开发了一种分区分配机制来确保共享安全。HiveD可以很容易地与Tiresias[46]结合，以减少排队延迟并进一步减少作业延迟。[95]用RL模型解决了集群碎片化问题，该模型能够尽可能地满足局部性约束。SPIN[57]观察到，从长远来看，延迟调度[170]可以为GPU数据中心带来回报，因为它在不久的将来满足放置位置。它需要作业运行时信息来确定延迟调度策略。SPIN提出了一种基于舍入的随机逼近方法来实现这一目标，即使在作业运行时间估计不准确的情况下，该方法也具有较强的鲁棒性。</p>
<p>2) <strong>Cost efficiency.</strong> This refers to the reduction of power consumption or financial cost for renting cloud services. This is another significant objective for training workload scheduling.</p>
<p><strong>2） 成本效益。</strong>这是指降低租用云服务的功耗或财务成本。这是训练工作量调度的另一个重要目标。</p>
<p>Existing GPU datacenters have considerable power waste as not all the GPUs are actively used all the time, while the datacenter managers prefer to keep all the devices on. To reduce the energy cost, ANDREAS [39] considers a scenario where the execution of each job can be postponed within a certain period. Then it judiciously schedules jobs at appropriate moments to keep all the GPUs busy in the datacenter. It formulates the power consumption as a Mixed Integer Non-Linear Programming problem, and proposes an effective greedy heuristic algorithm to achieve a significant cost reduction. Different from ANDREAS, the Cluster Saving Service (CES) in Helios [62] has no assumption about postponing the execution of DL training jobs. It leverages a prediction model to estimate the future resource utilization from the history logs. Then the scheduler can decide how many GPU nodes should be turned on/off. CES can save the electricity by up to 1.65 million kilowatt-hours per year in four production clusters from SenseTime. Additionally, recent energy optimization frameworks such as GPOEO [140] can significantly save the power consumption of training workloads. Although they are not tailored for GPU datacenters, they can be easily transplanted into the GPU datacenter with a customized scheduler to orchestrate between datacenters and jobs.</p>
<p>现有的GPU数据中心存在相当大的功率浪费，因为并非所有GPU都一直在使用，而数据中心管理者更喜欢保持所有设备都处于开启状态。为了降低能源成本，ANDREAS[39]考虑了一种情况，即每个作业的执行可以在一定时间内推迟。然后，它明智地在适当的时刻安排作业，以保持数据中心中所有GPU的繁忙。它将功耗公式化为一个混合整数非线性规划问题，并提出了一种有效的贪婪启发式算法来实现显著的成本降低。与ANDREAS不同，Helios[62]中的集群保存服务（CES）没有推迟DL训练作业执行的假设。它利用预测模型从历史日志中估计未来的资源利用率。然后调度器可以决定应该打开/关闭多少GPU节点。在SenseTime的四个生产集群中，CES每年可节省高达165万千瓦时的电力。此外，最近的能量优化框架，如GPOEO[140]，可以显著节省训练工作负载的功耗。尽管它们不是为GPU数据中心量身定制的，但它们可以通过定制的调度器轻松移植到GPU数据中心，以在数据中心和作业之间进行协调。</p>
<p>Cloud GPU resources are billed based on the amount and duration of usage. Training a model can be very time-consuming and resource-intensive C1. As such, the cost of a training workload could be considerably expensive. It is critical to reduce such financial cost to produce the model with the same quality. Particularly, PS training is a common method for distributed data-parallel model training in the cloud. Cynthia [182] is a scheduler to guarantee the cost-effectiveness of cloud resource provision for PS training. It introduces an analytical performance model to characterize the relationship between throughput and resource provision. </p>
<p>云GPU资源根据使用量和持续时间计费。训练一个模型可能非常耗时且资源密集C1。因此，培训工作量的成本可能相当昂贵。降低这样的财务成本以生产具有相同质量的模型是至关重要的。特别地，PS训练是在云中进行分布式数据并行模型训练的常用方法。Cynthia[182]是一个调度器，用于保证为PS训练提供云资源的成本效益。它引入了一个分析性能模型来表征吞吐量和资源供应之间的关系。</p>
<p>Through this performance model, this scheduler can identify an optimal resource type and PS configurations to maintain the training throughput while minimizing the monetary cost. Analogously, [134] is a scheduler, which recommends cost-effective and high-performing cloud resource configurations for PS training jobs.It selects the instances with the largest network bandwidth within the budget for the parameter server in order to avoid the communication bottleneck. It also proposes a heuristic method named Scala-Opt to decide the work instances which can guarantee the job throughput while maximizing the cost savings. Jahani [67] treats the compute node with different numbers of GPUs as different virtual machines (VMs). The renting cost and job throughput vary with different VM types. Then it models the scheduling process as a Mixed Integer Linear Programming (MILP) problem, and reduces the renting cost in a global manner while maintaining the job latency. MLCloudPrice [105] makes a quantitative analysis on the price difference among different GPU specifications and dynamic prices of the public cloud. It moves the workloads between spot and on-demand instances, which opportunistically utilizes the low-pricing spot instance to push forward the training progress</p>
<p>通过该性能模型，该调度器可以确定最佳的资源类型和PS配置，以保持训练吞吐量，同时最小化货币成本。类似地，[134]是一个调度器，它为PS训练作业推荐经济高效的云资源配置。它为参数服务器选择预算内网络带宽最大的实例，以避免通信瓶颈。它还提出了一种名为Scala-Opt的启发式方法来确定工作实例，该方法可以保证作业吞吐量，同时最大限度地节省成本。Jahani[67]将具有不同数量GPU的计算节点视为不同的虚拟机（VM）。租用成本和作业吞吐量因不同的VM类型而异。然后将调度过程建模为混合整数线性规划（MILP）问题，并在保持作业延迟的同时，全局降低租赁成本。MLCloudPrice[105]对公共云不同GPU规格和动态价格之间的价格差异进行了定量分析。它在现场实例和按需实例之间移动工作负载，从而机会主义地利用低价现场实例来推进训练进度。</p>
<h5 id="3-1-2-Fairness"><a href="#3-1-2-Fairness" class="headerlink" title="3.1.2 Fairness."></a>3.1.2 Fairness.</h5><p>Fairness indicates how fairly the compute resources are allocated among different entities, including user groups (i.e., tenants) and workloads. Fairness schedulers aim to guarantee that each entity can achieve better performance with the resource sharing mechanism than exclusively using the same portion of resources. For conventional workloads, the design of fairness schedulers follows some typical fairness principles, such as sharing incentive, strategy-proofness, envy-freeness and pareto efficiency [43]. It is more challenging to maintain fairness for DL training workloads for two reasons: (1) A GPU is an indivisible resource in common settings (gang scheduling) T6; (2) DL training exhibits resource heterogeneity preference T1 C3. Below we discuss the new works that can address these two challenges for fairness scheduling of training workloads.</p>
<p>公平性表示计算资源在不同实体之间分配的公平程度，包括用户组（即租户）和工作负载。公平调度器旨在保证每个实体通过资源共享机制可以获得比专门使用相同部分资源更好的性能。对于传统的工作负载，公平调度器的设计遵循一些典型的公平原则，如共享激励、策略验证、无嫉妒和帕累托效率[43]。由于两个原因，维持DL训练工作负载的公平性更具挑战性：（1）GPU在公共设置（集群调度）T6中是不可分割的资源；（2） DL训练表现出资源异质性偏好T1 C3。下面，我们将讨论可以解决训练工作负载公平调度这两个挑战的新工作。</p>
<p><strong>1) Homogeneous GPU resources.</strong> A datacenter with only one generation of GPU devices can be considered as a homogeneous GPU environment. The scheduler in this system achieves fairness sharing of indivisible GPU resources from the timing dimension. For instance, Themis [97] maintains the job-level fairness by introducing a new metric called finish-time fairness. This metric inspires the scheduler to allocate more resources to the jobs whose attained service is less than the deserved amount. Moreover, in existing fairness schedulers (e.g., DRF [43]), the placement preferences of DL training workloads can result in severe fairness sharing loss. To address this problem, Themis builds a two-level scheduling architecture for biding resource allocation among jobs and uses the game theory to guarantee the performance. Astraea [157] concentrates on the fairness across workloads and tenants. It introduces the Long-Term GPU-time Fairness (LTGF) metric to measure the sharing benefit of each tenant and job, and proposes a two-level max-min scheduling discipline to enforce job-level and tenant-level LTGF in a shared GPU datacenter.</p>
<p><strong>1） 同质GPU资源。</strong> 只有一代GPU设备的数据中心可以被视为同质GPU环境。该系统中的调度器从时序维度实现了不可分割GPU资源的公平共享。例如，Themis[97]通过引入一种称为完成时间公平性的新指标来保持工作级别的公平性。该度量激励调度器将更多的资源分配给所获得的服务少于应得数量的作业。此外，在现有的公平调度器（例如，DRF[43]）中，DL训练工作负载的放置偏好可能导致严重的公平共享损失。为了解决这一问题，Themis建立了一个两级调度架构，用于在作业之间竞标资源分配，并使用博弈论来保证性能。Astrea[157]专注于工作负载和租户之间的公平性。它引入了长期GPU时间公平（LTGF）度量来衡量每个租户和作业的共享效益，并提出了一种两级最大-最小调度规则来在共享GPU数据中心中实施作业级和租户级LTGF。</p>
<p><strong>2) Heterogeneous compute resources. </strong> It is relatively easy to maintain fairness over one type of GPUs. However, the existence of multiple generations of GPUs and other compute resources (e.g., CPUs, network links) can also exacerbate the fairness of workloads or user groups T1 C3. A couple of works have introduced solutions to achieve fairness in the heterogeneous environment.</p>
<p><strong>2） 异构计算资源。</strong> 在一种类型的GPU上保持公平性相对容易。然而，多代GPU和其他计算资源（例如，CPU、网络链路）的存在也会加剧工作负载或用户组T1 C3的公平性。一些工作介绍了在异构环境中实现公平的解决方案。</p>
<p>To achieve the fairness over GPUs and other compute resources, Allox [80] is a fairness scheduler, which assumes that both GPUs and CPUs are interchangeable resources, and takes into account the affinity of workloads towards different compute resources. It models the resource allocation as a min-cost bipartite matching problem with a theoretically optimal property. Then it proposes a greedy heuristic solution to solve this problem in an effective and scalable way. Dorm [133] is another fairness scheduler for the fair sharing of GPUs, CPUs and memory resources. It assumes that GPUs, CPUs and memory are complementary resources and the capacity of each one can influence the training job throughput. It dynamically partitions different types of compute resources for each DL training job. It formulates the resource allocation as an MILP problem with the resource utilization fairness as the optimization objective. The scheduling decision in each round is made by calling the MILP solver to optimize the utilization fairness.</p>
<p>为了实现GPU和其他计算资源的公平性，Allox[80]是一个公平调度器，它假设GPU和CPU都是可互换的资源，并考虑到工作负载对不同计算资源的亲和力。它将资源分配建模为具有理论最优性质的最小代价二分匹配问题。然后提出了一种贪婪启发式解决方案，以一种有效且可扩展的方式来解决这个问题。Dorm[133]是用于公平共享GPU、CPU和内存资源的另一个公平调度器。它假设GPU、CPU和内存是互补的资源，每一个的容量都会影响训练作业的吞吐量。它为每个DL训练作业动态地划分不同类型的计算资源。它将资源分配公式化为MILP问题，以资源利用公平性为优化目标。每轮的调度决策是通过调用MILP求解器来优化选择公平性。</p>
<p>It is also challenging to achieve fairness over different generations of GPUs. Datacenter users prefer to request the most powerful GPU resources for their training jobs. However, many jobs can not saturate the peak performance of these high-end GPUs. Besides, different DL training jobs have different sensitivities of runtime speed to the compute capability of GPUs.[16] is an early fairness scheduler dedicated for the heterogeneous GPU resource environment. It targets the inter-user fairness in the GPU heterogeneity. To maintain such fairness while maximizing the cluster-wide job efficiency,[16] allows users to transparently trade heterogeneous GPU-time by a couple of techniques including profiling and automatic trade pricing. Gavel [106] is another heterogeneity-aware fairness scheduler. It profiles the performance heterogeneity between different types of GPUs and DL model architectures. A round-based scheduling technique is adopted to improve the scheduling flexibility and ensure timely GPU-time re-allocation. This scheduler can satisfy different types of fairness defnitions, e.g., max-min fairness, makespan minimization, finish-time fairness minimization. However, it is prohibitive to scale up Gavel to a large datacenter due to the time-consuming mathematical solving process. To this end, POP [104] proposes to partition a large datacenter into several smaller ones. Then the original complex optimization formulation is decomposed into multiple smaller problems and can be solved in parallel. It provides a theoretical proof and several empirical evidences to demonstrate the effectiveness of this optimization technique.</p>
<p>实现不同代GPU的公平性也是一项挑战。数据中心用户更喜欢为他们的训练工作请求最强大的GPU资源。然而，许多工作并不能使这些高端GPU的峰值性能饱和。此外，不同的DL训练作业对GPU的计算能力具有不同的运行时速度敏感性。 [16] 是专用于异构GPU资源环境的早期公平调度器。它针对GPU异构中的用户间公平性。为了在最大化集群范围内的作业效率的同时保持这种公平性，[16]允许用户通过包括分析和自动交易定价在内的多种技术透明地交易异构GPU时间。Gavel[106]是另一个异构感知公平调度器。它描述了不同类型的GPU和DL模型体系结构之间的性能异构性。采用基于轮的调度技术，提高了调度的灵活性，保证了GPU时间的及时重新分配。该调度器可以满足不同类型的公平性定义，例如最大-最小公平性、完工时间最小化公平性。然而，由于耗时的数学求解过程，将Gavel扩展到大型数据中心是令人望而却步的。为此，POP[104]提出将一个大型数据中心划分为几个较小的数据中心。然后将原来的复杂优化公式分解为多个较小的问题，并可以并行求解。它提供了一个理论证明和几个经验证据来证明该优化技术的有效性。</p>
<h5 id="3-1-3-Deadline-Guarantee-最后期限保证"><a href="#3-1-3-Deadline-Guarantee-最后期限保证" class="headerlink" title="3.1.3 Deadline Guarantee. 最后期限保证"></a>3.1.3 Deadline Guarantee. 最后期限保证</h5><p>Different from the efficiency goal which aims to complete the job as soon as possible, this objective is to ensure the job can be done before the specified deadline. It is relatively less studied due to the lack of comprehensive analysis about the deadline requirement in DL workloads. An early deadline-aware scheduler for DL training workloads is GENIE [20]. It develops a performance model to predict the job throughput on different resource placement policies.The performance model only requires a small number of training iterations to profile without any significant degradation of job execution T3. With this performance model, GENIE can identify the best placement policy for each job to satisfy the corresponding deadline requirement. However, GENIE [20] does not investigate the deadline requirement from users and cannot support a mixed workload of deadline and best-effort jobs. In [42], a user survey is conducted to uncover users’ latent needs about the deadline guarantee, and comprehensively discuss the deadline requirement from GPU datacenter users. Motivated by this survey, it introduces Chronus, a scheduler to improve the deadline guarantee for Service-Level-Objective (SLO) jobs and latency of best-effort jobs at the same time. It formulates the deadline-guarantee scheduling task as an MILP problem with the resource and time constraints. The MILP solver can make effective scheduling decisions for a collection of jobs. Moreover, in consideration of the placement sensitivity of different training jobs, it proposes round-up and local-search techniques to make placement decisions. These designs successfully enable Chronus to outperform existing deadline schedulers in reducing deadline miss rates and improving the latency of best effort jobs.</p>
<p>与旨在尽快完成工作的效率目标不同，该目标是确保工作能够在规定的截止日期前完成。由于缺乏对DL工作负载中的截止日期要求的全面分析，因此对它的研究相对较少。用于DL训练工作负载的早期截止日期感知调度器是GENIE[20]。它开发了一个性能模型来预测不同资源放置策略下的作业吞吐量。性能模型只需要少量的训练迭代来评测，而不会显著降低作业执行T3。有了这个性能模型，GENIE可以为每个工作确定最佳的安置策略，以满足相应的截止日期要求。然而，GENIE[20]没有调查用户的截止日期要求，也不能支持截止日期和尽力而为工作的混合负载。在[42]中，进行了一项用户调查，以揭示用户对截止日期保证的潜在需求，并全面讨论GPU数据中心用户的截止日期要求。受此调查的启发，它引入了Chronous，这是一款调度器，用于同时改进服务级别目标（SLO）作业的截止日期保证和尽力而为作业的延迟。它将保证工期的调度任务定义为具有资源和时间约束的MILP问题。MILP求解器可以为一组作业做出有效的调度决策。此外，考虑到不同培训工作的安置敏感性，提出了四舍五入和局部搜索技术来进行安置决策。这些设计成功地使Chronous在降低截止日期未命中率和提高尽力而为作业的延迟方面优于现有的截止日期调度器。</p>
<h4 id="3-2-Resource-Consumption-Feature"><a href="#3-2-Resource-Consumption-Feature" class="headerlink" title="3.2 Resource Consumption Feature"></a>3.2 Resource Consumption Feature</h4><p>In addition to the scheduling objective, another orthogonal view to categorize training workloads is their resource consumption features. We discuss prior works based on whether they adopt heterogeneous resources, GPU sharing and elastic training.</p>
<p>除了调度目标之外，对训练工作负载进行分类的另一个角度是它们的资源消耗特征。我们讨论了先前的工作，基于它们是否采用异构资源、GPU共享和弹性训练。</p>
<h5 id="3-2-1-Heterogeneous-Resources"><a href="#3-2-1-Heterogeneous-Resources" class="headerlink" title="3.2.1 Heterogeneous Resources."></a>3.2.1 Heterogeneous Resources.</h5><p>Most schedulers focus on the allocation of GPU resources, as they dominate the DL training. However, the consumption of CPUs and memory can also affect the training performance C3. Synergy [102] observes that different DL training jobs exhibit different levels of sensitivity to the CPU and memory allocation. An optimal allocation can improve the overall cluster utilization and efficiency. Therefore, it introduces optimistic profiling to empirically profile the job throughput for various CPU allocations and analytically estimate all the combinations of CPUs and memory along with the respective storage bandwidth requirement. Based on the profiling results, it performs round-based scheduling and greedily packs runnable jobs along multiple resource dimensions with the objective of minimizing the fragmentation in each round T1. CODA [180] observes that CPU jobs colocating within the same compute node can interfere with the training jobs due to the CPU resource contention. It then designs three components to optimize system-wide performance: an adaptive CPU allocator identifies the optimal CPU cores for each DL training job; a real-time contention eliminator monitors and throttles the memory bandwidth of each CPU job to reduce its interference with the GPU training jobs; a multi-array job scheduler allows CPU jobs to preempt the CPU cores reserved by the GPU jobs accordingly, and vice versa. Experimental results demonstrate CODA can efficiently improve the GPU utilization without sacrificing the performance of CPU jobs.</p>
<p>大多数调度器关注GPU资源的分配，因为它们主导DL训练。然而，CPU和内存的消耗也会影响训练性能C3。Synergy[102]观察到，不同的DL训练作业对CPU和内存分配表现出不同程度的敏感性。优化分配可以提高集群的整体利用率和效率。因此，它引入了乐观分析来凭经验分析各种CPU分配的作业吞吐量，并分析估计CPU和内存的所有组合以及相应的存储带宽需求。基于分析结果，它执行基于轮的调度，并沿着多个资源维度贪婪地打包可运行的作业，目的是最小化每轮T1中的碎片。CODA[180]观察到，由于CPU资源争用，位于同一计算节点内的CPU作业可能会干扰训练作业。然后，它设计了三个组件来优化系统范围的性能：自适应CPU分配器为每个DL训练作业识别最佳CPU内核；实时争用消除器监视并调节每个CPU作业的存储器带宽，以减少其对GPU训练作业的干扰；多阵列作业调度器允许CPU作业相应地抢占由GPU作业保留的CPU核，反之亦然。实验结果表明，CODA可以在不牺牲CPU作业性能的情况下有效地提高GPU的利用率。</p>
<p>Beyond the CPU and memory resources, network bandwidth is another bottleneck for efficient DL training. Ada-SRSF [146] is a two-stage framework for mitigating the communication contention among DLT jobs. In the job scheduling stage, it is combined with the classical SRSF algorithm to relax the contention of two jobs if it can reduce the job completion time. In the job placement stage, it strives to balance the resource utilization and communication overhead. Liquid [47] proposes a cluster network-efficient scheduling solution to achieve better placement for PS-based distributed workloads. Specifically, it adopts a random forest model to predict job resource requirements and then uses the best-fit algorithm and grouping genetic algorithm to optimize the execution performance of DL jobs. Parrot [88] is a framework to manage network bandwidth contention among training jobs using the PS architecture. The communication scheme in a PS workload exhibits a coflow chain dependency where the event of parameter-pull happens after the event of parameter-push. Parrot tries to assign the bandwidth of each physical link to coflows while satisfying the dependency constraints in order to minimize the JCT. It adopts a least per-coflow attained service policy to prioritize jobs. Then it uses a linear program (LP) solution to derive a weighted bandwidth scaling strategy to minimize the time cost in the communication stage.</p>
<p>除了CPU和内存资源之外，网络带宽是高效DL训练的另一个瓶颈。Ada SRSF[146]是一个两阶段框架，用于缓解DLT作业之间的通信争用。在作业调度阶段，如果可以减少作业完成时间，则将其与经典的SRSF算法相结合，以缓解两个作业的争用。在任务放置阶段，它努力平衡资源利用率和通信开销。Liquid[47]提出了一种集群网络高效调度解决方案，以实现基于PS的分布式工作负载的更好布局。具体来说，它采用随机森林模型来预测作业资源需求，然后使用最佳拟合算法和分组遗传算法来优化DL作业的执行性能。Parrot[88]是一个使用PS架构管理训练作业之间网络带宽争用的框架。PS工作负载中的通信方案表现出共流链依赖性，其中参数拉取事件发生在参数推送事件之后。Parrot试图将每个物理链路的带宽分配给余流，同时满足依赖性约束，以最小化JCT。它采用了最低成本的服务政策来优先考虑工作。然后，它使用线性规划（LP）解决方案来推导加权带宽缩放策略，以最小化通信阶段的时间成本。</p>
<h5 id="3-2-2-GPU-Sharing"><a href="#3-2-2-GPU-Sharing" class="headerlink" title="3.2.2 GPU Sharing."></a>3.2.2 GPU Sharing.</h5><p>With the increased compute capability and memory capacity of GPUs, the conventional placement approach which makes each DL job exclusively use the GPU can lead to severe resource underutilization. It is now more promising to perform GPU sharing to fully exploit GPU resources and improve the system throughput T5. In this context, utilization is more inclined to the usage of every single GPU instead of the occupied GPU quantity at the datacenter scale.</p>
<p>随着GPU的计算能力和内存容量的增加，使每个DL作业专门使用GPU的传统放置方法可能导致严重的资源利用不足。现在更有希望执行GPU共享以充分利用GPU资源并提高系统吞吐量T5。在这种情况下，利用率更倾向于每个GPU的使用，而不是数据中心规模的占用GPU数量。</p>
<p>Some works profile and revoke unsuitable jobs to achieve efficient GPU sharing. Salus [168] focuses on fine-grained GPU sharing with two primitives: fast job switching enables efficient time sharing and rapid preemption for active DL jobs on a GPU; memory sharing addresses the memory management issues to ensure high utilization by packing more small DL jobs on the same device.Gandiva [152] designs a packing mechanism to pack multiple jobs on one GPU under the constraints of GPU memory and job performance. It utilizes a profiling mechanism to monitor and unpack jobs that could affect jobs’ performance. Jigsaw [79] is designed upon a novel distributed training scheme named Structured Partial Backpropagation (SPB). SPB allows each worker not to perform the entire backward pass in the distributed training. This can save lots of compute resources, and enable efficient time- and space-multiplexing across jobs in a single GPU. Although SPB can reduce the cluster-wide JCT, it might lead to accuracy loss to some extent. Recently, Antman [153] is introduced, which co-designs the infrastructure between the cluster scheduler and DL framework engine to efficiently manage GPU resources in a fine-grained manner. It supports the co-execution of multiple jobs on a GPU device, and thus largely improves the overall compute resource utilization. Ali-MLaaS [149] provides a comprehensive analysis of large-scale workload traces in Alibaba, and discloses the benefit of GPU sharing in production GPU datacenters.</p>
<p>一些工作配置和撤销不合适的作业，以实现高效的GPU共享。Salus[168]专注于具有两个基元的细粒度GPU共享：快速作业切换实现GPU上活跃的DL作业的高效时间共享和快速抢占；内存共享解决了内存管理问题，通过在同一设备上打包更多的小DL作业来确保高利用率。Gandiva[152]设计了一种打包机制，在GPU内存和作业性能的约束下，将多个作业打包在一个GPU上。它利用一种分析机制来监视和解包可能影响作业性能的作业。Jigsaw[79]是基于一种新的分布式训练方案设计的，该方案名为结构化部分反向传播（SPB）。SPB允许每个作业在分布式训练中不执行整个后向传播。这可以节省大量计算资源，并在单个GPU中实现跨作业的高效时间和空间复用。尽管SPB可以减少集群范围内的JCT，它可能会在一定程度上导致准确性损失。最近，Antman[153]被引入，它设计了集群调度器和DL框架引擎之间的基础设施，以细粒度的方式有效地管理GPU资源。它支持在GPU设备上协同执行多个作业，从而大大提高了整体计算资源利用率。Ali MLaaS[149]对阿里巴巴的大规模工作负载痕迹进行了全面分析，并揭示了GPU共享在生产GPU数据中心的好处。</p>
<p>Alternatively, some works use data-driven approaches to make the GPU sharing decision. Horus [159, 160] designs a prediction-based interference-aware mechanism that can be integrated with existing DL training scheduling frameworks. The prediction engine in Horus is in charge of estimating the GPU usage of each DL job by accessing its graph and dry running the model upon the job submission. Based on the prediction results, Horus allocates GPU resources to DL jobs via de-prioritizing co-location placement decisions that would result in JCT slowdown from the severe interference and communication delays. Co-scheML [76] also measures some metrics for each DL job and uses a random forest model to predict the interference. Then the scheduler makes the decision with the aim of fully utilizing the cluster resources. Analogously, Liquid [47] also supports fine-grained GPU sharing for further resource utilization improvement using a random forest model. Harmony [9] applies an RL model to make placement decisions for minimizing the interference and maximizing the throughput for bin packing DL workloads in a GPU datacenter. It contains a reward function for the prediction module and RL placement decision-making module.This reward function aims to maximize the normalized training speed across all the concurrent jobs in a fixed scheduling interval. The training speed estimation of bin-packing jobs can not be directly obtained, and it depends upon a neural network model via supervised learning from historical logs.To stabilize and accelerate RL model training, Harmony adopts several techniques including actorcritic, job-aware action space, and experience replay. Putting them together, Harmony outperforms significantly over heuristic baselines.</p>
<p>或者，一些作品使用数据驱动的方法来做出GPU共享决策。Horus[159160]设计了一种基于预测的干扰感知机制，该机制可以与现有的DL训练调度框架集成。Horus中的预测引擎负责通过访问每个DL作业的网络和在提交时试运行模型来估计其GPU使用情况。基于预测结果，Horus通过取消同位置放置决策的优先级来将GPU资源分配给DL作业，这将导致严重干扰和通信延迟，从而导致JCT减慢。Co-scheML[76]还测量了每个DL作业的一些度量，并使用随机森林模型来预测干扰。然后调度器以充分利用集群资源为目标进行决策。类似地，Liquid[47]还支持细粒度GPU共享，以使用随机森林模型进一步提高资源利用率。Harmony[9]应用RL模型来做出布局决策，以最大限度地减少GPU数据中心中DL工作负载的干扰并最大限度地提高吞吐量。它包含预测模块和RL布局决策模块的奖励函数。该奖励函数旨在在固定的调度间隔内最大化所有并发作业的归一化训练速度。装箱作业的训练速度估计不能直接获得，它依赖于通过从历史日志中监督学习的神经网络模型。为了稳定和加速RL模型训练，Harmony采用了几种技术，包括演员评论家、工作感知动作空间和经验回放。将它们放在一起，Harmony显著优于基线。</p>
<h5 id="3-2-3-Elastic-Training"><a href="#3-2-3-Elastic-Training" class="headerlink" title="3.2.3 Elastic Training."></a>3.2.3 Elastic Training.</h5><p>In order to maximize the GPU utilization and improve the training efficiency, many novel schedulers support elastic training, which dynamically adjusts the parallelism and resource allocation of workloads to achieve the objectives C1 T6.</p>
<p>为了最大限度地提高GPU利用率和训练效率，许多新颖的调度器支持弹性训练，该训练动态调整工作负载的并行性和资源分配，以实现目标C1 T6。</p>
<p>Gandiva [152] designs a Grow-Shrink mechanism which uses the profiling information to estimate each job’s progress rate and then allocates GPUs accordingly. Optimus [115] estimates the loss reduction rate on any placement policies based on a performance model. then it designs a greedy resource allocation scheme to prioritize the maximum marginal loss reduction. This greedy policy successfully maximizes the cluster-wide training throughput. Elan [154] designs several mechanisms to achieve efficient elastic training: hybrid scaling can better trade-off the training efficiency and model performance; concurrent IO-free replication leverages RDMA to reduce the numbers of IO and CPU-GPU memory copy operations; asynchronous coordination avoids the high overhead of start and initialization during re-adjustments. With the integration of the FIFO and backfill scheduling algorithms, Elan successfully improves the cluster resource utilization and reduces the job pending time. AFS [64] is proposed based on the insight that the scheduler should proactively prepare for the resource contention in the future by utilizing the current resources. It considers both resource efficiency and job length for resource allocation while amortizing the cost of future jobs into the calculation of the current share. Besides, a DL training system framework, CoDDL, is also implemented to deliver automatic job parallelization and efficient re-adjustments. EDL [151] also supports elasticity in DL job scheduling. It implements stop-free scaling and graceful exit to minimize the scale-out and scale-in overheads respectively. Furthermore, EDL optimizes the data allocation pipeline by on-demand and pre-fetching data.</p>
<p>Gandiva[152]设计了一种增长-收缩机制，该机制使用分析信息来估计每个作业的进度，然后相应地分配GPU。Optimus[115]基于性能模型估计任何放置策略的损失减少率。然后设计了一个贪心资源分配方案来优先考虑最大边际损失的减少。这种贪心策略成功地最大化了集群范围内的训练吞吐量。Elan[154]设计了几种机制来实现高效的弹性训练：混合缩放可以更好地权衡训练效率和模型性能；并发无IO复制利用RDMA来减少IO和CPU-GPU内存复制操作的数量；异步协调避免了重新调整期间启动和初始化的高开销。通过集成FIFO和回填调度算法，Elan成功地提高了集群资源利用率，减少了作业挂起时间。AFS[64]是基于调度器应该通过利用当前资源来主动为未来的资源争用做准备这一观点而提出的。它考虑了资源分配的资源效率和作业长度，同时将未来作业的成本分摊到当前份额的计算中。此外，还实现了DL训练系统框架CoDDL，以提供自动作业并行化和高效的重新调整。EDL[151]也支持DL作业调度的弹性。它实现了无停止扩展和优雅退出，最大限度地减少了缩小和扩展开销。此外，EDL通过按需和预取数据来优化数据分配管道。</p>
<p> MARBLE [56] enables elastic DL training in HPC systems. It determines the optimal number of GPUs through offline profiling and employs a FIFO-based policy for scheduling. Vaibhav et al [122] designs a job scalability analyzer and a dynamic programming based optimizer to determine the batch sizes and GPU counts for DL jobs.OASiS [10] introduces a primal-dual framework for optimizing the distributed PS-based jobs, which is coupled with efficient dual subroutines to achieve good long-term performance guarantees with polynomial time complexity. During the training, OASiS dynamically scales in or scales out the number of workers and parameter servers for the best resource utilization and training expedition.</p>
<p>MARBLE[56]在HPC系统中实现弹性DL训练。它通过离线评测确定GPU的最佳数量，并采用基于FIFO的策略进行调度。Vaibhav等人[122]设计了一个作业可扩展性分析器和一个基于动态编程的优化器，以确定DL作业的batch size和GPU个数。OASiS[10]引入了一种用于优化分布式基于PS的作业的原始-对偶框架，该框架与高效的对偶子程序相结合，以实现具有多项式时间复杂性的良好长期性能保证。在培训过程中，OASiS会动态地增加或减少作业数目和参数服务器的数量，以实现最佳的资源利用率和训练规模。</p>
<p>Some online scheduling algorithms adopt the elastic training mechanism for datacenter optimization. For instance, GADGET [166] formulates a resource scheduling analytical model for ring-all-reduce DL and uses a greedy approach to maximize the utility of unfinished jobs. It obtains provable guarantee for high performance. AOnline [178, 184] uses the integer linear program to formulate the maximum weighted schedule problem. It schedules a job if its weight is higher than its estimated serving cost to maximize the total weight of scheduled jobs.</p>
<p>一些在线调度算法采用弹性训练机制进行数据中心优化。例如，GADGET[166]为环形减少DL制定了一个资源调度分析模型，并使用贪心方法最大化未完成作业的效率。它获得了可证明的高性能保证。AOnline[178184]使用整数线性规划来制定最大加权调度问题。如果作业的权重高于其估计的服务成本，则它会调度作业，以最大化调度作业的总权重。</p>
<p>A number of works apply RL to optimize the elastic training policy. Specifically, RIFLING [18] adopts K-means to divide concurrent jobs into several groups based on the computationcommunication ratio similarity. The group operation reduces the state space and accelerates the convergence speed of the RL model. The RL model only determines the number of GPUs and nodes for each job. This can effectively reduce the action space. A reward function is designed to minimize the resource fragmentation and improve the job throughput. RIFLING chooses the Q-Learning algorithm and allows the RL model to perform online update from historical logs to adapt to the workload variation. [116] is another RL scheduler focusing on the PS architecture and dynamically adjusts the resources allocated to the parameter server and workers. It mitigates the optimization instability by a combination of offline supervised learning and online actor-critic reinforcement learning. The RL model also takes the job state and resource state as input and then makes the resource allocation decision for each job. The reward function targets the clusterwide normalized epoch progress. These techniques enable [116] to present satisfactory job latency reduction even for unseen job types.</p>
<p>许多工作应用RL来优化弹性训练策略。具体而言，RIFLING[18]采用K-means方法，根据计算通信比相似性将并发作业划分为多组。群运算减少了RL模型的状态空间，加快了RL模型收敛速度。RL模型仅确定每个作业的GPU和节点的数量。这样可以有效地减少动作空间。设计了一个奖励函数，以最大限度地减少资源碎片并提高作业吞吐量。RIFLING选择Q-Learning算法，并允许RL模型根据历史日志进行在线更新，以适应工作负载的变化。[116]是另一个专注于PS架构的RL调度器，并动态调整分配给参数服务器和作业的资源。它通过离线监督学习和在线演员-评论家强化学习的结合来缓解优化的不稳定性。RL模型还将作业状态和资源状态作为输入，然后为每个作业做出资源分配决策。奖励函数的目标是集群范围内的归一化epoch进度。这些技术使[116]即使对于看不见的作业类型也能提供令人满意的作业延迟减少。</p>
<p>Some works focus on the optimization of elasticity implementation in practical schedulers, e.g., Kubernetes. Wang et al [147] developed an elastic scheduling framework as plugins in Kubernetes.It uses the training job progress information to allocate and reallocate GPUs to minimize JCT. It efficiently reallocates GPUs based on a SideCar process, which introduces an early initialization mechanism for fast reshaping down and achieves non-intrusion to DL training frameworks. DynamoML [21] is a Kubernetes platform which combines KubeShare [158] and Dragon [92] for DL workload scheduling. Dragon [92] fills the gap that existing Kubernetes schedulers fail to manage the distributed training workloads. It resolves this issue by introducing three enhancements including gang-scheduling, locality-aware placement and autoscaling of training workloads. DynamoML also supports scheduling optimization for inference jobs, which will be discussed in Sec. 4.</p>
<p>一些工作集中于实际调度器中弹性实现的优化，例如Kubernetes。王等人[147]开发了一个弹性调度框架作为Kubernetes中的插件。它使用训练工作进度信息来分配和重新分配GPU，以最大限度地减少JCT。它基于SideCar过程有效地重新分配GPU，引入了一种早期初始化机制来快速重构，并实现了对DL训练框架的非入侵。DynamoML[21]是一个Kubernetes平台，它结合了KubeShare[158]和Dragon[92]用于DL工作负载调度。Dragon[92]填补了现有Kubernetes调度器无法管理分布式训练工作负载的空白。它通过引入三种增强功能来解决这个问题，包括集群调度、位置感知和训练工作量的自动缩放。DynamoML还支持推理作业的调度优化，这将在第4节中讨论。</p>
<p>In addition to the elasticity of GPU resources, DL job configurations can also be dynamically adjusted C3. Pollux [117] aims to achieve higher utilization by automatically configuring the DL training jobs and co-adaptively allocating resources to them. Specifically, it defines goodput, a metric for comprehensively measuring training performance including system throughput and statistical efficiency. It designs a joint scheduling architecture to maximize the goodput. At the job-level granularity, Pollux dynamically tunes the batch size and learning rate for the best utilization of the allocated resources. At the cluster-level granularity, Pollux dynamically (re-)allocates resources based on the goodput of all the jobs sharing the cluster as well as other cluster-level objectives (e.g., fairness, JCT). Aryl [85] further extends Pollux by dynamically loaning idle inference GPU nodes to training jobs. It brings higher cluster utilization and lower queuing time.</p>
<p>除了GPU资源的弹性之外，DL作业配置也可以动态调整C3。Pollux[117]旨在通过自动配置DL训练作业并自适应地为其分配资源来实现更高的利用率。具体地说，它定义了goodput，这是一种全面衡量训练性能的指标，包括系统吞吐量和统计效率。它设计了一个联合调度架构，以最大限度地提高吞吐量。在作业级别的粒度上，Pollux动态调整batch size和学习速率，以获得所分配资源的最佳利用率。在集群级别的粒度上，Pollux根据共享集群的所有作业的吞吐量以及其他集群级别的目标（例如，公平性、JCT）动态（重新）分配资源。Aryl[85]通过动态借用空闲推理GPU节点来训练作业，进一步扩展了Pollux。它带来了更高的集群利用率和更低的排队时间。</p>
<p> Similar to Pollux, ONES [12] automatically manages the elasticity of each job based on the training batch size.It designs an online evolutionary search algorithm to continuously optimize the scheduling decisions, which achieves superior performance compared with greedy strategies. More recently, Microsoft presents Singularity [129], an epochal distributed scheduling system for Azure DL training and inference workloads. It achieves transparent preemption, migration and elasticity across a global fleet of AI accelerators (e.g., GPUs, FPGAs). It implements device proxy for the decoupled execution and elastic scheduling across the workers. Although it is developed for public cloud services, the promising techniques are also effective in managing private GPU datacenters.</p>
<p>与Pollux类似，ONES[12]根据训练batch size自动管理每个作业的弹性。它设计了一种在线进化搜索算法来持续优化调度决策，与贪婪策略相比，该算法具有更好的性能。最近，微软推出了Singularity[129]，这是一个划时代的用于Azure DL训练和推理工作负载的分布式调度系统。它实现了全球人工智能加速器（如GPU、FPGA）的透明抢占、迁移和弹性。它实现了设备代理，用于解耦执行和跨作业的弹性调度。尽管它是为公共云服务开发的，但有前景的技术在管理私有GPU数据中心方面也很有效。</p>
<h4 id="3-3-Implications-影响"><a href="#3-3-Implications-影响" class="headerlink" title="3.3 Implications 影响"></a>3.3 Implications 影响</h4><p>The scheduling objective plays an important role in designing schedulers for GPU datacenters.A majority of schedulers consider timing-efficiency and fairness. In contrast, other objectives including deadline guarantee, cost efficiency and accuracy efficiency are not fully explored yet, although they have been thoroughly considered in the conventional cloud and HPC systems.Modern cloud providers are accelerating the pace of building GPU platforms to support a sizable number of training workloads. We anticipate these objectives are also important for training workload management. This inspires researchers and developers to jointly optimize their objectives with the constraints of deadline guarantee and cost.</p>
<p>调度目标在GPU数据中心的调度器设计中起着重要作用。大多数调度器考虑时间效率和公平性。相比之下，包括最后期限保证、成本效率和准确性效率在内的其他目标尚未得到充分探索，尽管它们在传统的云和HPC系统中已经得到了充分考虑。现代云提供商正在加快构建GPU平台的步伐，以支持大量的训练工作负载。我们预计这些目标对培训工作量管理也很重要。这激励了研究人员和开发人员在截止日期保证和成本的约束下共同优化他们的目标。</p>
<p>According to the unique resource consumption features of DL training jobs, datacenter managers can enhance the overall resource utilization and improve users’ experience through designing more efficient resource allocation mechanisms, e.g., fine-grained job placement on GPUs, dynamic job parallelism adjustment, adaptive CPU allocation, etc. However, these approaches have their limitations that can hinder their deployment in practice. For instance, adaptive training could change jobs’ batch size, learning rate and GPU amount, which can cause model convergence issues.Its generalization for more application scenarios also requires more validations. Job colocation can cause potential performance degradation and fault tolerance issue, which can make users unwilling to adopt this feature. How to address these practical issues is a promising and challenging future direction. We look forward to seeing more progress in this topic.</p>
<p>根据DL培训作业独特的资源消耗特征，数据中心管理者可以通过设计更高效的资源分配机制来提高整体资源利用率，改善用户体验，例如GPU上的细粒度作业布局、动态作业并行度调整、自适应CPU分配等，这些方法有其局限性，可能会阻碍它们在实践中的部署。例如，自适应训练可能会改变作业的批量大小、学习率和GPU数量，这可能会导致模型收敛问题。它对更多应用场景的泛化也需要更多的验证。作业主机代管可能会导致潜在的性能下降和容错问题，这可能会让用户不愿意采用此功能。如何解决这些实际问题是一个充满希望和挑战的未来方向。我们期待着在这一议题上取得更多进展。</p>
<p>Although different scheduling algorithms for conventional workloads and systems have been extensively studied for decades, it still requires more efforts to design effective scheduling algorithms for large-scale GPU datacenters to reduce the operational cost and improve the job throughput. The rapid development of AI technology motivates researchers to investigate the possibility of using machine learning to optimize scheduler designs. From our summary, ML-based schedulers have shown their effectiveness in some scenarios. However, the datacenter managers are still concerned about the reliability and scalability of these ML-based schedulers. We expect more research works will be performed to address these concerns and make these schedulers more practical.</p>
<p>尽管针对传统工作负载和系统的不同调度算法已经被广泛研究了几十年，但为大规模GPU数据中心设计有效的调度算法以降低操作成本并提高作业吞吐量仍然需要付出更多的努力。人工智能技术的快速发展促使研究人员研究使用机器学习优化调度器设计的可能性。根据我们的总结，基于ML的调度器在某些场景中显示了其有效性。然而，<strong>数据中心管理者仍然关心这些基于ML的调度器的可靠性和可扩展性。</strong>我们预计将进行更多的研究工作来解决这些问题，并使这些调度器更加实用。</p>
<h3 id="4-SCHEDULING-DL-INFERENCE-WORKLOADS-调度DL推理工作负载"><a href="#4-SCHEDULING-DL-INFERENCE-WORKLOADS-调度DL推理工作负载" class="headerlink" title="4.SCHEDULING DL INFERENCE WORKLOADS 调度DL推理工作负载"></a>4.SCHEDULING DL INFERENCE WORKLOADS 调度DL推理工作负载</h3><p>As more DL-based applications are released as online services in our daily life, it becomes more critical to manage and schedule large-scale inference workloads in the GPU datacenter. Different from the resource-intensive and long-term training workloads, inference jobs have unique characteristics and requirements (Sec. 2.1.2), which demand new scheduling solutions. Similar as Sec.3, we categorize these inference scheduling techniques based on their objectives, and resource consumption features. Then we give some implications from these works at the end of this section.Table 2 summaries the relevant papers and their features.</p>
<p>随着越来越多基于DL的应用程序在我们的日常生活中作为在线服务发布，在GPU数据中心管理和调度大规模推理工作负载变得更加重要。与资源密集型和长期训练工作负载不同，推理工作具有独特的特点和要求（第2.1.2节），需要新的调度解决方案。类似于Sec。3，我们根据这些推理调度技术的目标和资源消耗特征对其进行了分类。然后，我们在本节末尾给出了这些工作的一些启示。表2总结了相关论文及其特点。</p>
<h5 id="4-1-1-Efficiency"><a href="#4-1-1-Efficiency" class="headerlink" title="4.1.1 Efficiency."></a>4.1.1 Efficiency.</h5><p> As discussed in Sec. 2.2.3, the main objective for scheduling an inference workload is to improve its efficiency. This includes the reduction of inference latency and cost, and improvement of the prediction accuracy I2. The challenge here is that there exist trade-offs among different efficiency goals. Here we discuss the techniques to improve each goal as well as to jointly balance and improve them.</p>
<p>如第2.2.3节所述，调度推理工作负载的主要目标是提高其效率。这包括减少推断延迟和成本，以及提高预测精度I2。这里的挑战是，在不同的效率目标之间存在权衡。在这里，我们讨论了改进每个目标以及共同平衡和改进这些目标的技术。</p>
<p><strong>1）Accuracy efficiency.</strong> Improving the prediction accuracy is a perpetual objective in an inference system. To achieve this, one approach is to collect a set of models, and select the best one to predict the result for each input query. The scheduling decision made includes model selection and resource allocation among different candidates. Ease.ml [87] leverages the input and output shape information of the query sample to automatically select the model. It estimates the potential accuracy improvement of each candidate model and then picks the highest one for actual inference.It also formulates the cost-aware model selection process under both single-tenant and multi-tenant settings with multi-armed bandit and Bayesian Optimization. Another effective approach is model ensemble, which combines the prediction results from multiple models to improve the prediction accuracy and generalization. Clipper [25] examines the benefits brought from the model ensemble in computer vision tasks and applies a linear ensemble method to compute a weighted average of the base model predictions. The linear weights are decided by bandit- and learning-based approaches.Rafiki [148] leverages an RL model to determine the model set for the ensemble. This model is also used to identify the final optimal model combinations, and tune critical parameters, e.g., batch size.</p>
<p><strong>1） 精度效率。</strong> 提高预测精度是推理系统的永恒目标。为了实现这一点，一种方法是收集一组模型，并选择最好的模型来预测每个输入查询的结果。所做出的调度决策包括模型选择和不同候选者之间的资源分配。Ease.ml[87]利用查询样本的输入和输出形状信息来自动选择模型。它估计每个候选模型的潜在精度提高，然后选择最高的模型进行实际推理。它还利用multi-armed bandit(多臂老虎机算法)和贝叶斯优化制定了单租户和多租户环境下的成本感知模型选择过程。另一种有效的方法是模型集成，它将多个模型的预测结果相结合，以提高预测精度和泛化能力。Clipper[25]研究了计算机视觉任务中模型集成带来的好处，并应用线性集成方法计算基本模型预测的加权平均值。线性权重由基于bandit和学习的方法决定。Rafiki[148]利用RL模型来确定集合的模型集。该模型还用于识别最终的最优模型组合，并调整关键参数，例如批量大小。</p>
<p><strong>2）Latency efficiency.</strong> An inference system should have a satisfactory response time, even for burst and fluctuating query requests C7. The latency requirement poses challenges for the scheduler to decide which jobs to be prioritized in the job assignment and rearrangement process.This objective can be achieved via carefully optimizing the resource allocation.</p>
<p><strong>2） 延迟效率。</strong> 推理系统应该具有令人满意的响应时间，即使对于突发和波动的查询请求C7也是如此。延迟要求给调度器决定在作业分配和重排过程中优先考虑哪些作业带来了挑战。这一目标可以通过仔细优化资源分配来实现。</p>
<p>It is common to launch multiple inference execution instances concurrently to meet the corresponding latency requirement as much as possible due to the low GPU utilization for each request(C5). Therefore, the inference scheduler can make scheduling decisions aiming at scaling up resources according to the request density to maintain a satisfactory latency. Clipper [25] conducts linear scaling of inference instances and uses separate docker containers to isolate different models. It replicates the model containers according to the number of queries and applies adaptive batching independently for each model due to the varied execution time. MArk [172, 173] scales the inference instances with the cloud services. It selects and combines different cloud services like AWS EC2 and Lambda in order based on their prices and scaling abilities. Also, it monitors the system loads and request queuing situations proactively and leverages Lambda to scale up instances when there exist requests violating the latency demands. InferLine [24] targets the pipelined inference workloads with multiple stages. It monitors the frequency of queries to each model and makes the scaling decisions of each component separately, to maintain the latency SLOs even during sharp bursts.<br>由于每个请求的GPU利用率较低，通常并发启动多个推理执行实例以尽可能满足相应的延迟需求(C5)。因此，推理调度器可以根据请求密度做出旨在扩展资源的调度决策，以保持令人满意的延迟。Clipper[25]对推理实例进行线性缩放，并使用单独的docker容器来隔离不同的模型。它根据查询的数量复制模型容器，并根据不同的执行时间独立地为每个模型应用自适应批处理。MArk[172,173]用云服务扩展推理实例。它根据价格和扩展能力选择并组合不同的云服务，如AWS EC2和Lambda。此外，它还主动监视系统负载和请求排队情况，并在存在违反延迟需求的请求时利用Lambda扩展实例。interline[24]针对多阶段的流水线推理工作负载。它监视对每个模型的查询频率，并单独做出每个组件的缩放决策，以便即使在急剧爆发期间也能保持延迟slo。<br>A number of works aim to provide bounded latency for inference execution at the system level considering its deterministic execution I1. Clockwork [51] discovers that many DL inference models have deterministic performance because of the underlying deterministic computations. Thus, it guarantees deterministic latency by alleviating the uncertainty introduced by other components of the system. To overcome the uncertainty from memory and cache usages, hardware interactions, and other uncontrollable external performance variations, Clockwork consolidates the configurations among all the system layers during the inference execution, by proactively controlling the memory allocation and deallocation, and disabling concurrent execution of multiple inference workloads to eliminate the interaction. Reducing the parallelism of execution eliminates the interference from other tasks, but inevitably brings lower throughput and resource utilization. To address this issue, Abacus [29] tries to guarantee SLO for query requests under the GPU co-location scenarios. It controls the execution sequence and the co-location situation proactively, rather than the default random-ordered execution overlap. Given the explicit order and specific co-location operators on GPUs, Abacus could predict the estimated running time under co-location from the early offline profiling stage. Based on the estimation, the query controller schedules all the simultaneous inference workloads to guarantee the QoS by searching the optimal execution sequence of DNN operators. ParM [78] migrates the concept of erasure codes from distributed computing to model inference systems, and uses learning-based coded computation to introduce redundancy and thus supports the recovery of inference executions with tail latency or failures.<br>考虑到推理执行的确定性，许多工作旨在为系统级的推理执行提供有界延迟(I1)。Clockwork[51]发现，由于底层的确定性计算，许多深度学习推理模型具有确定性性能。因此，它通过减轻系统其他组件引入的不确定性来保证确定性延迟。为了克服内存和缓存使用、硬件交互和其他不可控的外部性能变化带来的不确定性，Clockwork在推理执行期间整合了所有系统层之间的配置，通过主动控制内存分配和释放，并禁用多个推理工作负载的并发执行来消除交互。降低执行的并行性可以消除来自其他任务的干扰，但不可避免地会降低吞吐量和资源利用率。为了解决这个问题，Abacus[29]尝试在GPU共置场景下保证查询请求的SLO。它主动控制执行顺序和共定位情况，而不是默认的随机顺序执行重叠。给定gpu上明确的顺序和特定的共址运算符，Abacus可以从早期的离线分析阶段预测共址下的估计运行时间。在此基础上，查询控制器通过搜索DNN算子的最优执行顺序，调度所有同时进行的推理工作负载，以保证QoS。ParM[78]将擦除码的概念从分布式计算迁移到模型推理系统，并使用基于学习的编码计算引入冗余，从而支持具有尾部延迟或失败的推理执行的恢复。<br>Some solutions proactively schedules the inference workloads and rearranges the execution sequence at the job level. Irina [150] is the first online inference scheduling system, modeling the satisfaction of latency demands as a scheduling problem. By leveraging preemption for DL inference workloads, Irina dynamically decides whether to preempt the ongoing query and launch the later arrived one, which brings significant reduction of average completion time for inference workloads. The main challenge is that existing ML frameworks are not designed and suitable for preemption during execution. Irina carefully manages the preemption process by adding an exit node to the existing dataflow graph of the inference workload, thus enabling safe preemption at arbitrary moments. It is necessary to have more runtime information about the inference workloads for effective scheduling. Kube-Knots [137] makes predictions about the resource utilization of each inference workload from two aspects. From the spatial aspect, Kube-Knots discovers the correlations across different resource utilization metrics, and then forecasts the future resource utilization. From the temporal aspect, Kube-Knots predicts the peak inference usage and tries to avoid co-locating jobs which could attain peak consumption of the same resources simultaneously.<br>一些解决方案主动调度推理工作负载，并在作业级别重新安排执行顺序。Irina[150]是第一个在线推理调度系统，将延迟需求的满足建模为调度问题。通过利用DL推理工作负载的抢占，Irina动态决定是否抢占正在进行的查询并启动后到达的查询，这大大减少了推理工作负载的平均完成时间。主要的挑战是现有的ML框架的设计不适合在执行期间进行抢占。Irina通过在推理工作负载的现有数据流图中添加退出节点来仔细管理抢占过程，从而在任意时刻实现安全抢占。为了有效地调度，有必要拥有更多关于推理工作负载的运行时信息。Kube-Knots[137]从两个方面对每个推理工作负载的资源利用率进行了预测。从空间方面来看，Kube-Knots发现不同资源利用指标之间的相关性，然后预测未来的资源利用情况。从时间方面来看，Kube-Knots预测峰值推理使用情况，并试图避免可能同时达到相同资源消耗峰值的任务共存。<br><strong>3) Cost-efficiency.</strong>   The monetary cost becomes one of the main concerns when using public cloud resources to deploy DL inference workloads. Considering the varied compute capabilities and prices for different types of resources and services, a couple of schedulers implement many mechanisms to achieve cost-efficient inference. MArk [172, 173] analyzes the cost of utilizing different levels of resource abstractions in Amazon Web Services (AWS) and Google Cloud Platform (GCP) for inference. It finds that the Infrastructure-as-a-Service (IaaS) provides better cost efficiency than Content-as-a-Service (CaaS), while Function-as-a-Service (FaaS) could compensate for the relatively long cold start latency of IaaS at the cost of increased costs. Small instances with advanced CPUs and burstable instances are also recommended. For GPU instances, the cost can be greatly reduced by batch processing as well. Given different levels of capability, scalability, and pricing,<br>MArk greedily selects the most cost-effective type of instances and leverages the spot instances for cost-saving. AutoDeep [89] considers not only the resource type in the cloud but also the device placement for DL inference. It leverages Bayesian Optimization for the nearly optimal cloud configuration and Deep Reinforcement Learning for the nearly optimal device placement. Constrained by the SLO requirements, AutoDeep performs joint optimization to minimize the total cost of the inference execution. Cocktail [52] develops a distributed weighted auto-scaling policyand leverages the spot instances to minimize the cost.<br><strong>3)成本效率。</strong>在使用公共云资源部署深度学习推理工作负载时，货币成本成为主要问题之一。考虑到不同类型的资源和服务的不同计算能力和价格，几个调度器实现了许多机制来实现经济高效的推理。MArk[172,173]分析了在Amazon Web Services (AWS)和Google Cloud Platform (GCP)中利用不同级别的资源抽象进行推理的成本。研究发现，基础设施即服务(IaaS)比内容即服务(CaaS)提供了更好的成本效率，而功能即服务(FaaS)可以弥补IaaS相对较长的冷启动延迟，但代价是增加成本。还建议使用具有高级cpu和突发实例的小型实例。对于GPU实例，批处理也可以大大降低成本。考虑到不同水平的能力、可扩展性和定价，MArk[]172,17.]地选择最具成本效益的实例类型，并利用现货实例来节省成本。AutoDeep[89]不仅考虑了云中的资源类型，还考虑了深度学习推理的设备位置。它利用贝叶斯优化几乎最优的云配置和深度强化学习几乎最优的设备放置。受SLO要求的限制，AutoDeep执行联合优化以最小化推理执行的总成本。Cocktail[52]开发了一种分布式加权自动缩放策略，并利用spot实例来最小化成本。<br><strong>4) Trade-offs between accuracy, latency and cost.</strong>  The objectives of accuracy, latency and cost are not independent C6. Improving one goal may possibly compromise another goal if the solution is not designed properly. Besides, users may also have their specific expectations about different objectives. This motivates researchers to explore the trade-offs between these objectives, and devise more flexible and comprehensive scheduling systems.<br><strong>4)在准确性、延迟和成本之间进行权衡。</strong> 准确性、延迟和成本的目标不是相互独立的(C6)。如果解决方案设计不当，改进一个目标可能会损害另一个目标。此外，用户也可能对不同的目标有自己特定的期望。这促使研究人员探索这些目标之间的权衡，并设计出更灵活、更全面的调度系统。<br>The adoption of multiple models can improve the model inference accuracy, but might also increase the response latency and cost. Several works track the latency and prediction accuracy of different models and implement mechanisms to select the most appropriate ones determined by the schedulers. Clipper [25] introduces a model selection abstraction, which supports both single model selection and model ensemble selection. It executes the inference for all the models and combines their results. It observes the corresponding accuracy and latency feedback continuously to make the selection with a best-effort search method. Model-Switching [176] pursues the trade-offs between computational cost and service accuracy by switching different model variants proactively, to improve the accuracy of responses under the latency constraint. By maximizing the ratio of correct predictions returned within the deadline, it makes selections among model variations with different computation demands and accuracy. Cocktail [52] balances the cost with accuracy and latency on the public cloud via the optimization of the model ensemble. With a dynamic model selection policy that searches models tightly with the accuracy and latency bounds, Cocktail reduces the candidates in the ensemble and accomplishes fast and efficient model selection.<br>采用多模型可以提高模型的推理精度，但也可能增加响应延迟和成本。一些工作跟踪了不同模型的延迟和预测精度，并实现了由调度器确定的选择最合适模型的机制。Clipper[25]引入了一个模型选择抽象，它既支持单个模型选择，也支持模型集成选择。它对所有模型执行推理并组合它们的结果。连续观察相应的精度和延迟反馈，用最优搜索法进行选择。模型切换[176]通过主动切换不同的模型变量，在计算成本和服务精度之间进行权衡，以提高延迟约束下响应的准确性。通过最大限度地提高在期限内返回正确预测的比例，在不同计算需求和精度的模型变量中进行选择。Cocktail[52]通过优化模型集合，在公共云上平衡了成本与准确性和延迟。Cocktail采用一种动态模型选择策略，严格按照准确率和延迟界限搜索模型，减少了集合中的候选模型，实现了快速高效的模型选择。<br>Some schedulers allow users to specify their demands about accuracy, latency and cost, and make scheduling decisions directly according to the demands. Tolerance Tiers [55] discloses the efforts the system can offer to achieve each objective, and makes users programmatically select their demands. Observing that improving the accuracy of some extreme requests can increase the latency greatly, Tolerance Tiers relaxes and sacrifices the accuracy demand to improve the latency and service cost. Each tier defines an error tolerance to indicate the tolerable accuracy loss, and an optimization objective. Then, Tolerance Tiers optimizes the objective under the constraint of the maximum error tolerance. INFaaS [121, 155] also asks for the performance and accuracy demands from the users. It generates some variants from the existing models with different specific parameters (e.g., batch size, hardware configurations, hardware-specific parameters). After one-time profiling for each variant, INFaaS selects the model variant based on the resource consumption and performance information from the profiling to serve users’ requests. Since each model variant may have different performance and monetary costs during execution, INFaaS makes the selection via a heuristic-based approach, which selects the variant with the minimum cost while meeting the SLO constraint, or upgrades existing variants with higher performance to fulfill the burst queries.<br>一些调度器允许用户指定他们对准确性、延迟和成本的要求，并直接根据需求做出调度决策。容差等级[55]揭示了系统为实现每个目标所能付出的努力，并让用户以编程方式选择他们的需求。考虑到提高某些极端请求的准确性会大大增加延迟，容忍层放宽和牺牲准确性需求，以提高延迟和服务成本。每一层都定义了一个误差容忍度，以表示可容忍的精度损失，以及一个优化目标。然后，在最大误差容忍度约束下，对目标进行优化。INFaaS[121,155]也要求用户对性能和精度提出要求。它从具有不同特定参数(例如，批处理大小、硬件配置、硬件特定参数)的现有模型中生成一些变体。在对每个变体进行一次性分析之后，INFaaS根据分析中的资源消耗和性能信息选择模型变体，以满足用户的请求。由于每个模型变体在执行过程中可能具有不同的性能和货币成本，因此INFaaS通过基于启发式的方法进行选择，该方法在满足SLO约束的情况下选择成本最低的变体，或者升级具有更高性能的现有变体以满足突发查询。<br>4.1.2 System Throughput. Another important objective for scheduling inference workloads is to improve its throughput capability. The techniques to achieve this goal is summarized as follows.<br>4.1.2系统吞吐量。调度推理工作负载的另一个重要目标是提高其吞吐量能力。实现这一目标的技术总结如下。</p>
<p><strong>1) Batching execution. </strong> One common approach is to batch multiple inference queries, and execute them concurrently. Handling individual inference queries usually leads to GPU underutilization C5. Hence, batching inference can efficiently improve the utilization and reduce the system overhead. Like job queuing in parallel job scheduling, batching multiple queries can delay the execution of the requests that come earlier, and possibly jeopardize the SLO requirement. Setting a proper batch size is critical to balance such delays and system throughput. Most schedulers dynamically adjust this hyperparameter based on the actual SLO requirement and queuing situation.<br><strong>1)批量执行。</strong> 一种常见的方法是批量处理多个推理查询，并并发地执行它们。处理单个推理查询通常会导致GPU利用率不足(C5)。因此，批处理推理可以有效地提高利用率，降低系统开销。与并行作业调度中的作业排队一样，批处理多个查询可能会延迟较早到来的请求的执行，并可能危及SLO需求。设置适当的批处理大小对于平衡此类延迟和系统吞吐量至关重要。大多数调度器都会根据实际的SLO需求和排队情况动态调整这个超参数。<br>First, some schedulers adopt heuristic methods to tune the batch size. Clipper [25] and Rafiki [148] apply the practical Additive-Increase-Multiplicative-Decrease (AIMD) algorithm to adjust the inference batch size. Specifically, the batch size is additively increased by a fixed amount until the latency of processing a batch exceeds the latency requirement and then multiplicatively decreased by a fixed percent. Clipper evaluates that AIMD is simple yet effective and adaptive to the changing throughput of a model in special cases. It also aggressively delays the execution of queries under moderate loads to the subsequent batch, which can bring a significant throughput increase for some models. GSLICE [31] also applies a similar adaptive and self-learning approach to determine the optimal batch size. It carefully tracks the execution time and increases the batch size until the execution of the last batch exceeds the SLO requirement. Ebird [26, 27] proposes a novel elastic batching mechanism, which runs different CUDA streams concurrently for different batches. Motivated by the observation that processing multiple queries in a large batch has similar performance as multiple CUDA streams with smaller batch sizes, Ebird dynamically adjusts the batch size per stream to utilize the spare GPU resources and fulfill the whole GPU. In each scheduling round, it selects and launches a job based on its batch size and remaining GPU resources in a best-effort manner, squeezing the full GPU utilization and minimizing inference queuing delay.<br>首先，一些调度器采用启发式方法来调优批处理大小。Clipper[25]和Rafiki[148]采用实用的AIMD (additive - increase - multiply - reduction)算法来调整推理批大小。具体地说，批大小以固定的数量增加，直到处理批的延迟超过延迟要求，然后以固定的百分比乘法减少。Clipper评价AIMD简单有效，在特殊情况下能够适应模型吞吐量的变化。它还积极地将中等负载下的查询执行延迟到后续批处理，这可以为某些模型带来显着的吞吐量增加。GSLICE[31]也采用类似的自适应和自学习方法来确定最优批大小。它仔细跟踪执行时间并增加批处理大小，直到最后一批处理的执行超过SLO要求。Ebird[26,27]提出了一种新的弹性批处理机制，该机制为不同的批处理并发运行不同的CUDA流。Ebird观察到大批量处理多个查询与批量大小较小的多个CUDA流具有相似的性能，因此Ebird动态调整每个流的批处理大小，以利用备用GPU资源并满足整个GPU的需求。在每个调度轮中，它根据其批处理大小和剩余GPU资源以最佳方式选择并启动作业，从而压缩GPU的全部利用率并最大限度地减少推理排队延迟。<br>Second, some schedulers propose optimization-based methods to balance the inference delay and throughput. MArk [172, 173] considers the maximum time of delaying a query, profiles the processing rate without batching, and searches for the optimal batch size under the SLO constraint. Nanily [136] presents the upper bound of the batch size by retrieving the maximum remaining time for the requests, calculated as the remaining time towards the deadline subtracted by the least queuing time for the available resources. It then derives the corresponding batch size, which makes the inference execution time equal or close to the maximum remaining time. DyBatch [179] considers the fairness of the delay for each independent workload when batching. It implements fine-grained batching schemes along with fairness-driven scheduling, which can compensate for the deviation of slowdown for small inference workloads. DyBatch organizes the workload batches in a time-sharing manner and selects the batch with the lowest resource utilization for running, thus maintaining fairness and minimizing the slowdown of each workload.<br>其次，一些调度器提出了基于优化的方法来平衡推理延迟和吞吐量。MArk[172,173]考虑了查询延迟的最大时间，在没有批处理的情况下分析了处理速率，并在SLO约束下搜索了最优批处理大小。Nanily[136]通过检索请求的最大剩余时间来给出批大小的上界，计算为截止日期前的剩余时间减去可用资源的最小排队时间。然后，它派生出相应的批处理大小，这使得推理执行时间等于或接近最大剩余时间。DyBatch[179]在批处理时考虑了每个独立工作负载延迟的公平性。它实现了细粒度的批处理方案以及公平驱动的调度，这可以补偿小型推理工作负载的减速偏差。DyBatch以分时的方式组织工作负载批次，并选择资源利用率最低的批次运行，从而保持公平性并最大限度地减少每个工作负载的减速。<br><strong>2) Caching and reusing.</strong>  Another widely-used strategy for throughput improvement is caching and reusing the prediction results across different requests C7. The scheduler selects the request that benefits most from caching and allocates proper resources. This can be done at two levels.<br><strong>2)缓存和重用。</strong> 另一个广泛使用的吞吐量改进策略是缓存和跨不同请求重用预测结果C7。调度器选择从缓存中获益最多的请求，并分配适当的资源。这可以在两个层面上完成。<br>The first direction is to perform optimization at the query level. To provide fast responses to different queries, the inference system can cache the inference execution and prediction results for burst queries. Clipper [25] maintains a prediction cache for requests with the same target model and the query input. Then it can produce the results for some queries without evaluating the model, thus increasing the inference throughput. Clipper also applies an LRU cache eviction policy to optimize the caching efficiency. However, this approach may be less efficient when the queries do not have high similarities in practical scenarios, which leads to high cache miss rates and evictions.<br>第一个方向是在查询级别执行优化。为了提供对不同查询的快速响应，推理系统可以缓存突发查询的推理执行和预测结果。Clipper[25]为具有相同目标模型和查询输入的请求维护一个预测缓存。然后，它可以在不评估模型的情况下产生某些查询的结果，从而提高推理吞吐量。Clipper还应用了LRU缓存退出策略来优化缓存效率。然而，当查询在实际场景中没有很高的相似性时，这种方法可能效率较低，这会导致高缓存丢失率和清除。<br>The second direction is to perform optimization at the device level. Gillman et al [45] proposed to cache the DL models instead of the inference results. It schedules models to be loaded into the limited GPU memory to maximize the probability of servicing an incoming request without swapping the models in and out of the memory, thus accelerating the inference by eliminating the cold start latency with cache hits. The caching and eviction policy considers many runtime aspects of DL inference workloads, including model size, frequency, model accuracy, and speed. This work also discusses some future directions for more dynamic caching mechanisms and policies, like framework-level GPU memory-friendly optimization, proactively loading and evicting, and cluster-level GPU memory allocation. To address the limitation of GPU memory, GSLICE [31] and HiveMind [108] explore the common GPU memory component in different inference models, and propose to save GPU resources via memory sharing. Particularly, GSLICE enables efficient GPU memory sharing by allowing the reuse of model parameters via modifications to the DL framework, exposing the CUDA address to different instances. Therefore, it supports loading the inference model-related parameters only once to the GPUs, resulting in faster module loading. HiveMind extends the shared content and brings more possibilities of shared model weights and layers across different inference workloads, saving the overhead from both model loading and inference evaluation. TrIMS [30] organizes the memory sharing of different models in a more systematic design. The model resource manager in TrIMS offers a multi-tiered cache for DL models to be shared across users’ FaaS functions and enables DL model sharing across all levels of the memory hierarchy in the GPU, CPU, local storage, and remote storage in the cloud. TrIMS reconciles the lifecycle of model memory consumption and carefully handles the cache misses and evictions. It also considers multi-node, isolation, and fairness problems during sharing. Extensive evaluations on different models show its general abilities to improve the inference performance by mitigating the model loading overhead.<br>第二个方向是在设备级执行优化。Gillman等[45]提出缓存深度学习模型，而不是缓存推理结果。它计划将模型加载到有限的GPU内存中，以最大限度地为传入请求提供服务，而不需要在内存中交换模型，从而通过消除缓存命中的冷启动延迟来加速推理。缓存和回收策略考虑DL推理工作负载的许多运行时方面，包括模型大小、频率、模型准确性和速度。这项工作还讨论了一些动态缓存机制和策略的未来方向，如框架级GPU内存友好优化，主动加载和驱逐，以及集群级GPU内存分配。为了解决GPU内存的局限性，GSLICE[31]和HiveMind[108]探索了不同推理模型中常见的GPU内存组件，并提出通过内存共享来节省GPU资源。特别是，GSLICE允许通过修改DL框架来重用模型参数，从而将CUDA地址暴露给不同的实例，从而实现高效的GPU内存共享。因此，它支持只将与推理模型相关的参数加载到gpu一次，从而加快模块加载速度。HiveMind扩展了共享内容，并在不同的推理工作负载之间提供了更多共享模型权重和层的可能性，从而节省了模型加载和推理评估的开销。TrIMS[30]对不同车型的内存共享进行了更系统的组织设计。TrIMS中的模型资源管理器为DL模型提供了一个多层缓存，以便在用户的FaaS功能之间共享，并使DL模型能够在GPU、CPU、本地存储和云中的远程存储中的所有内存层次中共享。TrIMS协调模型内存消耗的生命周期，并仔细处理缓存丢失和清除。它还考虑了共享过程中的多节点、隔离和公平性问题。对不同模型的广泛评估表明，它能够通过减轻模型负载开销来提高推理性能。<br><strong>3) System configuration tuning.</strong>  Besides the optimization techniques detailed above, there exist some schedulers leveraging end-to-end configuration tuning to improve the system throughput. Morphling [143] formulates the optimal configuration search as a few-shot learning problem. Then it adopts model-agnostic meta-learning (MAML) [40] to combine offline meta-model training for inference serving performance modeling under varied hardware and runtime configurations, and performs online few-shot learning to predict the service performance. Based on the prediction, Morphling auto-tunes the resource provisioning configurations and makes better scheduling decisions. RRL [118] concentrates on optimizing the parallelism configurations from different levels, including request level parallelism and intra-request level (inter-op and intra-op) parallelism, which have strong impacts on the latency of the entire system. RRL utilizes a region-based RL method to tune the parallelism configurations and reduce the inference processing latency, based on the system performance similarity between different configurations within a similar parallelism setting.<br><strong>3)系统配置调优。</strong> 除了上面详细介绍的优化技术之外，还有一些调度器利用端到端配置调优来提高系统吞吐量。Morphling[143]将最优构型搜索表述为一个少次学习问题。然后采用模型无关元学习(model-agnostic meta-learning, MAML)[40]，结合离线元模型训练进行不同硬件和运行时配置下的推理服务性能建模，并进行在线少次学习进行服务性能预测。基于预测，Morphling自动调整资源供应配置并做出更好的调度决策。RRL[118]侧重于从不同层次优化并行配置，包括请求级并行和请求内级(inter-op和intra-op)并行，这对整个系统的延迟有很大的影响。RRL利用基于区域的RL方法来调优并行配置并减少推理处理延迟，这是基于相似并行设置中不同配置之间的系统性能相似性。<br>4.2 Resource Consumption Feature<br>4.2资源消耗特性<br>Similar to training workloads, the inference schedulers can also be categorized based on the resource consumption feature. Below we detail the scheduling solutions designed to target these features.<br>与训练工作负载类似，推理调度器也可以基于资源消耗特性进行分类。下面我们将详细介绍针对这些特性设计的调度解决方案。<br><strong>4.2.1 Colocation and resource sharing.</strong><br> From Challenge C5 in Sec. 2.2.3, executing one inference request can lead to GPU resource underutilization. The recent development of GPU architecture designs motivates the GPU sharing from both the hardware perspective [2, 4] and software perspective [127, 168]. GPU sharing across different inference requests can significantly improve the resource utilization by better leveraging GPUs’ parallel compute capability. However, it can also incur the risks of violating the latency requirements due to the uncertain running time.<br><strong>4.2.1主机托管和资源共享。</strong><br>从第2.2.3节的挑战C5来看，执行一个推理请求可能导致GPU资源利用率不足。GPU架构设计的最新发展从硬件角度[2,4]和软件角度[127,168]激发了GPU共享。跨不同推理请求的GPU共享可以更好地利用GPU的并行计算能力，从而显著提高资源利用率。然而，由于不确定的运行时间，它也可能导致违反延迟需求的风险。<br>One line of research works adopt the static colocation strategy to guarantee the inference latency. Space-Time [68] calls for both space-sharing and time-sharing for DL inference with GPUs. It preserves the predictability and isolation during virtualization by monitoring the inference latency per kernel. Then it improves the utilization by merging concurrent small kernels into larger superkernels that can fill up the GPU utilization under the time-sharing mechanism. Irina [150] only considers a safe GPU colocation situation based on the peak GPU requirement of the workloads, when their total GPU requirement does not exceed the capacity. It assumes there is no interference and slowdown on the job completion time and heuristically places the newly arrived job on the GPU with the smallest JCT. Nexus [126] also applies a heuristic approach to select the requests to be co-located on the same GPU. First, it identifies the most appropriate batch size for throughput and SLO needs of the existing inference workload. Afterward, it establishes all possible combinations within a GPU’s duty cycle on a single GPU in a best-fit manner, and maximizes the utilization without violating the latency requirement.<br>其中一项研究工作采用静态主机托管策略来保证推理延迟。时空[68]要求gpu的深度学习推理同时需要空间共享和时间共享。它通过监视每个内核的推理延迟来保持虚拟化期间的可预测性和隔离性。然后在分时机制下，通过将并发的小内核合并成更大的超级内核来填补GPU的利用率，从而提高利用率。Irina[150]仅在GPU总需求不超过容量的情况下，根据工作负载的GPU峰值需求来考虑安全的GPU托管情况。它假设作业完成时间没有干扰和延迟，并启发式地将新到达的作业放在具有最小JCT的GPU上。Nexus[126]还采用启发式方法来选择在同一GPU上共同定位的请求。首先，它为现有推理工作负载的吞吐量和SLO需求确定最合适的批处理大小。之后，它以最合适的方式在单个GPU上建立GPU占空比内的所有可能组合，并在不违反延迟要求的情况下最大化利用率。<br>Some other works introduce dynamic colocation mechanisms for managing the inference workloads. GSLICE [31] is an inference system to systematically achieve safe and efficient GPU sharing. It leverages spatial GPU multiplexing for different inference requests on top of the state-of-the-art GPU spatial multiplexing framework MPS. It intensively evaluates the colocation performance with and without MPS, and with the resource provisioning limits. It discovers that the colocation interference could be amortized under the careful configuration of the resource limits. The performance improvement reaches a point of diminishing returns (i.e., kneepoint) after certain configurations, which has a non-linear relationship with the throughput and latency of the inference. Based on these observations, GSLICE tracks the kneepoint of different inference workloads and partition the whole GPU according to their kneepoints. It also designs a hot-standby mechanism to dynamically adjust the resource limit configuration of the specific inference job, along with other implementation optimizations including minimizing the data transfer overhead. MIG-SERVING [135] leverages the hardware support for GPU virtualization (i.e., MIG [3] on NVIDIA A100[2]) for efficient GPU colocation. With MIG, an A100 card could be dynamically partitioned into several instances with smaller hardware capacities under some hard constraints. MIG-SERVING discovers that the throughput of most models does not grow linearly with the increase of resources on different instances. It establishes a reconfigurable scheduling problem and applies a generic algorithm to find a sub-optimal and feasible solution, and improve it via a search-based method.<br>其他一些工作介绍了管理推理工作负载的动态托管机制。GSLICE[31]是一个系统实现安全高效GPU共享的推理系统。它利用空间GPU多路复用在最先进的GPU空间多路复用框架MPS之上的不同推理请求。它集中评估了使用和不使用MPS以及使用资源供应限制时的主机托管性能。研究发现，在合理配置资源限制的情况下，可以平摊托管干扰。经过某些配置后，性能改进达到收益递减点(即膝点)，这与推理的吞吐量和延迟具有非线性关系。基于这些观察，GSLICE跟踪不同推理工作负载的膝点，并根据其膝点对整个GPU进行分区。它还设计了一个热备用机制来动态调整特定推理作业的资源限制配置，以及其他实现优化，包括最小化数据传输开销。MIG- serving[135]利用GPU虚拟化的硬件支持(即，在NVIDIA A100[2]上的MIG[3])来实现高效的GPU托管。使用MIG，在一些硬约束下，可以将A100卡动态地划分为几个具有较小硬件容量的实例。MIG-SERVING发现大多数模型的吞吐量并不随着不同实例上资源的增加而线性增长。建立了一个可重构调度问题，应用通用算法寻找次优可行解，并通过基于搜索的方法对其进行改进。</p>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
        <category>调度</category>
      </categories>
      <tags>
        <tag>scheduler</tag>
      </tags>
  </entry>
  <entry>
    <title>使用 torch.profiler记录模型训练轨迹</title>
    <url>/2025/02/03/torchProfile/</url>
    <content><![CDATA[<p>使用 torch.profiler记录模型训练轨迹，并使用Tensorboard进行可视化分析，首先导入需要的库，准备模型和数据集，设置记录器，生成json格式的文件，最后通过Tensorboard可视化。</p>
<span id="more"></span>
<h4 id="Steps"><a href="#Steps" class="headerlink" title="Steps"></a>Steps</h4><ol>
<li>Prepare the data and model</li>
<li>Use profiler to record execution events</li>
<li>Run the profiler</li>
<li>Use TensorBoard to view results and analyze model performance</li>
<li>Improve performance with the help of profiler</li>
<li>Analyze performance with other advanced features</li>
<li>Additional Practices: Profiling PyTorch on AMD GPUs</li>
</ol>
<h4 id="1-Prepare-the-data-and-model"><a href="#1-Prepare-the-data-and-model" class="headerlink" title="1. Prepare the data and model"></a>1. Prepare the data and model</h4><p><strong>导入需要的库:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn</span><br><span class="line"><span class="keyword">import</span> torch.optim</span><br><span class="line"><span class="keyword">import</span> torch.profiler</span><br><span class="line"><span class="keyword">import</span> torch.utils.data</span><br><span class="line"><span class="keyword">import</span> torchvision.datasets</span><br><span class="line"><span class="keyword">import</span> torchvision.models</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> T</span><br></pre></td></tr></table></figure>
<p><strong>准备数据集</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">transform = T.Compose(</span><br><span class="line">    [T.Resize(<span class="number">224</span>),</span><br><span class="line">     T.ToTensor(),</span><br><span class="line">     T.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])</span><br><span class="line">train_set = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">train_loader = torch.utils.data.DataLoader(train_set, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p><strong>模型定义</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span>)</span><br><span class="line">model = torchvision.models.resnet18(weights=<span class="string">&#x27;IMAGENET1K_V1&#x27;</span>).cuda(device)</span><br><span class="line">criterion = torch.nn.CrossEntropyLoss().cuda(device)</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">model.train()</span><br></pre></td></tr></table></figure>
<p><strong>模型训练</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">data</span>):</span><br><span class="line">    inputs, labels = data[<span class="number">0</span>].to(device=device), data[<span class="number">1</span>].to(device=device)</span><br><span class="line">    outputs = model(inputs)</span><br><span class="line">    loss = criterion(outputs, labels)</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>
<h4 id="2-使用Profiler记录轨迹"><a href="#2-使用Profiler记录轨迹" class="headerlink" title="2. 使用Profiler记录轨迹"></a>2. 使用Profiler记录轨迹</h4><p>some useful parameters are as follow:</p>
<p><code>schedule</code>: 参数例如<code>wait=1,warmup=1,active=3,repeat=1</code>(profiler 会跳过第一个step/iteration，在第二个iter热身，记录三个iter。). In total, the cycle repeats once. Each cycle is called a “span” in TensorBoard plugin.</p>
<p>在<code>wait</code>阶段，profiler 不生效，在<code>warmup</code> 阶段，proliler 开始工作但不记录结果，是为了减少开销，proliling 的开始开销很大，会影响结果。</p>
<p><code>on_trace_ready</code> :  在每个cylce结束时调用，例如使用<code>torch.profiler.tensorboard_trace_handler</code>来时生成Tensorboard使用的结果文件，在Profiling后，结果文件存储在<code>./log/resnet18</code>中。</p>
<p><code>record_shapes</code>：是否记录输入张亮的形状</p>
<p><code>profile_memory</code>: 追踪张量空间申请和释放。</p>
<p><code>with_stack</code>：记录算子的代码信息，如果在vscode中集成TensorBoard, 单击可以跳转到特定行。</p>
<p><a href="https://code.visualstudio.com/docs/datascience/pytorch-support#_tensorboard-integration">https://code.visualstudio.com/docs/datascience/pytorch-support#_tensorboard-integration</a></p>
<p><strong>以上下文管理器启动/停止：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.profiler.profile(</span><br><span class="line">        schedule=torch.profiler.schedule(wait=<span class="number">1</span>, warmup=<span class="number">1</span>, active=<span class="number">3</span>, repeat=<span class="number">1</span>),</span><br><span class="line">        on_trace_ready=torch.profiler.tensorboard_trace_handler(<span class="string">&#x27;./log/resnet18&#x27;</span>),</span><br><span class="line">        record_shapes=<span class="literal">True</span>,</span><br><span class="line">        profile_memory=<span class="literal">True</span>,</span><br><span class="line">        with_stack=<span class="literal">True</span></span><br><span class="line">) <span class="keyword">as</span> prof:</span><br><span class="line">    <span class="keyword">for</span> step, batch_data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        prof.step()  <span class="comment"># Need to call this at each step to notify profiler of steps&#x27; boundary.</span></span><br><span class="line">        <span class="keyword">if</span> step &gt;= <span class="number">1</span> + <span class="number">1</span> + <span class="number">3</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        train(batch_data)</span><br></pre></td></tr></table></figure>
<p><strong>也可以以非上下文管理器启动/停止：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">prof = torch.profiler.profile(</span><br><span class="line">        schedule=torch.profiler.schedule(wait=<span class="number">1</span>, warmup=<span class="number">1</span>, active=<span class="number">3</span>, repeat=<span class="number">1</span>),</span><br><span class="line">        on_trace_ready=torch.profiler.tensorboard_trace_handler(<span class="string">&#x27;./log/resnet18&#x27;</span>),</span><br><span class="line">        record_shapes=<span class="literal">True</span>,</span><br><span class="line">        with_stack=<span class="literal">True</span>)</span><br><span class="line">prof.start()</span><br><span class="line"><span class="keyword">for</span> step, batch_data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">    prof.step()</span><br><span class="line">    <span class="keyword">if</span> step &gt;= <span class="number">1</span> + <span class="number">1</span> + <span class="number">3</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    train(batch_data)</span><br><span class="line">prof.stop()</span><br></pre></td></tr></table></figure>
<h4 id="3-运行profiler"><a href="#3-运行profiler" class="headerlink" title="3. 运行profiler"></a>3. 运行profiler</h4><h4 id="4-使用Tensorboard展示结果"><a href="#4-使用Tensorboard展示结果" class="headerlink" title="4. 使用Tensorboard展示结果"></a>4. 使用Tensorboard展示结果</h4><p>安装Pytorch Profiler TensorBoard Plugin</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pip install torch_tb_profiler</span><br></pre></td></tr></table></figure>
<p>登录TensorBoard</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tensorboard --logdir=./log</span><br></pre></td></tr></table></figure>
<p>打开TensorBoard</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">http://localhost:6006/#pytorch_profiler</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>配置</category>
      </categories>
      <tags>
        <tag>configure</tag>
      </tags>
  </entry>
  <entry>
    <title>transformer学习笔记</title>
    <url>/2025/02/03/transformer-not/</url>
    <content><![CDATA[<p>介绍Transformer的两个阶段，以及如何控制推理过程中的KV-Cach缓存大小来优化推理速度，最后分析Transformer的性能瓶颈和优化策略。</p>
<span id="more"></span>
<p>使用 Transformer 解码器（Transformer decoder）生成 tokens 需要以下两个步骤。这两个步骤分别是处理提示语步骤和多个自回归步骤。两个步骤在硬件利用上有着截然不同的特征。</p>
<p><a href="https://segmentfault.com/a/1190000044790264">程序员 - LLM 推理优化探微 (4) ：模型性能瓶颈分类及优化策略 - IDP技术干货 - SegmentFault 思否</a></p>
<h4 id="1-LLM-做出回答的两个阶段"><a href="#1-LLM-做出回答的两个阶段" class="headerlink" title="1.LLM 做出回答的两个阶段"></a>1.LLM 做出回答的两个阶段</h4><p>文本生成的两个阶段包括:启动阶段和生成阶段。</p>
<p>Transformer 解码器的模型轮廓图:<br><img src="/2025/02/03/transformer-not/bed5e37508e74fa390f4ed8949a91dd0.png" alt="在这里插入图片描述"></p>
<p>解码器本身并不输出 tokens，而是输出 logits（数量与词汇表的大小相同）（译者注：logits 是一个数值向量，其维度等于词汇表的大小，表示每个 token 的可能性分数。）在生成文本时，通过 logits 提取 tokens 的过程是通过一种被称为搜索策略（search strategy）、生成策略（generation strategy）或解码策略（decoding strategy）的启发式方法完成的。</p>
<p>基于 Transformer 的解码器从输入文本序列（通常称为提示语（prompt））生成文本（通常也被称为对输入文本的扩展或补充）基本上包括以下步骤：</p>
<ol>
<li>将模型权重加载到GPU</li>
<li>在CPU上对提示词(prompt)进行分词(tokenizing)，并将token张量传输到GPU。</li>
</ol>
<p>分词步骤示意图:</p>
<p><img src="/2025/02/03/transformer-not/0525b0974fe6495098901a06816ec47d.png" alt="在这里插入图片描述"></p>
<ol>
<li>将分词完成后的提示语输入神经网络，生成扩展的第一个token</li>
</ol>
<p><strong>这一阶段通常被称为启动阶段（initiation phase）。</strong> 在下一篇文章中，我们将看到它也经常被称为预填充阶段（pre-fill phase）。</p>
<ol>
<li>将生成token附加到输入的token序列中，并将其用作生成扩展文本中第二个token的新输入。然后，重复此过程，直到生成了停止序列或达到所配置的最大序列长度。</li>
</ol>
<p><strong>这个由多个步骤组成的阶段通常被称为生成阶段（generation phase）、解码阶段（decoding phase）、自回归阶段（auto-regressive phase），甚至是增量阶段（incremental phase）。</strong></p>
<ol>
<li>将完成的 tokens 从 GPU 获取到 CPU ，并对它们进行 detokenize（译者注：”detokenize“指的是将模型生成的 tokens 序列转换回原始文本或句子的过程。可能包括去除 tokens 之间的空格、添加标点符号、还原缩写等操作，以还原生成文本的自然语言形式。），以获取生成的文本（图5）。</li>
</ol>
<p>论在硬件上如何进行计算，两个阶段之间确实没有区别，因此两个阶段在这方面都没有什么特别之处。这种设置涉及大量冗余计算，因此在许多情况下效率低下。缓解这种情况的一种重要方式是缓存我们不想重新计算的内容。这种优化被称为 KV 缓存，并引入了我一直在暗示的这两个阶段之间的关键差异。</p>
<p><strong>单头注意力</strong> :</p>
<p>假设只处理长度为 t 的单个输入序列,则会有 t 个查询向量、t 个键向量和 t 个值向量。对于每个查询向量，都会生成一个输出向量，输出向量是输入序列中所有值向量的线性组合，每个值向量在线性组合中的权重由对应的注意力分数决定。换句话说，对于每个查询向量，生成的输出向量是通过对输入序列中的值向量进行加权求和而得到的，其中权重由注意力分数确定。对于给定的查询向量，都会与所有的键向量进行点积运算。点积运算的结果表示了查询向量与每个键向量之间的关联度，即它们的<strong>相似性</strong>。这些点积的结果经过适当的处理后，成为了<strong>注意力分数</strong>，用于权衡对应值向量在输出向量中的贡献。这样，我们就能为序列中的每个 token 生成一个包含其他 token 信息的向量表征，也就是说，我们为每个 token 创建了一个上下文表征（contextual representation）。</p>
<p>然而，在自回归解码（auto-regressive decoding）的情境中，我们不能使用所有可能的值向量来构建给定查询向量的输出表征。实际上，在计算与特定 token 相关的查询向量的输出时，我们不能使用序列中后面出现的 token 的值向量。<strong>这种限制是通过一种称为 masking 的技术实现的，实质上是将被禁止的值向量（即被禁止的 token）的注意力分数设置为零。</strong></p>
<h4 id="2-masking-技术的使用导致生成阶段出现冗余计算"><a href="#2-masking-技术的使用导致生成阶段出现冗余计算" class="headerlink" title="2. masking 技术的使用导致生成阶段出现冗余计算"></a>2. masking 技术的使用导致生成阶段出现冗余计算</h4><p>由于 masking 技术的使用，在生成当前 tokens 的输出表征时，仅使用之前已生成 tokens 的信息，而不使用之后生成的 tokens 的信息。<strong>因为之前的 tokens 在各次迭代中都是相同的，所以对于该特定 tokens 的输出表征在随后的所有迭代中也都是相同的，这就意味着存在冗余计算。</strong></p>
<p>在自回归解码步骤的新一次迭代中，使用了“What color is the sky? The sky is ”作为输入序列，在之前的步骤中唯一尚未计算的是输入序列中的最后一个token “is”的表征。</p>
<p>需要的量有:</p>
<ol>
<li>“is”的查询向量。</li>
<li>用于计算注意力分数的“What”，“ color”，“ is”，“ the”，“ sky”，“?”，“The ”，“sky ”和“is ” 的键向量。</li>
<li>用于计算输出的“What”，“ color”，“ is”，“ the”，“ sky”，“?”，“The ”，“sky ”和“is ” 的值向量。</li>
</ol>
<p>至于键（key）和值（value）向量，除了 ”is “之外，它们已经在之前的迭代中为所有 tokens 计算过了。因此，我们可以保存（即缓存）并重复使用先前迭代中的键和值向量（译者注：原文是“query vectors”，可能是作者笔误，此处译者修改为“值向量”）。这种优化简单地被称为 KV 缓存。为“is ”计算输出表征将会变得非常简单：</p>
<ol>
<li>计算“is ”的查询向量、键向量和值向量。</li>
<li>从缓存中获取“What”，“ color”，“ is”，“ the”，“ sky”，“?”，“The ” 和 “sky ”的键和值向量，并将它们与刚刚为“is ”计算的键向量和值向量连接起来。</li>
<li>使用“is ”查询向量和所有键向量计算注意力分数。</li>
<li>使用注意力分数和所有值向量计算“is ”的输出向量。</li>
</ol>
<p><strong>当我们使用 KV 缓存时，模型的实际输入是最后生成的 tokens （而非整个序列）和 KV 缓存。</strong></p>
<p><strong>KV cacge</strong>可以节省多少运算量?假设有一批输入序列，数量为b个，每个序列由N个生成的tokens和t个输入的tokens(总长度为 N+t)组成。对于这些序列的前 t+N-1 个 tokens，计算 KV 值是冗余的，也就是说，在生成步骤的第 N 步，我们可以为每个序列节省 t+N-1 次 KV 计算。如果不重新计算，那么在前 N 个生成步骤中，每个序列总共可以节省 N.t+N.(N-1)/2 次 KV 计算。</p>
<p>通过 KV 缓存节省的运算数量与生成的 tokens 数量的平方成正比。（换句话说，如果生成的 tokens 数量翻倍，通过KV缓存所节省的运算数量将变为原来的四倍。）</p>
<p>KV 缓存是一种妥协，因此并不是免费的午餐：<strong>其实是使用更多的内存消耗和数据传输来换取更少的计算量。</strong></p>
<p>与启动阶段所需的输入相比，强制执行这一缓存策略改变了注意力层在生成阶段的输入。在启动阶段，注意力层会一次性处理整个输入序列，而启用 KV 缓存的生成阶段只需要最后生成的 token 和 KV 缓存作为输入。这种启动阶段和生成阶段之间的新差异不仅仅是概念上的。例如，<strong>与在两个阶段使用相同的 GPU 内核相比，在每个阶段使用特定的 GPU 内核能带来更好的性能。</strong></p>
<h4 id="3-有效控制KV-cache的内存占用，优化推理速度"><a href="#3-有效控制KV-cache的内存占用，优化推理速度" class="headerlink" title="3. 有效控制KV cache的内存占用，优化推理速度"></a>3. 有效控制KV cache的内存占用，优化推理速度</h4><p>多头注意力（MHA）模型的 KV 缓存确实会<strong>消耗大量 GPU 内存</strong>，并且很容易增长到比模型权重还大的规模， KV 缓存大小的控制对于优化大模型的推理至关重要。</p>
<p><strong>KV 缓存阻碍了我们处理或生成超长序列（即长上下文窗口带来的挑战或障碍）和/或处理大 batches ，因此无法最大限度地提高硬件效率。</strong></p>
<p>从这个角度来看，最大化模型处理能力意味着为 KV 缓存留出尽可能多的内存空间，可以通过以下方式实现：</p>
<ul>
<li>减少模型权重的内存占用(权重量化)</li>
<li>较少KV cache 的内存占用，滑动窗口，缓存丢弃，</li>
<li>将模型分片到多个GPU上，以牺牲网络通信为代价(模型并行)或私用其他类型的存储，如CPU内存或者磁盘，从而将多个设备的内存池化。</li>
</ul>
<p>PagedAttention 没有涉及到的另一个可能的优化措施是跨请求重用键值缓存（reusing the key-value cache across requests）。当提示词（prompts）共享某一类共同的前缀时，这种优化就会适用，这种情况通常出现在聊天界面和Agent等多轮用例或使用提示词模板时（图 4）。</p>
<p><img src="/2025/02/03/transformer-not/760c80e82311464dad6e90e345f747e1.png" alt="在这里插入图片描述"></p>
<p>RadixAttention 算法在完成内容生成请求后，并非直接丢弃 KV 缓存，而是将其保留在 GPU 内存中，并向专用数据结构（radix tree）添加一个新条目，将 tokens 序列映射到其 KV 缓存张量。当新请求到达时，调度程序会使用 radix tree 进行前缀匹配。如果有缓存命中，调度程序就会重新使用缓存的 KV 张量来满足请求。</p>
<h4 id="4-模型性能瓶颈分类及优化策略"><a href="#4-模型性能瓶颈分类及优化策略" class="headerlink" title="4. 模型性能瓶颈分类及优化策略"></a>4. 模型性能瓶颈分类及优化策略</h4><p><strong>模型性能瓶颈的4种主要类型:</strong></p>
<ul>
<li>计算能力受限情况。在该情况下，大部分时间（即延迟）都耗费在执行计算操作上。</li>
<li>内存带宽受限情况。在这种情况下，大部分时间都用于在嵌入于处理器芯片上的内存和处理器之间移动数据。</li>
<li>通信受限情况。仅适用于计算和数据分布在多个芯片上的情况。大部分任务处理时间被芯片间的网络数据传输占用。</li>
<li>开销受限情况。是软件导致的瓶颈。</li>
</ul>
<p>通常情况下，不可能所有内核都运行在同一种机制下。因此，<strong>关键在于识别出大部分时间都耗费在哪一种机制上。</strong> 然后，我们应该<strong>优先优化这一主要的性能瓶颈，再找出下一个影响最大的性能瓶颈，如此循环往复。</strong></p>
]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>notes</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文阅读】Characterization of Large Language Model Development in the Datacenter</title>
    <url>/2025/02/03/scheduler_1/</url>
    <content><![CDATA[<p>大语言模型（LLMs）在许多任务中表现出色。然而，要高效利用大规模集群资源开发LLM并非易事，常常伴随着频繁的硬件故障、复杂的并行化策略和资源利用不平衡等诸多挑战。为此，我们针对Acme GPU数据中心在为期六个月的LLM开发工作负载中所累积的跟踪数据，进行了一次深入的特征分析研究。我们特别探讨了LLM与以往深度学习（DL）工作负载之间的差异，研究了资源利用模式，分析了各种任务失败的影响，总结了所遇到的难题，并揭示了优化LLM系统的潜在机会。</p>
<span id="more"></span>
<h3 id="【论文阅读】Characterization-of-Large-Language-Model-Development-in-the-Datacenter"><a href="#【论文阅读】Characterization-of-Large-Language-Model-Development-in-the-Datacenter" class="headerlink" title="【论文阅读】Characterization of Large Language Model Development in the Datacenter"></a>【论文阅读】Characterization of Large Language Model Development in the Datacenter</h3><ul>
<li>出处: NSDI-2024                                             <a href="https://www.usenix.org/system/files/nsdi24-hu.pdf">数据中心中大型语言模型开发的表征</a></li>
<li><a href="https://github.com/InternLM/AcmeTrace">InternLM/AcmeTrace (github.com)</a></li>
</ul>
<h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p>大语言模型（LLMs）在许多任务中表现出色。然而，要高效利用大规模集群资源开发LLM并非易事，常常伴随着频繁的硬件故障、复杂的并行化策略和资源利用不平衡等诸多挑战。为此，我们针对Acme GPU数据中心在为期六个月的LLM开发工作负载中所累积的跟踪数据，进行了一次深入的特征分析研究。</p>
<p>我们特别探讨了LLM与以往深度学习（DL）工作负载之间的差异，研究了资源利用模式，分析了各种任务失败的影响，总结了所遇到的难题，并揭示了优化LLM系统的潜在机会。此外，我们介绍了改进措施：（1）预训练容错，通过LLM参与的故障诊断和自动恢复，增强了容错能力。(2)针对评估任务负载的解耦调度，通过任务分解和调度优化，实现即时的性能反馈。</p>
<h4 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h4><p>近年来，大语言模型（LLMs）引起了学术界和工业界的广泛关注，例如ChatGPT和GitHub Copilot。然而，由于这些模型规模庞大且对数据需求量巨大，训练这些模型需要庞大的计算基础设施，通常需要成千上万的加速器。因此，科技公司和云服务提供商通常会构建大规模的GPU集群来促进LLM的开发，特别是在ChatGPT流行之后。然而，在如此高成本的基础设施上进行高效的LLM开发并非易事。<strong>开发人员常常面临诸多问题和挑战，包括频繁的硬件故障、复杂的并行化策略、不稳定的训练进度、长时间的排队延迟等。</strong></p>
<p>LLM的开发在各个方面都与GPU集群的支持密不可分。<strong>对集群工作负载的全面分析对于理解挑战和发现为LLM量身定制系统设计的机会至关重要</strong>。然而，许多在LLM兴起之前提出的深度学习工作负载分析工作的结论和启示并不适用于LLM开发。这主要是由于LLM具有不同的特性和需求： </p>
<p>1.<strong>范式转变</strong>：深度学习（DL）工作负载通常遵循特定任务的范式，在特定领域的数据上训练模型以解决特定任务（例如翻译）。相比之下，大语言模型（LLMs）采用一种新兴的范式，通过自监督训练在广泛的数据上生成基础模型，然后适应各种下游任务。<strong>这一转变标志着模型开发流程（如预训练、对齐）和工作负载特征与之前的深度学习工作负载存在显著差异。</strong></p>
<p>2.<strong>定制软件栈</strong> ：为了适应LLMs庞大的模型规模，一系列系统实现了先进的技术来优化LLMs的执行。例如，Deepspeed、Megatron和Alpa通过混合并行或状态分片优化器加速训练。而在模型服务方面，Orca和vLLM通过迭代调度或内存管理来提高吞吐量。</p>
<p>3.<strong>统一架构</strong> ：之前的深度学习工作负载通常采用各种模型架构（例如CNN、RNN）来解决不同的任务。相反，LLMs普遍采用Transformer架构，如BERT、GPT-3、LLaMA和PaLM。这种架构上的一致性表明LLM开发流程高度统一，不同数据中心之间具有很高的相似性。</p>
<p><strong>为了解决这一差距，我们对上海人工智能实验室的数据中心Acme的运营经验进行了深入研究。</strong> 该中心拥有两个专门用于LLM开发的集群，Seren和Kalos，<strong>总共配备了4,704个A100 GPU</strong>。我们的分析基于从2023年3月到8月期间收集的日志数据，<strong>包括调度器日志、基础设施监控数据、故障日志和细粒度的性能分析数据</strong> 。我们的主要发现和识别的挑战总结如下：</p>
<p><strong>1.更短的作业时长和不公平的排队延迟</strong> ：与传统的认知相反，<strong>我们数据中心的工作负载平均作业时长比以往的DL工作负载缩短了2.7到12.8倍</strong> 。这主要是由于大量短期任务（如评估任务）的存在。在作业排队延迟方面，我们的发现也与以往的DL工作负载不同，往的DL工作负载中规模较大的作业通常会有更长的等待时间。我们观察到，尽管评估任务是短期且小规模的，但它们却有最长的排队延迟。这一差异源于<strong>为了减少预训练作业的排队延迟，系统保留了大部分资源用于预训练作业，而评估作业则被安排在优先级较低的位置，使用有限的备用资源。</strong></p>
<p><strong>2.资源使用不均衡</strong>：这种不平衡体现在两个方面。首先，在工作负载分布方面，<strong>预训练作业仅占总作业量的3.2%，但消耗了Kalos集群94.0%的计算资源</strong>（即GPU时间）。相反，<strong>评估作业虽然占所有作业的92.9%，却仅使用了0.8%的资源。</strong>其次，从基础设施利用率来看，我们发现包括CPU、主机内存和网络在内的相关资源经常处于未充分利用状态。相比之下，作为主要资源的GPU显示出很高的利用率。Kalos集群（LLMs负载）中<strong>GPU内存和GPU利用率的中位数分别高达75%（60GB）和99%</strong>，而在PAI集群（传统DL负载）中，这两个数值分别仅为10%和4%。<strong>这些观察结果证实了LLMs在计算和内存方面的高需求，也表明基于GPU共享的技术可能不适用于LLMs。</strong></p>
<p><strong>3.评估任务负载GPU比较空闲</strong>：我们的<strong>评估任务负载分析显示，各个阶段的GPU资源利用率严重不足</strong>。例如，HumanEval评估作业有29.5%的时间用于模型加载和数据预处理，另有19.0%的时间用于合成程序正确性评估。因此，<strong>只有一半的时间用于GPU推理</strong>，导致评估试验的排队延迟时间较长。</p>
<p><strong>4.频繁的作业失败</strong>：我们发现，<strong>LLM工作负载的各种错误主要发生在作业开始阶段，导致快速终止</strong>。然而，基础设施故障在长期预训练作业中很常见，严重影响训练效率。因此，及时诊断和恢复这些故障对于提高训练效率至关重要。</p>
<p>根据我们的特征研究，我们发现了LLM开发过程中遇到的几个挑战，如<strong>训练进度不稳定、远程存储瓶颈和模型性能反馈延迟</strong>。为了解决这些问题，我们结合运营经验，构建了两个集成到LLM框架中的系统，以提高开发的稳健性和效率。</p>
<p><strong>首先，为了缓解频繁的故障问题，我们建立了一个预训练容错系统</strong>。它包含三个关键设计：(1) 通过异步检查点实现频繁的模型保存，(2) 结合启发式规则和LLM识别各种故障的根本原因，(3) 使用综合检测工具包定位故障节点，并从适当保存的检查点自动重新开始训练。该系统使检查点速度提高了3.6至58.7倍，并显著减少了人工干预。</p>
<p><strong>其次，我们开发了一个用于评估任务的解耦调度系统</strong>，为开发人员提供及时的模型质量反馈。它不仅通过解耦模型检索解决了远程模型加载争用问题，还通过解耦指标计算过程最小化了GPU空闲时间。该系统还利用数据集的先验知识和灵活性在所有GPU上平衡工作负载。实验表明，它可以将评估时间缩短最多达1.8倍。</p>
<h4 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h4><h4 id="2-1-LLM-Development-Pipeline"><a href="#2-1-LLM-Development-Pipeline" class="headerlink" title="2.1 LLM Development Pipeline"></a>2.1 LLM Development Pipeline</h4><p>与传统的深度学习模型不同，大语言模型（LLM）采用一种新兴的自监督学习范式，在广泛的数据上进行训练，并进一步后训练适应各种下游任务。LLM的开发由于其庞大的<strong>模型规模（包含数十亿参数）和大量的训练数据</strong>，通常需要庞大的计算基础设施。图1展示了LLM开发的完整流程，包括从零开始到服务上线的五个不同阶段（蓝色方块），蓝色箭头表示流程顺序。灰色的循环箭头表示预训练阶段可以进行定期的对齐和评估，以评估中间模型并及时调整配置。各个阶段具体如下：</p>
<p> <img src="/2025/02/03/scheduler_1/4f4dd76580e7490ab18120e1cba78cdb.png" alt="请添加图片描述"></p>
<p><strong>数据准备</strong>：初始阶段包括数据的收集和预处理，可分为两个部分：（1）预训练数据，由从公共或私人来源获取的大量未标注语料组成，通过净化和去重等过程进行筛选；（2）对齐数据，由用于将模型对齐到特定任务的小规模高质量标注语料组成，这些数据通常通过昂贵的人力注释或标注获取。此外，所有数据必须进行分词处理，以确保与模型输入的兼容性。</p>
<p><strong>预训练</strong>：这一阶段涉及在大规模精选数据上进行自监督训练，占据了整个开发流程中大部分资源。要在大规模上高效训练LLMs，需要采用多种系统创新技术，如状态分片优化器、数据并行、流水线并行和张量并行等精细的模型放置方法。</p>
<p><strong>对齐</strong>：这一阶段旨在使LLMs能够适应用户意图，以应对各种下游任务。常用的两种对齐范式是：（1）提示工程，通过指定提示（即输入）而不修改模型参数。例如，在文本摘要中，在输入文章后添加提示“TL;DR”可以提高模型性能；（2）微调，通过在特定任务的数据集上更新模型参数来提高在特定领域的性能。此外，从人类反馈中进行的强化学习（RLHF）进一步增强了对齐效果，并且像LoRA这样的参数高效技术被提出以降低微调的成本。</p>
<p><strong>评估</strong>：鉴于LLMs的广泛应用场景，仅依靠训练损失等单一指标来评估模型质量可能不准确。需要考虑的因素有很多，如准确性、公平性和毒性。因此，有必要采用多样的标准并在多个任务上衡量性能。此外，在预训练阶段进行定期评估，以提供关于模型质量的及时反馈也是至关重要的</p>
<p><strong>部署</strong>：为了满足LLM应用的严格成本和延迟要求，已经开发了多种先进技术来实现高效的模型服务，包括量化、蒸馏、CUDA内核优化、模型并行和内存管理等。</p>
<h5 id="2-2-Acme-Overview"><a href="#2-2-Acme-Overview" class="headerlink" title="2.2 Acme Overview"></a>2.2 Acme Overview</h5><p>Acme 是我们的私人 GPU 数据中心，支持研究人员和工程师在多个领域开发深度学习模型。<strong>在本文中，我们专注于分析两个专门用于开发大语言模型（LLM）的集群：Seren 和 Kalos。</strong>我们收集并分析了这两个集群中的所有作业。需要注意的是，Acme 中还有其他集群用于不同领域，如自动驾驶和科学研究中的人工智能，但这些集群因与本文无关而被排除在外。<br><img src="/2025/02/03/scheduler_1/aca92d57dd464d749030dfa64f2cca73.png" alt="在这里插入图片描述"></p>
<p><strong>集群架构：</strong> 表1总结了这两个同质 LLM 集群的配置。Seren 和 Kalos 分别有 2,288 和 2,416 个 GPU。每个节点配备 8× NVIDIA A100-SXM 80GB GPU 和 2× Intel Xeon Platinum 8358P CPU（共128个线程）。GPU 通过 NVLink 和 NVSwitch 互连，节点间通信通过 NVIDIA Mellanox 200Gbps HDR InfiniBand 实现。相比 Seren，Kalos 是一个相对较新的集群，网络配置有所改进。Kalos 每个节点具有更大的主机内存（2TB），配备了四个专用于应用通信的 InfiniBand HCA 以及一个专用于存储的额外 HCA。</p>
<p>此外，分布式存储系统对工作负载性能也至关重要。Acme 采用了全 NVMe 共享并行文件系统以实现快速数据访问和存储。随着时间的推移，我们的资源调度系统已经演变为支持多种集群环境。具体来说，Seren 和 Kalos 上的调度器分别基于 Slurm 和 Kubernetes 构建。为了为大规模预训练作业提供资源保证，我们的调度器实现了资源隔离和配额预留。此外，它还结合了尽力而为的作业机制以提高利用率。</p>
<p><strong>LLM 工作负载：</strong> 我们开发了一系列 LLM，参数规模从 70 亿到超过 1230 亿不等。这些模型都采用基于 Transformer 的解码器架构，类似于 GPT 和 LLaMA 系列。Acme 涵盖了前述的一般 LLM 开发流程中的任务（见 §2.1）。需要注意的是，Acme 不涉及任何服务任务，因为我们的 LLM 被部署在专门用于服务的独立集群上。</p>
<p><strong>软件栈：</strong> 为了支持在数千个 GPU 上训练亿级参数的模型，我们构建了一个名为 InternEvo 的系统，集成了多种系统优化技术，如 FlashAttention、3D 并行、零冗余优化、混合精度训练、选择性激活重计算以及细粒度通信重叠。此外，该系统还支持模型微调和评估等附加任务。</p>
<h5 id="2-3-Traces-from-Acme"><a href="#2-3-Traces-from-Acme" class="headerlink" title="2.3 Traces from Acme"></a>2.3 Traces from Acme</h5><p>表2比较了Acme的规格和跟踪信息与由Microsoft、SenseTime和Alibaba进行的先前跟踪分析工作。与Acme专注于LLM开发不同，这些数据中心包含来自各个领域的通用深度学习工作负载。例如，Helios [38] 包含4个专用于计算机视觉和强化学习模型训练的集群，而PAI [97] 则包括用于训练和服务作业的多种服务器。</p>
<p> <img src="/2025/02/03/scheduler_1/b62d91ae4ef045a8a11d4e55e5db2511.png" alt="在这里插入图片描述"></p>
<p><strong>跟踪来源：</strong> 我们的特征化研究基于从Acme的两个LLM集群收集的跟踪数据。这些跟踪数据跨越2023年3月至8月的6个月时间。Seren集群包含了368K个CPU作业和664K个GPU作业，而Kalos集群的作业跟踪数据包含了42K个CPU作业和20K个GPU作业。此外，我们总结了我们研究中使用的跟踪数据来源：</p>
<ol>
<li><strong>作业日志</strong>：我们从调度数据库中收集作业日志，包括每个作业的执行时间（提交、开始和结束时间）、最终状态（完成、取消、失败）、请求的资源（CPU、GPU、内存）、工作目录以及其他相关数据。</li>
<li><strong>硬件监控数据</strong>：这些数据来源广泛，来自多个维度长期采集。我们从Prometheus数据库获取CPU、内存和网络使用数据，从NVIDIA DCGM获取与GPU相关的指标，以及从IPMI获取与电源相关的数据。这些数据的采样间隔设置为15秒。</li>
<li><strong>运行时日志</strong>：为了进行精确的作业失败分析，我们捕获LLM框架在作业执行期间的标准输出和标准错误日志。</li>
<li><strong>性能分析数据</strong>：针对一部分具有代表性的作业，我们使用DCGM等工具进行深入的性能分析。这些跟踪维度的协同作用使我们能够全面了解数据中心中LLM作业的特性。</li>
</ol>
<h4 id="3-Datacenter-Characterization"><a href="#3-Datacenter-Characterization" class="headerlink" title="3. Datacenter Characterization"></a>3. Datacenter Characterization</h4><p>本节中，我们对Acme进行了深入分析，包括比较LLM与之前DL工作负载之间的工作负载分布（§3.1），调查不同LLM工作负载类型（§3.2），探索资源利用模式（§3.3），以及评估环境影响（§3.4）。</p>
<h5 id="3-1-LLM与先前DL工作负载的对比"><a href="#3-1-LLM与先前DL工作负载的对比" class="headerlink" title="3.1 LLM与先前DL工作负载的对比"></a>3.1 LLM与先前DL工作负载的对比</h5><p>  <img src="/2025/02/03/scheduler_1/31cf84e94ceb421ea1100eef22dffe13.png" alt="在这里插入图片描述"></p>
<p><strong>更短的作业持续时间：</strong> 如图2(a)所示，与普遍认为的LLM相关作业通常长时间运行的刻板印象相反，我们发现我们集群中的工作负载（蓝色和橙色线条）展示出较短的GPU作业持续时间（即作业运行时间，不包括排队延迟），与先前作业追踪中观察到的DL工作负载（虚线）相比。具体来说，Seren和Kalos的中位数作业持续时间为2分钟，比其他集群的中位数作业持续时间短了1.7∼7.2倍。此外，显然最近的追踪显示了较短的作业持续时间分布。特别是，在考虑到Philly集群的平均作业持续时间（2017年收集）时，它比Helios（2020年）和PAI（2020年）长了2.7∼3.8倍，比Acme（2023年）长了12.8倍。<strong>为了解释这一观察结果，我们概述了三个潜在因素：</strong>（1）硬件升级。GPU和网络的迭代带来了显著的效率提升。（2）资源丰富。用户通常请求更多资源（如表2所示），在Seren平均5.7个GPU和Kalos平均26.8个GPU。这可以显著加速训练过程。（3）广泛的关联工作负载：LLM开发流水线涉及许多小规模的关联作业，例如评估。我们将在§3.2中深入探讨这一点。（4）高未完成率：大约40%的作业失败，完成的作业只消耗GPU资源的20∼30%。这突显了需要一个容错系统的紧迫性。有关更多详细信息，请参见图17和附录A.1。</p>
<p><strong>两极化的GPU利用率：</strong>图2(b)展示了各个集群中整体的GPU利用率分布。显然，我们两个集群中的GPU利用表现出两极化的模式，主要集中在两个明显的状态：0%和100%。这种两极化主要源于我们集群中的<strong>工作负载具有相似的模型架构</strong>，即基于transformer的LLMs。相比之下，Philly和PAI则涵盖了更广泛的利用率范围。此外，当比较中位数GPU利用率时，Seren和Kalos分别表现出97%和99%的显著高值，而Philly和PAI的观察值分别为48%和4%。这一观察结果符合LLMs计算密集的普遍理解。它还表示，基于GPU共享的调度技术[40,98,99,106]可能不适合LLM的开发。需要注意的是，“GPU利用率”有时可能是一个较弱的利用率指标[8, 94]。我们将在§3.3中提供更精确的利用率分析。</p>
<p> <img src="/2025/02/03/scheduler_1/9ea3fdddc4574ec998dda10c73f6acd4.png" alt="在这里插入图片描述"></p>
<p><strong>高度倾斜的工作负载分布：</strong> 我们进一步研究了与作业数量相关的GPU需求的累积分布函数（图3(a)）和GPU时间（图3(b)）。在作业数量方面，所有集群都显示出类似的模式，即大多数作业为单GPU作业，少于7%的作业请求超过8个GPU。然而，当考虑GPU时间时，单GPU作业仅占我们两个集群不到2%的资源，而在PAI中却占据了超过68%的GPU时间。与之形成鲜明对比的是，Kalos集群中大规模作业（≥256个GPU）主导了GPU时间，占据了超过96%的资源。</p>
<p>这种更为陡峭的分布给集群调度器的设计带来了重大挑战。<strong>大部分资源被分配给少数预训练作业，可能导致头部阻塞问题，从而造成严重的排队延迟。</strong>现有的DL集群调度器通常依赖于抢占机制，但是考虑到恢复的巨大开销，这些机制并不适用于LLM工作负载。这凸显了迫切需要为LLM集群量身定制调度系统的重要性，需要考虑整个流程的工作负载特征。</p>
<h5 id="3-2-Workload-Categories"><a href="#3-2-Workload-Categories" class="headerlink" title="3.2 Workload Categories"></a>3.2 Workload Categories</h5><p>为了深入了解LLM开发流程（§2.1）中不同工作负载的特征，我们根据它们的生产分工和元数据（如作业名称），进一步将作业分类为特定类型。</p>
<p> <img src="/2025/02/03/scheduler_1/c3861282fe424f81a3c1194420dc8ddd.png" alt="在这里插入图片描述"></p>
<p><strong>作业数和资源利用的不相关性：</strong> 图4展示了各种工作负载类型的作业数量和GPU时间分布，其中只有Seren包含SFT和MLLM工作负载。此外，MLLM作业包含自己的开发流程（如预训练），并采用较小规模的模型进行探索。我们的分析主要集中在LLM作业上。显而易见，评估作业在总作业计数中占据了大多数，然而它们在资源消耗方面相对较小（Kalos中为0.8%）。相反，预训练作业仅占总作业计数的0.9%和3.2%，但在Seren和Kalos中分别消耗了69.5%和94.0%的总GPU时间</p>
<p>  <img src="/2025/02/03/scheduler_1/ed4dfdb7f82c48c7882fee37ab2bcc67.png" alt="在这里插入图片描述"></p>
<p><strong>作业类型与GPU需求相关性：</strong> 我们进一步在图5中描述了各种工作负载类型的GPU需求分布。每个箱子由第一四分位数和第三四分位数框定，箱内黑线表示中位数值。两个“须”在1.5倍四分位距（IQR）处定义。与通常需要少于4个GPU的评估作业相比，预训练作业通常需要超过100个GPU。这一观察部分解释了为什么图4(d)中Kalos中的评估作业消耗的资源很少。此外，我们注意到调试作业的GPU请求范围很广，这与评估作业通常需要各种类型任务的事实相一致。<br><img src="/2025/02/03/scheduler_1/28d5061e5b9b4bdba07e2f2938329257.png" alt="在这里插入图片描述"></p>
<p><strong>相同的时间分布：</strong> 图6展示了不同工作负载的作业持续时间和排队延迟分布。就作业持续时间而言，尽管预训练作业持续时间最长，但它们在中位数上超过其他工作负载一个数量级，并且在两个集群中不到5%的作业持续时间超过1天。这可以归因于预训练过程中频繁的失败，这将在§5中进一步探讨。至于作业排队延迟，与之前的报告[38, 45, 97]建议的大规模作业经历更长等待时间不同，我们观察到<strong>尽管评估作业的GPU需求最低，作业持续时间最短，但它们的排队延迟最长。</strong>这种差异是因为大多数资源都被保留用于预训练作业，以最小化它们的排队延迟。评估作业通常作为低优先级的批量同时提交，利用有限的备用资源。</p>
<h5 id="3-3-Infrastructure"><a href="#3-3-Infrastructure" class="headerlink" title="3.3 Infrastructure"></a>3.3 Infrastructure</h5><p>除了负载特性分析外，我们进一步对基础设施利用情况进行了全面分析。</p>
<p> <img src="/2025/02/03/scheduler_1/bcc0520bd99a486ab69f4f8e4c0e4d51.png" alt="在这里插入图片描述"></p>
<p><strong>更高的GPU利用率：</strong> 如图7(a, b)所示，考虑到GPU在LLM开发中的关键作用，我们从DCGM [7]中收集了细粒度的性能计数器指标，包括SM活动（PROF_SM_ACTIVE）张量核心活动（PROF_PIPE_TENSOR_ACTIVE）和GPU内存占用（DEV_FB_USED）。与PAI [97]中大部分GPU内存未充分利用（内存利用率低于25%）不同，我们在Kalos中的观察表明，<strong>50%的GPU占用了超过75%的GPU内存（60 GB）</strong>。此外，我们观察到两个集群的中位数SM活动约为40%，是PAI报告的20%的两倍。这些发现与LLM的内存密集型和计算密集型特性一致。</p>
<p><strong>未充分利用的关联资源：</strong>我们还深入研究了与LLM开发密切相关的CPU、主机内存和网络方面。在图7(b)中，我们比较了主机端和GPU端的内存占用情况。显而易见，CPU内存利用率保持在50%以下。需要注意的是，Kalos相比Seren拥有两倍的内存容量（2TB）（见表1）。这显示了<strong>CPU内存的显著未充分利用</strong>。附录A.2提供了更详细的分析。尽管GPU内存卸载技术[80,81]提高了CPU内存利用率并缓解了GPU内存限制，但由于有限的PCIe带宽，它也会影响训练吞吐量。因此，我们不采用卸载机制。此外，由于高CPU与GPU比例（每GPU 16个CPU），CPU通常处于低利用状态，如图7(c)所示。此外，在图7(d)中，我们测量了Seren中IB的网络发送和接收带宽。两条线重叠良好，IB在LLM执行期间用于对称通信。我们观察到NIC在超过60%的时间内保持空闲状态，并且活动带宽很少超过IB提供的最大带宽的25%。</p>
<h5 id="3-4-Environemntal-Impact"><a href="#3-4-Environemntal-Impact" class="headerlink" title="3.4 Environemntal Impact"></a>3.4 Environemntal Impact</h5><p> <img src="/2025/02/03/scheduler_1/1de8768282ca4e0ea101fcf1f1c921c1.png" alt="在这里插入图片描述"></p>
<p>图8(a)展示了GPU能耗的分布情况。我们观察到约30%的GPU处于空闲状态，但仍需消耗60W的电力。此外，由于计算需求强烈，我们发现在Seren和Kalos中，有22.1%和12.5%的GPU消耗超过400W（TDP），甚至有些达到600W，可能引发一些亚稳态问题[41]。图8(b)呈现了Seren中所有GPU服务器以及额外6台仅CPU服务器的能耗分布情况。我们发现<strong>GPU服务器的平均能耗是CPU服务器的5倍</strong>。</p>
<h4 id="4-Workload-Profilling"><a href="#4-Workload-Profilling" class="headerlink" title="4.Workload Profilling"></a>4.Workload Profilling</h4><p>在本节中，我们对代表性任务的资源利用进行了细粒度分析。我们关注预训练和评估任务，因为它们是最消耗资源或数量密集的工作负载。</p>
<h5 id="4-1-Pretraining-Workload"><a href="#4-1-Pretraining-Workload" class="headerlink" title="4.1 Pretraining Workload"></a>4.1 Pretraining Workload</h5><p> <img src="/2025/02/03/scheduler_1/26f8299ab7a34a24a99eed47ae1be892.png" alt="在这里插入图片描述"></p>
<p>如前所述，预训练LLMs需要大量的计算资源。为了提高训练效率，我们的预训练框架InternEvo [25] 在系统设计上进行了持续的优化和迭代。如图10所示，InternEvo的初始版本（早期作业采用的版本）分别如下：(a) 主要采用类似于MegatronLM [68] 的3D并行，(b) 使用分层的ZeRO机制 [25]，实现模型状态的选择性冗余分片。举例说明，我们对一个拥有1230亿参数的LLM在2048个GPU上的性能进行了详细的分析。我们也在附录A.4中提供了1024个GPU的分析结果。(a) 3D并行方法，我们采用了管道并行ism配置= 4，张量并行ism= 8。我们对第一个管道秩的第一个GPU进行了性能分析。(b) 分层ZeRO方法，我们将参数分片限制为每组64个GPU，并启用了重新计算。我们以1毫秒间隔采集了像DCGM指标这样的GPU性能计数器。</p>
<p><strong>GPU SM利用率</strong>：图10展示了相同LLM在不同训练策略下的GPU SM利用率。这两个版本保持了相同的全局批处理大小，并根据各自的配置进行了优化。显然，相比于InternEvo V1，InternEvo V2呈现出更优秀的峰值SM利用率，并减少了空闲时期，实现了约16%的加速。<strong>3D并行的相对低利用率主要是由于混合并行引入的通信对关键路径的影响，例如管道并行中的气泡。</strong>需要注意的是，不同的节点内和节点间通信硬件设置可能导致不同的最佳配置。</p>
<p> <img src="/2025/02/03/scheduler_1/a64d3c829d184d0caa34b4f89d64ba7d.png" alt="在这里插入图片描述"></p>
<p><strong>GPU内存占用：</strong> 对于一个包含Ψ个参数的模型，在使用Adam [48]优化器进行主流混合精度训练时，参数、梯度和优化器状态的内存占用分别为2Ψ、2Ψ和12Ψ。为了降低内存成本，ZeRO [79]有效地将这些元素的冗余内存分片到全局GPU工作进程中。图11展示了使用Pytorch内存快照工具 [11]捕获的GPU随时间的实际内存使用情况。上部动态部分代表激活和梯度，而下部静态部分代表参数和优化器状态所占用的内存。需要注意的是，图中仅展示已分配的内存，而保留的内存未呈现。<strong>我们的分析显示，与分层ZeRO相比，3D并行中激活内存的需求显著更高。这一观察强调了高效的激活内存管理作为提升3D并行批处理大小和吞吐量的关键因素</strong>。</p>
<p> <img src="/2025/02/03/scheduler_1/225aed6035df48e7adfff6e131e7b4f3.png" alt="在这里插入图片描述"></p>
<p><strong>激活大小的不平衡：</strong> 在使用管道并行时，每个等级需要持有不同数量的激活，因为在各个管道等级上，待进行反向计算的微批次数量各不相同。图12展示了这种不平衡问题在不同管道等级上的情况。<strong>这表明我们应该采用专门的分区机制来解决管道并行中不同等级之间的内存使用不平衡问题，以实现更高的效率，比如重新计算激活。</strong></p>
<h5 id="4-2-Evilatopm-Workload"><a href="#4-2-Evilatopm-Workload" class="headerlink" title="4.2 Evilatopm Workload"></a>4.2 Evilatopm Workload</h5><p><strong>在LLM预训练过程中，定期评估生成的检查点对指导LLM预训练的演进至关重要</strong> 。因此，LLM评估作业占据了大多数作业，每个作业在不同的LLM基准数据集上执行指标计算。我们分析整个评估工作流程，并结合细粒度的资源使用信息定量地展示两个即将到来的资源利用问题。我们还将在第6.2节讨论相应的解决方案</p>
<p> <img src="/2025/02/03/scheduler_1/e744bd43d4324fc19ac1422361be39bd.png" alt="在这里插入图片描述"></p>
<p><strong>模型加载和数据预处理高开销：</strong> 在评估作业的初始化阶段，为每个任务加载模型检查点至关重要。此外，数据预处理阶段，特别是标记化，构成了显著的时间开销。这些因素导致了分配的GPU资源在相对较长时间内的低利用率。如图13所示，在实际GPU推理之前，评估任务消耗超过1分钟，占评估持续时间的29.5%。这种开销可能会随着模型或数据集规模的增大而增加。为了解决预处理开销，一个有效的策略是缓存标记化数据。此外，评估作业具有灵活性，可以将多个评估任务（数据集）合并为单个作业。这种合并可以有效减少评估过程中模型加载阶段的相对时间开销。</p>
<p><strong>指标计算高开销：</strong>评估过程经常涉及复杂且耗时的指标计算。例如，需要在编码数据集（如HumanEval [24]和MBPP [17]）上执行合成程序正确性测试。此外，还会调用OpenAI GPT-4 API评估模型对话的表现（例如Chatbot Arena [112]）。这些过程可能需要长达30分钟的时间，在此期间GPU保持空闲状态。因此，我们可以观察到GPU使用的不同阶段，包括需要GPU进行推理和生成的阶段，以及不需要GPU进行指标计算和验证的阶段。以HumanEval基准测试为例，如图13所示，GPU在最后42秒处于空闲状态，浪费了总GPU时间的约19.0%。</p>
<h4 id="5-Failure-Analysis"><a href="#5-Failure-Analysis" class="headerlink" title="5 Failure Analysis"></a>5 Failure Analysis</h4><p>本节我们将基于我们两个集群的运行日志和硬件监控数据，对作业失败进行全面分析。在Kalos集群中，我们收集了32,500个任务的日志，其中包括31,293个（96.3%）推理任务，647个（2.0%）预训练任务和调试任务（1.7%）。在Seren集群中，我们仅收集了675个预训练任务的日志。此外，针对预训练任务，我们提取了所有相关信息和元数据，包括实际训练步骤、冷启动开销、恢复时间戳等。我们希望通过我们的分析为未来LLM开发中的容错研究提供深刻见解。</p>
<h5 id="5-1-故障分类"><a href="#5-1-故障分类" class="headerlink" title="5.1 故障分类"></a>5.1 故障分类</h5><p>我们采用了一种故障诊断系统，结合基于规则和LLM技术的方法，从运行时日志中提取错误信息。我们在第6.1节详细解释了这个系统。此外，为确保准确识别故障类型和根本原因，进行了手动检查和修正。表3总结了Acme中常见故障的发生频率和重启时间。基本上，它们可以分为以下三类。请注意，这些分类可能有重叠，对特定错误类型分类的主要标准是其最频繁发生的情况</p>
<p> <img src="/2025/02/03/scheduler_1/1f41f9b81f1347aaa27ec152d5cab774.png" alt="在这里插入图片描述"></p>
<ul>
<li><strong>基础设施：</strong> 基础设施相关的故障源于底层计算平台或远程存储的问题。这些故障主要发生在作业执行过程的中途阶段，尤其是在预训练任务中。它们由于复杂且耗时的恢复过程严重影响训练进度。</li>
<li><strong>框架：</strong> 多种类型的运行时错误，如RuntimeError、ValueError和AttributeError，可能与张量操作、形状、数据类型或意外行为有关。它们通常出现在作业的初始阶段，并且通常通过修复配置来解决。</li>
<li><strong>脚本：</strong> 脚本错误通常源于编程错误或用户疏忽。它们占据了大多数的故障，并且通常通过修订代码来解决.</li>
</ul>
<h5 id="5-2-Failure-Characterization"><a href="#5-2-Failure-Characterization" class="headerlink" title="5.2 Failure Characterization"></a>5.2 Failure Characterization</h5><p>我们从分析中得出了几个关键观察：<strong>基础设施故障造成的影响最为严重</strong>。如表3所示，由于基础设施问题导致的作业失败往往使用大量GPU资源（GPU需求），并且重新启动需要相当大的工作量（重启时间）。它们占用了超过82%的GPU时间（GPU时间），但失败作业数量仅占11%（数量）。这些作业大多是长期的预训练任务，可能多次遭遇硬件故障，如GPU问题（例如CUDAError、ECCError）、NVLink问题（NVLinkError）以及网络系统问题（NCCLRemoteError、S3StorageError）。需要注意的是，NodeFailure表示由于不明确的硬件问题而导致的未分类错误。解决这些基础设施故障需要精细的诊断工作，以准确定位问题的根源，通常需要对有缺陷的硬件进行维护或更换，这导致了显著的重启成本。</p>
<p><strong>高温引起的故障：</strong> 另一个显著的观察是，在Kalos训练7B模型往往会导致GPU过热，可能引发NVLinkError或ECCError。这种现象主要是由于高度优化的通信成本，导致GPU空闲率异常低。我们观察到，在训练这些模型时，整个集群服务器室内的温度大约上升了5°C。此外，我们发现大多数这类故障发生在2023年7月，这是有记录以来最炎热的月份[63]。这种异常的气候可能是这些故障的潜在原因，这与微软最近的研究结果相一致[100]。我们在附录A.5中提供了关于GPU温度的更详细数据。随后，我们的团队增强了集群的冷却能力，显著减少了这类故障的发生频率。</p>
<p><strong>许多故障由辅助服务引起：</strong> 在我们的预训练框架中，我们连接到外部组件或服务进行指标报告、日志记录、监控和警报。这些辅助服务容易受到网络不稳定性的影响，可能导致超时或故障，从而减缓或中断训练过程。大量的ConnectionError和NetworkError事件源自这些辅助服务。</p>
<p><strong>评估作业很少遇到错误：</strong> 在Kalos中，仅有6.7%的评估任务遇到错误，特别地，并未记录GPU或NVLink的故障情况。低错误率可能归因于评估任务持续时间较短，对GPU和NVLink连接的压力较小。因此，这降低了硬件和操作故障在评估作业中更频繁发生的可能性<br><img src="/2025/02/03/scheduler_1/f138dab1049d4e2f912c929c509fa958.png" alt="在这里插入图片描述"></p>
<h5 id="5-3-Failure-Characterization"><a href="#5-3-Failure-Characterization" class="headerlink" title="5.3 Failure Characterization"></a>5.3 Failure Characterization</h5><p>我们在以下三种情况下会重新启动作业：(1) 当作业内部发生错误时，(2) 当训练指标异常，如损失急剧上升时，以及(3) 当训练过程陷入停滞时。所谓的“损失急剧上升”指的是先前正常下降的损失突然增加，并且在一定时间内无法恢复。重新启动时，作业会回到上一个检查点，导致训练进度的损失。由于现有的LLM框架缺乏自动恢复支持，开发人员通常需要手动重新启动中断的训练作业。开发人员经常需要轮流值班，以确保预训练模型及时完成。</p>
<p>如图14所示，在我们手动处理所有故障的早期阶段（3月至4月），我们选择了两个预训练作业。我们从两个集群大规模模型训练过程的日志中提取信息，包括每次提交的运行持续时间、开始和结束时间，以及训练的初始和最终迭代次数。104B模型是在框架还在开发初期时的早期尝试。因此，加载之前的模型检查点导致了整体训练过程中的重大损失。相比之下，一个月后123B模型的训练中，我们改进了框架并采用了更小的检查点保存间隔。此外，我们增加了一个优雅终止作业的功能，在结束作业之前允许保存当前的训练结果。显然，123B模型的训练过程更为稳定，由于回滚而造成的损失更少。然而，这一进展是有代价的，因为不同时间中断的作业必须迅速重新启动。</p>
<h4 id="6-Deployed-LLM-Systems"><a href="#6-Deployed-LLM-Systems" class="headerlink" title="6. Deployed LLM Systems"></a>6. Deployed LLM Systems</h4><p>正如前文所强调的，LLM的开发过程面临重重障碍，但也揭示了克服这些问题的可行策略。本节将分两个阶段介绍我们的工作：(1) 预训练阶段：通过LLM相关的故障诊断和自动恢复来增强容错能力。(2) 评估阶段：通过任务分解实现及时的性能响应。</p>
<h5 id="6-1-Fault-tolerant-Pretraining"><a href="#6-1-Fault-tolerant-Pretraining" class="headerlink" title="6.1 Fault-tolerant Pretraining"></a>6.1 Fault-tolerant Pretraining</h5><p><strong>动机：</strong> 在LLM预训练过程中，由于涉及大量GPU和训练过程的长时间持续性，故障不可避免且经常发生 [15, 44, 88, 96]。这些故障会严重阻碍训练进度，导致资源利用效率低下（§5）。因此，为了最小化基础设施的停机时间，通常采用轮值值班的方式手动处理故障。这给工程师和研究人员带来了重大负担，正如Meta OPT [110]和BigScience BLOOM [1]团队所抱怨的那样。我们的团队也面临这个问题。为了减轻这一负担并提升硬件效率，我们开发了一个系统，自动检测故障根本原因并促进恢复。</p>
<p><strong>系统设计：</strong> 我们的容错系统无缝集成到LLM预训练框架中，包括三个关键模块：(1) 检查点：增加频繁的模型保存，以最小化训练进度的损失；(2) 诊断：使用启发式规则和LLM结合，准确识别不同故障的根本原因；(3) 恢复：采用全面的检测工具包，确定故障节点，并从适当保存的检查点自动重新启动训练。我们将详细讨论这些模块。</p>
<p><strong>1.异步检查点</strong>：频繁的检查点有效减少了由意外故障引起的浪费时间 [32]。然而，由于LLM可以生成TB级别的模型状态（即跨所有GPU的总模型状态），保存检查点本身可能会引入显著的开销，导致训练时间减慢高达43% [60]。为了解决这个问题，我们采用了异步检查点策略 [64, 69]，有效地将检查点过程与训练过程分离。我们的观察表明，CPU内存（参见图7 (b)）能够容纳多个检查点。通过利用这一点，我们可以将模型状态存储在内存中，并利用单独的线程定期将这些状态保存到远程持久存储。这种简单的策略显著减少了检查点的开销。</p>
<p> <img src="/2025/02/03/scheduler_1/1638a6ca0b3e4e91b107242e268430dc.png" alt="在这里插入图片描述"></p>
<p><strong>2.故障诊断.：</strong> 正如我们在§5讨论的那样，故障可能源自多种复杂因素，包括用户脚本或框架的错误，以及处于高压条件下的硬件问题。确定故障是否可恢复对于自动恢复至关重要。一种常见的方法是利用启发式规则的组合，对故障作业的日志进行过滤和正则表达式匹配 [23, 45, 52, 53, 66]。然而，由于错误日志的广泛多样性和复杂性，这种方法经常表现不准确。许多情况下可能没有特定的错误声明，而是多种错误同时存在。例如，一个作业可能因包含NCCLTimeoutError、CUDAError和多种RuntimeError的消息而失败，而根本原因可能是CUDAError。尝试用特定规则集匹配每种错误场景可能变得不切实际。</p>
<p><strong>➤Real-time Log Compression</strong>：由预训练作业生成的大量日志文件主要包含训练指标记录，其大小可达数百MB。为了加速诊断并满足LLM的上下文长度限制，首先进行日志压缩。系统持续更新一组正则表达式集合，称为过滤规则。这些规则高效地移除常规的日志输出，如初始化信息、训练指标记录、框架输出和调试信息。系统的关键组成部分是基于LLM的日志代理，负责分析实时生成的日志片段，并识别符合固定模式的行。通过这种方式，基于LLM的日志代理动态地编写正则表达式，更新过滤规则，有效地减小日志文件的大小。此外，日志代理将识别的错误消息转发给后续的诊断模块。</p>
<p>此外，我们采用自一致性方法来确保日志代理结果的稳健性和这些结果的格式化 [95]。这包括对每个日志片段进行多次处理，并让另一个LLM对日志代理的多个结果进行投票，通过正则表达式确保匹配的准确性。随着时间的推移，过滤规则对当前任务变得更加全面，使日志过滤过程更加高效。此外，系统可以利用任务的元数据识别重复或相似的任务，直接应用现有的过滤规则进行日志过滤，从而避免冗余工作。在大型模型集群环境中，这一特性尤其有益，因为较少的租户和任务重新提交是常见的。</p>
<p>➤ <strong>LLM-assisted Automated Diagnosis</strong>：日志代理高效压缩运行时日志，隔离像CUDA错误或运行时异常等关键错误日志。尽管日志在抵达该模块时已经压缩，但错误日志可能仍然很长。我们采用两步方法来解决这个问题。首先，将错误日志与在过去多失败作业错误的诊断定义的规则进行比较。如果预定义的规则无法诊断问题，压缩日志将通过嵌入模型进行向量化，并存储在向量存储库中，作为检索库。然后，故障代理介入。它利用查询引擎 [55] 搜索向量存储库。通过这种搜索，故障代理可以识别反映作业中断根本原因的日志行，提取错误类型，并指示错误是否源自用户错误或基础设施故障，为恢复过程提供线索。此外，它还为用户或运维团队生成缓解建议。</p>
<p>故障代理还对故障诊断系统的持续学习做出贡献。对于每一个新的故障，一旦诊断并解决完成，故障代理会编写相应的正则表达式，并将其添加到基于规则的诊断模块中。这个过程是迭代的，确保故障诊断系统不断进化，能够更加熟练地诊断并提出故障的缓解方法。为了实现更加稳健的性能，目前我们使用GPT-4 [2] 进行诊断，计划逐步过渡到我们的LLM模型。</p>
<p><strong>3. 快速故障检测与恢复：</strong> 根据故障诊断结果，如果属于某种基础设施故障，我们会进行相应的检测测试以识别问题节点。例如，为了迅速解决频繁出现的NVLinkError问题，我们采用了两轮NCCL测试[5]方法。首先，我们将所有节点分成多个两节点组，并在每对节点间执行allgather任务。如果服务器总数为奇数，我们将一个组设为三节点。如果某个组中的allgather任务失败，则该组中的节点可能是故障节点。在第二轮中，我们将可能的故障节点与正常节点配对，形成新的组。每组中的节点继续执行allgather任务，从而识别出故障节点并隔离它们。另一方面，如果故障归因于损失的突然增加（即‘loss spike’[27, 110]），这会自动由我们的预训练框架触发，我们选择恢复到较早的健康检查点并跳过后续的数据批次。此方法有效维护了模型质量。</p>
<p><strong>系统性能：</strong> 我们的异步检查点策略大幅减少了检查点开销，因为检查点过程不会阻塞训练过程。7B和123B模型大小的检查点时间和开销比例分别减少了3.6到58.7倍（间隔=30分钟）。请注意，持久化存储所需的时间不包含在异步检查点测量中。此外，我们的故障诊断系统显著减少了约90%的人工干预，从而减轻了开发人员的负担。请注意，由于系统的某些组件仍在改进中，这不是严格的评估。</p>
<p><strong>动机：</strong> 仅凭单一指标（如训练损失）来评估LLM的质量可能无法提供准确的评估[58]。因此，必须结合多种标准并在一系列任务中评估性能[22]。我们的LLM框架在预训练阶段的每个检查点都会进行定期评估。这使开发人员能够跟踪模型训练的进展，并识别出最佳的模型检查点。我们旨在提供快速反馈，以便及时调整。然而，如图6所示，由于资源有限和大量试验的同时提交，评估任务经历了最长的排队延迟。尽管面临这些挑战，我们还是发现了几种加速评估过程的机会。</p>
<p><strong>系统设计：</strong> 我们开发了一个试验协调器，以协调集群调度器和LLM框架的操作。该设计包括以下三种关键技术，旨在提高评估过程的效率。</p>
<p> <img src="/2025/02/03/scheduler_1/4789cf2ae27f4ad1a4ef2d28d7f165d7.png" alt="在这里插入图片描述"></p>
<p><strong>1. 解耦远程模型加载</strong>：由于LLM的规模庞大，从远程存储检索和加载它们可能是一个漫长的过程。此外，同时执行大量评估任务（约60个数据集）会因争用增加而使加载过程更加复杂。如图16（左）所示，在Seren中并行评估试验时，单个节点上单GPU试验的数量从1增加到8时，模型加载速度显著下降，这是由于存储NIC的带宽限制（25Gb/s）。另一方面，当试验数量在8到256 GPU之间时，加载速度趋于稳定。这个观察启发了我们采用一种战略方法。我们将模型加载过程与评估过程分离，而不是将每个评估数据集作为一个单独的试验提交，如图16（右）所示。具体来说，试验协调器首先从集群调度器获取可用节点列表，然后为每个节点生成一系列前置任务。这些任务将模型从远程存储加载到本地共享内存。接着，协调器将评估任务提交给调度器，评估任务通过高带宽PCIe加载模型。该方法有效地利用了空闲的主机内存。在评估完成后，协调器清理文件。</p>
<p><strong>2. 解耦指标计算</strong>：如图13所示，评估过程通常涉及复杂且耗时的指标计算。例如，在编码数据集如HumanEval [24] 和MBPP [17]上，必须执行合成程序正确性测试。为了解决这一问题，我们将指标计算过程与评估试验分离开来。当模型推理在GPU上完成后，其输出会迅速保存到文件中，从而终止推理任务。由于输出通常是基于文本的，体积较小，这个文件转储过程非常迅速。然后，我们生成CPU任务来进行指标计算。这种方法有效地减少了GPU的空闲时间，加快了评估过程。</p>
<p><strong>3. 基于优先级的弹性调度：</strong> 除了解耦方法外，我们注意到，对于每个评估数据集的近似试验运行时间，我们已有相当可靠的先验知识。此外，这些数据集是灵活的，我们可以将多个数据集批量合并到一个试验中，以规避模型加载。我们还可以拆分大型数据集并解耦指标计算。因此，试验协调器能够通过分解最大化GPU的占用率，利用先验信息平衡每个GPU的工作负载，并在排序的作业队列中采用轮循分配策略。此外，我们优先处理作业队列中CPU指标计算时间较长的评估试验，以更好地重叠其计算过程。此方法不仅增强了工作负载平衡，还最小化了试验切换的开销。</p>
<p><strong>系统性能：</strong> 我们对试验协调器进行了代表性测试，使用了一个典型的7B规模LLM评估任务，涉及在63个数据集上的工作负载评估。我们分别在两种不同条件下测量了完成所有评估试验所需的总时间：单节点（代表资源有限）和四节点（代表资源相对充足）。结果显示，试验协调器能够分别将总时间减少1.3倍和1.8倍。</p>
<p><strong>7 Discussion</strong></p>
<p><strong>范围限制：</strong> 尽管我们尽最大努力分析了Acme的工作负载，但我们无法覆盖所有类型的工作负载。具体限制包括：</p>
<ol>
<li>我们的分析集中在模型服务之前的开发过程，而Acme不包含任何服务作业（即部署阶段的工作负载）。</li>
<li>我们的分析主要集中在GPU作业，对CPU作业的关注较少。</li>
<li>我们主要描述了基于Transformer的、仅包含解码器架构的模型（如GPT-3和LLaMA 2）。对于新型的模型架构，我们在附录A.6中简单描述了专家混合（MoE）模型。其他模型架构，如多模态LLM，不在我们的分析范围内。</li>
</ol>
<p><strong>持续系统改进：</strong>随着大模型的快速发展，本文所述的系统可能无法满足未来工作负载的需求。对此，我们正在积极优化我们的系统，以适应更先进的训练工作负载，包括长序列预训练、MoE预训练和高效的RLHF。此外，我们正在升级基础设施，特别关注网络接口控制器（NIC），并扩展计算集群，以促进更大规模的预训练。我们还在探索一些有前景的方向，例如通过使用Hydro进行超参数优化来提高LLM的质量，以及为新兴模型架构（如Diffusion和Mamba）提供高效的系统支持。</p>
<h4 id="8-Conclusion"><a href="#8-Conclusion" class="headerlink" title="8.Conclusion"></a>8.Conclusion</h4><p>总之，我们分析了数据中心Acme中LLM的工作负载和资源利用情况，揭示了LLM开发的独特特点和挑战，例如资源低效和故障影响。我们还发现了一些为LLM系统优化的潜在机会，并介绍了我们在预训练和评估工作负载方面的努力。我们相信，这些经验和见解具有广泛的适用性，能够为后续研究带来显著益处。</p>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
        <category>调度</category>
      </categories>
      <tags>
        <tag>scheduler</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu20.04安装conda和pytorch</title>
    <url>/2025/02/03/ubuntu-conda-torch/</url>
    <content><![CDATA[<p>在Ubuntu 20.04上安装Anaconda 3 ，创建虚拟环境并安装pytorch。并指出一些常见的问题并给出解决办法。主要问题有，下载Aanconda后无法激活环境。pytorch版本和C湖大版本不匹配。</p>
<span id="more"></span>
<h3 id="Ubuntu20-04安装Anaconda3和pytorch"><a href="#Ubuntu20-04安装Anaconda3和pytorch" class="headerlink" title="Ubuntu20.04安装Anaconda3和pytorch"></a><center>Ubuntu20.04安装Anaconda3和pytorch</center></h3><h5 id="一-查看系统架构"><a href="#一-查看系统架构" class="headerlink" title="一. 查看系统架构"></a>一. 查看系统架构</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">lsb_release -a</span><br><span class="line">dpkg --print-architecture</span><br><span class="line">arch</span><br></pre></td></tr></table></figure>
<p> <img src="/2025/02/03/ubuntu-conda-torch/1be2c3d87307620a6e5d20c9564532e2.png" alt="在这里插入图片描述"></p>
<p><strong>x86_64与amd64</strong><br>由于32位系统x86架构的种种限制，包括速度，性能等方面，Intel开始向64位架构发展，那么有2选择：</p>
<p>向下兼容x86<br>完全重新设计指令集，不兼容x86<br>结果AMD领先，比Intel率先制造出了商用的兼容x86的CPU，AMD称之为AMD64，抢了64位PC的第一桶金，得到了用户的认同。而Intel选择了设计一种不兼容x86的全新64为指令集，称之为IA-64，但是比amd晚了一步，而且IA-64也挺惨淡的，因为是全新设计的CPU，没有编译器，也不支持windows（微软把intel给忽悠了，承诺了会出安腾版windows server版，但是迟迟拿不出东西）。。。后来不得不在时机落后的情况下也开始支持AMD64的指令集，但是换了个名字，叫x86_64，表示是x86指令集的64扩展。也就是说实际上，x86_64,x64,AMD64基本上是同一个东西，我们现在用的intel/amd的桌面级CPU基本上都是x86_64。</p>
<h4 id="二-下载安装文件"><a href="#二-下载安装文件" class="headerlink" title="二. 下载安装文件"></a>二. 下载安装文件</h4><p>注意，如果不是 x86_64，需要去镜像看对应的版本   <a href="https://mirrors.bfsu.edu.cn/anaconda/archive/">Anaconda wget 地址</a><br><figure class="highlight text"><table><tr><td class="code"><pre><span class="line">wget https://mirrors.bfsu.edu.cn/anaconda/archive/Anaconda3-2023.09-0-Linux-x86_64.sh --no-check-certificate</span><br></pre></td></tr></table></figure></p>
<p><img src="/2025/02/03/ubuntu-conda-torch/fef14fd779621f099e9c72100b42ba6b.png" alt="在这里插入图片描述"></p>
<h4 id="三-安装"><a href="#三-安装" class="headerlink" title="三.安装"></a>三.安装</h4><figure class="highlight text"><table><tr><td class="code"><pre><span class="line">bash Anaconda3-2021.11-Linux-x86_64.sh</span><br></pre></td></tr></table></figure>
<p>显示协议，按 下键，输入yes</p>
<p> <img src="/2025/02/03/ubuntu-conda-torch/06d4af7986356a72534ebb129bf6bfbb.png" alt="在这里插入图片描述"></p>
<p>选择安装位置，一般为 /home/用户名/anconda3</p>
<h5 id="四-修改环境变量"><a href="#四-修改环境变量" class="headerlink" title="四.修改环境变量"></a>四.修改环境变量</h5><p>安装后输入 conda</p>
<p>可能会显示 conda: command not found</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo vim ~/.bashrc</span><br></pre></td></tr></table></figure>
<p><img src="/2025/02/03/ubuntu-conda-torch/797876cfe2007624e1846013de74cabb.png" alt="在这里插入图片描述"></p>
<p>选择E</p>
<p>按<code>i</code></p>
<p>最后一行加上</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export PATH=$PATH:/home/用户名/anaconda3/bin</span><br></pre></td></tr></table></figure>
<p>按 Esc 输入<code>:wq</code></p>
<p>输入 <code>conda info --env</code>显示</p>
<p><img src="/2025/02/03/ubuntu-conda-torch/64cdb450f94e32e926291af6d293a5ed.png" alt="在这里插入图片描述"></p>
<p>说明安装成功</p>
<h4 id="五-安装torch"><a href="#五-安装torch" class="headerlink" title="五.安装torch"></a>五.安装torch</h4><p>创建新环境，</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda create -n torchTest</span><br></pre></td></tr></table></figure>
<p>激活环境</p>
<p><code>conda activate testTorch</code></p>
<p>可能会报错</p>
<p> <img src="/2025/02/03/ubuntu-conda-torch/fa1946e65eaebd9433449b9e21cdf3ea.png" alt="在这里插入图片描述"></p>
<p>重新运行conda.sh</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">source &#123;conda路径&#125;/anaconda3/etc/profile.d/conda.sh</span><br></pre></td></tr></table></figure>
<p>{conda路径}这里为 /home/用户名/</p>
<p><strong>查看cuda版本</strong><br><code>nivdia-smi</code></p>
<p><img src="/2025/02/03/ubuntu-conda-torch/de389bcf7a78397a8431947759156e84.png" alt="在这里插入图片描述"></p>
<p><strong>解释：CUDA版本问题</strong></p>
<p>一般查看有两个方式</p>
<p><code>nvidia-smi</code> 和 <code>nvcc -V</code> 两种命令区别是：</p>
<p><code>nvidia-smi</code> 是查看电脑支持的最高版CUDA</p>
<p><code>nvcc -V</code> 查看的是你下载的CUDA版本</p>
<p>如果你已经下载了CUDA，安装torch时就要跟<code>nvcc -V</code> 一致，如果没有，可以直接使用torch的命令下载 cuda toolkit 等，就可以了，</p>
<p>根据 <code>nvidia-smi</code>版本从pytorch 官网找命令 <a href="https://pytorch.org/">PyTorch</a></p>
<p><img src="/2025/02/03/ubuntu-conda-torch/d1457f631c813bedcf4922cb09c6a52d.png" alt="在这里插入图片描述"></p>
<p>过去的版本可以在这里找 <a href="https://pytorch.org/get-started/previous-versions/">Previous PyTorch Versions | PyTorch</a></p>
<p>比如我的版本是cu114但官网没有cu114 对应的命令，就选择cu113</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda install pytorch==1.12.1 torchvision==0.13.1 torchaudio==0.12.1 cudatoolkit=11</span><br></pre></td></tr></table></figure>
<p>最后检验<br><img src="/2025/02/03/ubuntu-conda-torch/52a29b31c5387c804be094123aa45d6c.png" alt="在这里插入图片描述"></p>
]]></content>
      <categories>
        <category>配置</category>
      </categories>
      <tags>
        <tag>configure</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu安装Docker</title>
    <url>/2025/02/05/ubuntu-docker/</url>
    <content><![CDATA[<p>在Ubuntu上安装Docker，启动并验证第一个Docker容器</p>
<span id="more"></span>
<h4 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h4><p>ubuntu 20.0.4<br><img src="/2025/02/05/ubuntu-docker/bd2b53474ad7946019850359700b7eae.png" alt="在这里插入图片描述"></p>
<h4 id="1-配置Docker的apt源"><a href="#1-配置Docker的apt源" class="headerlink" title="1. 配置Docker的apt源"></a>1. 配置Docker的apt源</h4><p>（1）安装包，允许apt命令HTTPS访问Docker源：<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo apt update</span><br><span class="line">sudo apt install apt-transport-https ca-certificates curl gnupg-agent software-properties-common</span><br></pre></td></tr></table></figure></p>
<p>（2）添加Docker官方的GPG key<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -</span><br></pre></td></tr></table></figure><br>（3）将 Docker的源添加到 /etc/apt/sources.list<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable&quot;</span><br></pre></td></tr></table></figure><br>（4）安装Docker<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo apt update</span><br><span class="line">sudo apt install docker-ce</span><br></pre></td></tr></table></figure></p>
<h4 id="运行第一个docker"><a href="#运行第一个docker" class="headerlink" title="运行第一个docker"></a>运行第一个docker</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker run -d -p 80:80 httpd</span><br></pre></td></tr></table></figure>
<p>显示如下：<br><img src="/2025/02/05/ubuntu-docker/3424fbe44af842fd7c31d34e5b00a7c9.png" alt="在这里插入图片描述"><br>主要执行两步</p>
<ul>
<li>从Docker Hub 下载httpd镜像，镜像中已经装好了 Apache HTTP Server.</li>
<li>启动 httpd 容器，并将容器的80端口映射到host的80端口<h4 id="验证容器"><a href="#验证容器" class="headerlink" title="验证容器"></a>验证容器</h4>在本地浏览器输入<code>http://[your ubuntu host IP]</code> 显示如下页面，则Docker启动成功。<br><img src="/2025/02/05/ubuntu-docker/dcc3beef1bd75a2916ad2fe8b2de188b.png" alt="在这里插入图片描述"></li>
</ul>
]]></content>
      <categories>
        <category>配置</category>
      </categories>
      <tags>
        <tag>configure</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>ubuntu创建新用户，添加用户权限，删除用户</title>
    <url>/2025/02/03/ubuntu-opts/</url>
    <content><![CDATA[<p>ubuntu创建新用户，添加用户权限，删除用户</p>
<span id="more"></span>
<p><strong>进入root用户</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">su root</span><br></pre></td></tr></table></figure>
<p><strong>创建新用户</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo useradd -r -m -s /bin/bash abcd #abcd是用户名称</span><br></pre></td></tr></table></figure>
<p><strong>设置密码</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo passwd abcd</span><br></pre></td></tr></table></figure>
<p>来设置新用户的密码。</p>
<p>其中参数的意义如下：<br>-r：建立系统账号<br>-m：自动建立用户的登入目录<br>-s：指定用户登入后所使用的shell</p>
<p><strong>修改用户权限</strong></p>
<p>修改 /etc/sudoers 文件，该文件为只读文件，所以先修改为w权限，再改写，再改回r权限。</p>
<p>修改文件权限</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo chmod +w /etc/sudoers</span><br></pre></td></tr></table></figure>
<p>改写文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim /etc/sudoers</span><br></pre></td></tr></table></figure>
<p>在文件最后一行加入下列语句<br>有关 vim  的操作可参考 ：<a href="https://blog.csdn.net/weixin_46091520/article/details/138215969">ubuntu 下 vim 的使用-CSDN博客</a></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">用户名(这里是abcd)  ALL=(ALL:ALL) ALL</span><br></pre></td></tr></table></figure>
<p>改回文件权限：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo chmod -w /etc/sudoers</span><br></pre></td></tr></table></figure>
<p><strong>删除用户</strong></p>
<p>分三步：</p>
<ol>
<li>删除用户</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo userdel 用户名(这里是abcd)</span><br></pre></td></tr></table></figure>
<ol>
<li>删除用户目录</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo rm -rf /home/用户名(这里是abcd)</span><br></pre></td></tr></table></figure>
<ol>
<li>删除用户权限相关配置：删除或者注释掉/etc/sudoers中关于要删除用户的配置，否则无法再次创建同名用户</li>
</ol>
]]></content>
      <categories>
        <category>配置</category>
      </categories>
      <tags>
        <tag>configure</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>ubuntu_git</title>
    <url>/2025/02/03/ubuntu-git/</url>
    <content><![CDATA[<p>ubuntu 下git常用指令，后续遇到新的指令会持续更新</p>
<span id="more"></span>
<h4 id="ubuntu-下git常用指令【持续更新中】"><a href="#ubuntu-下git常用指令【持续更新中】" class="headerlink" title="ubuntu 下git常用指令【持续更新中】"></a><center>ubuntu 下git常用指令【持续更新中】</center></h4><h5 id="1-下载"><a href="#1-下载" class="headerlink" title="1.下载"></a>1.下载</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo apt install git		</span><br></pre></td></tr></table></figure>
<h5 id="2-查看版本"><a href="#2-查看版本" class="headerlink" title="2. 查看版本"></a>2. 查看版本</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git --version</span><br></pre></td></tr></table></figure>
<h5 id="3-登录git账号"><a href="#3-登录git账号" class="headerlink" title="3. 登录git账号"></a>3. 登录git账号</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git config --global user.email &quot;you@example.com&quot;</span><br><span class="line">git config --global user.name &quot;Your Name&quot;</span><br></pre></td></tr></table></figure>
<h5 id="4-生成密钥对"><a href="#4-生成密钥对" class="headerlink" title="4.生成密钥对"></a>4.生成密钥对</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa -C &quot;your_email@youremail.com&quot;</span><br></pre></td></tr></table></figure>
<p>复制公钥</p>
<h5 id="5-复制公钥到github"><a href="#5-复制公钥到github" class="headerlink" title="5. 复制公钥到github"></a>5. 复制公钥到github</h5><p><img src="/2025/02/03/ubuntu-git/91800e522e0020909564e868640d31a5.png" alt="请添加图片描述"></p>
]]></content>
      <categories>
        <category>配置</category>
      </categories>
      <tags>
        <tag>configure</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>win 11 将wsl转为wsl2并安装Ubuntu20.04到指定位置，挂载Ubuntu文件夹</title>
    <url>/2025/02/03/ubuntu-wsl/</url>
    <content><![CDATA[<p>win11 使用wsl安装Ubuntu20.04子系统到指定位置，并将wsl升级为wsl2。同时将Ubuntu文件系统生成挂载盘，方便使用</p>
<span id="more"></span>
<h4 id="一-安装wsl-2"><a href="#一-安装wsl-2" class="headerlink" title="一.安装wsl 2"></a>一.安装wsl 2</h4><p>在win11安装ubuntu20.04子系统，并安装在指定位置。</p>
<p>选择控制面板-&gt;程序-&gt;启用或关闭Windows功能，勾选适用于Linux的Windows子系统和hyper-V</p>
<p>重启<br><img src="/2025/02/03/ubuntu-wsl/73b21204d0248f557174560437e0ec85.png" alt="在这里插入图片描述"><br><img src="/2025/02/03/ubuntu-wsl/8144ccac5e9164eba47164aaef156492.png" alt="在这里插入图片描述"><br>hyper-V是为了启动虚拟化。<br><img src="/2025/02/03/ubuntu-wsl/af2d862be551b860f1db1f8cb4a7bdec.png" alt="在这里插入图片描述"><br>启动命令行，输入 <code>wsl</code><br><img src="/2025/02/03/ubuntu-wsl/2033957dcb8c6bb8f4ec134f5dc3f427.png" alt="在这里插入图片描述"></p>
<h5 id="二-安装ubuntu20-04发行版"><a href="#二-安装ubuntu20-04发行版" class="headerlink" title="二.安装ubuntu20.04发行版"></a>二.安装ubuntu20.04发行版</h5><p>从 Microft store 下载并打开，如果报错，检查是否有打开 上面的两个功能并重启。</p>
<p>设置用户名和密码<br><img src="/2025/02/03/ubuntu-wsl/51e64a8c2f50112fbf1370cb0fef4998.png" alt="在这里插入图片描述"><br>至此，win11下Ubuntu20.04子系统安装完毕，打开命令行，输入wsl即可进入子系统‘<br><img src="/2025/02/03/ubuntu-wsl/47935627adeeb9f493356a08aa4fb8ff.png" alt="在这里插入图片描述"></p>
<h5 id="三-更改-wsl-为-wsl2"><a href="#三-更改-wsl-为-wsl2" class="headerlink" title="三.更改 wsl 为 wsl2"></a>三.更改 wsl 为 wsl2</h5><p>参考链接：先下载 内核更新包，在设置wsl2wei 默认版本</p>
<p><a href="https://learn.microsoft.com/zh-cn/windows/wsl/install-manual#step-4---download-the-linux-kernel-update-package">旧版 WSL 的手动安装步骤 | Microsoft Learn</a></p>
<h5 id="四-将发行版移动到其他盘"><a href="#四-将发行版移动到其他盘" class="headerlink" title="四,将发行版移动到其他盘"></a>四,将发行版移动到其他盘</h5><p>win+i 搜索设置，查看安装的应用，将Ubuntu 移动到想要的盘<br><img src="/2025/02/03/ubuntu-wsl/c7f5e784bd1d2c6fe1f04cc40108a5b5.png" alt="在这里插入图片描述"><br>命令行输入, 查看发行版</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wsl --list</span><br></pre></td></tr></table></figure>
<p>然后打包</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wsl --export Ubuntu-22.04 E:\Ubuntu-22.04.tar</span><br></pre></td></tr></table></figure>
<p> 格式为：wsl —export \<Distribution name> \<FileName><br>\<Distribution name> 为目标 Linux 发行版的名称，我安装的为： Ubuntu-22.04<br>\<FileName> 为导出的文件名，这里我导出到 E:\Ubuntu-22.04.tar</FileName></Distribution></FileName></Distribution></p>
<p>然后注销现有发行版</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wsl --unregister Ubuntu-22.04</span><br></pre></td></tr></table></figure>
<p>查看已安装版本,已经没有了。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wsl -l -v</span><br></pre></td></tr></table></figure>
<p> <img src="/2025/02/03/ubuntu-wsl/971b1476f68c061ae072f49e8a68c0db.png" alt="在这里插入图片描述"><br>将Ubuntu 20.04导入到新位置</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wsl --import Ubuntu-22.04 E:\wsl E:\Ubuntu-22.04.tar</span><br></pre></td></tr></table></figure>
<p>格式为：wsl —import  \<Distribution name> \<InstallLocation> \<FileName><br>\<Distribution name> 为目标 Linux 发行版的名称，我安装的为： Ubuntu-22.04<br>\<InstallLocation> 为要安装到的新位置，这里我安装到：E:\wsl<br>\<FileName> 为要导入的文件名，这里导入刚才导出的 E:\Ubuntu-22.04.tar<br>更改默认用户</FileName></InstallLocation></Distribution></FileName></InstallLocation></Distribution></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Ubuntu2204 config --default-user zhangsan</span><br></pre></td></tr></table></figure>
<p>格式为：\<DistributionName> config —default-user \<Username><br>\<Distribution name> 为目标 Linux 发行版的名称，我安装的为： Ubuntu-22.04，命令里要写为：Ubuntu2204<br>\<Username> 为 WSL 发行版中存在的用户名，在设置 Linux 用户名和密码时我创建的用户为：zhangsan ，所以命令里的 \<Username> 这里就为：zhangsan</Username></Username></Distribution></Username></DistributionName></p>
<h5 id="五-进入Ubuntu-文件目录"><a href="#五-进入Ubuntu-文件目录" class="headerlink" title="五.进入Ubuntu 文件目录"></a>五.进入Ubuntu 文件目录</h5><p>微软为我们提供了一个默认的变量可以直接指向WSL的目录，<code>wsl$</code> 你可以在运行(win+R)或资源管理器的路径里直接输入<code>\\wsl$</code>进入Ubuntu的目录<br><img src="/2025/02/03/ubuntu-wsl/12d88b4501ea387b1077a69d2e09e180.png" alt="在这里插入图片描述"><br><img src="/2025/02/03/ubuntu-wsl/7bdf965427e50e043e45032a289366c8.png" alt="在这里插入图片描述"></p>
]]></content>
      <categories>
        <category>配置</category>
      </categories>
      <tags>
        <tag>configure</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>vscode 使用code runner 运行代码输出乱码原因及解决办法</title>
    <url>/2025/02/03/vscode-codeRunner/</url>
    <content><![CDATA[<p>解决vscode使用code runner 乱码的问题，主要保持代码的编码方式和终端编码方式一致。可以通过设置终端默认编码方式或代码文件默认编码方式解决。</p>
<span id="more"></span>
<h3 id="vscode-使用code-runner-运行代码输出乱码"><a href="#vscode-使用code-runner-运行代码输出乱码" class="headerlink" title="vscode 使用code runner 运行代码输出乱码"></a><center>vscode 使用code runner 运行代码输出乱码</center></h3><h4 id="问题所在："><a href="#问题所在：" class="headerlink" title="问题所在："></a>问题所在：</h4><p><strong>代码文件使用的编码格式</strong>和<strong>终端使用的编码格式不一致</strong>，查看代码文件右下角，会显示代码文件的编码格式。</p>
<p>测试代码如下：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    cout&lt;&lt;<span class="string">&quot;你好！&quot;</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>此时输出乱码：</p>
<p> <img src="/2025/02/03/vscode-codeRunner/484d36f4669b9f38c499083d5143ca98.png" alt="请添加图片描述"></p>
<p>查看右下角的代码文件编码格式，显示为 <code>utf-8</code><br><img src="/2025/02/03/vscode-codeRunner/cbe03b81b7f64a96dc12c1d89acaf026.png" alt="请添加图片描述"></p>
<p>终端输入<code>chcp</code>, 显示当前终端编码为 <code>936</code>，即为<code>gbk</code>。<code>utf-8</code>为<code>65001</code>。<br><img src="/2025/02/03/vscode-codeRunner/94714c7fe347e014f04d4f257293dcf5.png" alt="请添加图片描述"></p>
<h4 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h4><p>可以点击文件右下角，通过编码保存，选择<code>gbk</code>保存；也可以在终端输入<code>chcp 65001</code>将终端编码改为<code>utf-8</code>。这里选择第二种，输入<code>chcp 65001</code> 则代码和终端编码都为<code>utf-8</code>。</p>
<p>重新运行，正常输出：<br> <img src="/2025/02/03/vscode-codeRunner/2241e55ddc5800fb6022e892c3eb9553.png" alt="请添加图片描述"></p>
<p>但这种方式只是暂时的，如果终端重新打开，则又得重新改。可以在<code>settings.json</code>中配置终端信息。</p>
<p>点击扩张-&gt;设置-&gt;在settings.json中设置。</p>
<p><img src="/2025/02/03/vscode-codeRunner/217f6db2953a80c63dbacb8a046b4f54.png" alt="请添加图片描述"></p>
<p>在<code>&#123;&#125;</code>中输入以下代码</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 设置终端的参数，编码格式等</span></span><br><span class="line"><span class="string">&quot;terminal.integrated.profiles.windows&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;Command Prompt&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;path&quot;</span>: <span class="string">&quot;C:\\Windows\\System32\\cmd.exe&quot;</span>,</span><br><span class="line">        <span class="string">&quot;args&quot;</span>: [<span class="string">&quot;-NoExit&quot;</span>, <span class="string">&quot;/K&quot;</span>, <span class="string">&quot;chcp 65001&quot;</span>]</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">&quot;PowerShell&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;source&quot;</span>: <span class="string">&quot;PowerShell&quot;</span>,</span><br><span class="line">        <span class="string">&quot;args&quot;</span>: [<span class="string">&quot;-NoExit&quot;</span>, <span class="string">&quot;/C&quot;</span>, <span class="string">&quot;chcp 65001&quot;</span>]</span><br><span class="line">    &#125;</span><br><span class="line">&#125;,</span><br><span class="line"><span class="string">&quot;terminal.integrated.defaultProfile.windows&quot;</span>: <span class="string">&quot;PowerShell&quot;</span>,   <span class="comment">//使用PowerShell 或者Command Prompt作为终端</span></span><br></pre></td></tr></table></figure>
<p><img src="/2025/02/03/vscode-codeRunner/2f59fab9df71d38d711c86059862222c.png" alt="请添加图片描述"></p>
<p>注意前后的<code>,</code>否则会影响其他设置。现在，每次打开终端都会设置编码为UTF-8</p>
<p><img src="/2025/02/03/vscode-codeRunner/4af924d125c0be037bd0137122579235.png" alt="请添加图片描述"></p>
]]></content>
  </entry>
  <entry>
    <title>模型显存占用计算以及Zero优化器</title>
    <url>/2025/03/30/zero-optimizer/</url>
    <content><![CDATA[<p>介绍如何根据模型的参数量计算显存占用，介绍了分布式训练常见的通信原语，包括Reduce, Gather, Broadcast,Scatter,ReduceScatter,AllGather,AllReduce等，以及介绍Zero优化器三个级别各自的执行过程。</p>
<span id="more"></span>
<p>零冗余优化器(ZeRO)通过对三个模型状态(优化器状态、梯度和模型参数)进行划分而不是复制他们，消除了数据并行进程中的内存冗余。该方法与传统的数据并行相比，内存效率得到极大的提高，而计算粒度和通信效率得到了保留。</p>
<p>目前训练超大参数规模的模型仍然有许多显存方面的问题，目前常见的解决方案是数据并行(Data Parallelisms ，DP) 和模型并行(Model Parallelisms, MP)。基础的DP方法并不能降低每个GPU上的内存占用，因为他会拷贝模型。为了解决这个问题，有诸如流水线并行(Pipeline Parallelism, PP),MP或者借助CPU内存的方法，但这种方法都是以牺牲某些关键性能(如，内存，计算，通信开销)为代价。目前最有前景的方法是MP，即垂直地切分模型，将网络和参数切分到多个设备，需要每层之间进行大量的通信。</p>
<h4 id="一-优化前"><a href="#一-优化前" class="headerlink" title="一. 优化前"></a>一. 优化前</h4><p>ZeRO(Zero Redundancy Optimizer)的开发便是用于解决DP和MP的问题。在理解ZeRO的原理之前，需要深入了解DP、MP的问题：</p>
<p>● 内存为什么占用这么大</p>
<p>● 是参数所占的真实空间，还是存在冗余</p>
<p>● 可以如何优化</p>
<p>训练期间大部分的内存被模型状态(Model States)所消耗，包括优化器参数(Optimizer states), 梯度(Gradients) 和模型参数(Model Parameters)所消耗。除此之外，残余状态(Residual States)消耗了剩余的内存，包括前向传播是得到的Activateions,即时计算通信的临时穿冲区还有没有被妥善管理的内存碎片。将内存分为两部分: Model States和Residual进行讨论：</p>
<p>● Model States</p>
<p>混合精度训练和Adam优化器基本已经是训练语言模型的标配, 其中模型参数，模型梯度都是FP16，如果优化器是Adam，则还有FP32的一阶动量(momentum)和二阶动量(variance)，混合精度训练同时存在FP16和FP32两种格式的数值，其中模型参数，模型梯度都是FP16，Adam的一阶动量和二阶动量是FP32,此外还有FP32的模型参数备份(backup)。(为什么要有FP32的模型参数备份？因为FP16累加误差会积累)，假设模型参数规模为$\Phi$, 那么Model States的内存总开销为:</p>
<p>$Model States = Param + Gradient + Adam(momentum, variance) + backup = 2\Phi + 2\Phi + 2\times 4 \times \Phi + 4 \times \Phi = 16\Phi$</p>
<p>如果是1.5B的模型，$Model States=16\times 1.5B=24GB$, Model States远大于模型存储模型参数的显存，所以存在巨大优化空间。</p>
<p>● Residual States Memory</p>
<p>首先是激活值 Activations，Activation是在网络前向传播过程中每层神经元的计算结果，在反向传播时将结合梯度和这些激活值更新模型参数，所以需要保留这些Activation，常用的优化有：</p>
<p><strong>Activation ChaeckPointing</strong>, 即只保存特定点的Activation，并在反向传播时重新计算没有保存的Activations, 这种方法代价是额外的计算时间，具体选择哪些点保存，一般是保存计算代价高但内存占用小的Activation，具体策略可以是固定或者启发式的。</p>
<p><strong>Temporary buffers，</strong>一些操作如Gradient AllReduce或Gradient norm computation在使用高性能库时会尝试将所有参数融合到一个单一缓冲区以提高吞吐量，并且这些缓冲区的大小大多数情况下跟模型大小有关。（<strong>为啥这会影响内存开销？</strong>因为，当模型大小很大时，由于某些操作/高性能库的原因，会等待装填或者分配一个非常大的融合缓冲区去执行操作，这虽然会带来带宽和效率上的优势，但是有时却成为了内存瓶颈）</p>
<p>Memory Fragmentation, 如Activation原来存储在连续的空间，使用Activation checkpoint技术后释放一些激活值，这时其他激活值仍未释放，就会造成碎片化。</p>
<h4 id="二-分布式训练常见的通信原语"><a href="#二-分布式训练常见的通信原语" class="headerlink" title="二. 分布式训练常见的通信原语"></a>二. 分布式训练常见的通信原语</h4><h5 id="2-1-Reduce"><a href="#2-1-Reduce" class="headerlink" title="2.1 Reduce"></a>2.1 Reduce</h5><p>reduce: 归约。属于多对一通信原语，多个数据发送者, 一个数据接收者。可以在集群内把多个节点的数据归约到一个节点上，如下图, 归约前每个节点有各自的数据，归约后接收节点有所有节点的数据的计算结果。常用于累加，累乘，求最值等操作。</p>
<p> <img src="/2025/03/30/zero-optimizer/image-20250330170740533.png" alt="image-20250330170740533"></p>
<h5 id="2-2-Gather"><a href="#2-2-Gather" class="headerlink" title="2.2 Gather"></a>2.2 Gather</h5><p>Gather：聚合。属于多对一通信，多个数据发送者，一个数据接收者，可以在集群内把多个节点的数据聚合到一个节点上，聚合前每个节点拥有一份完整数据的一部分，聚合后接收节点拥有完整数据，Gather只是把所有多个节点的数据放到一个节点上，reduce会进行映射操作(求和，求最值等)。</p>
<p> <img src="/2025/03/30/zero-optimizer/image-20250330171247355.png" alt="image-20250330171247355">  <img src="/2025/03/30/zero-optimizer/image-20250330171343407.png" alt="image-20250330171343407"> </p>
<h5 id="2-3-Broadcast"><a href="#2-3-Broadcast" class="headerlink" title="2.3 Broadcast"></a>2.3 Broadcast</h5><p>Broadcast: 广播。属于一对多通信原语，一个数据发送者，多个数据接收者。可以在集群内把一个节点的数据广播到其他节点上。广播后每个其他节点具有和发出节点相同的数据。</p>
<p> <img src="/2025/03/30/zero-optimizer/image-20250330171439241.png" alt="image-20250330171439241"></p>
<h5 id="2-4-Scatter"><a href="#2-4-Scatter" class="headerlink" title="2.4 Scatter"></a>2.4 Scatter</h5><p>Scatter：散开。属于一对多通信原语，一个数据发送者，多个数据接收者。可以在集群内把一个节点的数据发散到其他节点上，与Broadcast不同的是Scatter是先切分后发送，每个接收节点只收到发送节点的一部分数据(合起来是完整的)。</p>
<p> <img src="/2025/03/30/zero-optimizer/image-20250330171733698.png" alt="image-20250330171733698"></p>
<h5 id="2-5-ReduceScatter"><a href="#2-5-ReduceScatter" class="headerlink" title="2.5 ReduceScatter"></a>2.5 ReduceScatter</h5><p>ReduceScatter: 属于多对多通信。多个数据发送者，多个数据接收者。先把所有节点的数据通过reduce归约到同个节点，再从这个节点发散到其他节点。最终每个节点都拥有归约后完整数据的一部分。</p>
<p> <img src="/2025/03/30/zero-optimizer/image-20250330171852507.png" alt="image-20250330171852507"></p>
<h5 id="2-6-AllGather"><a href="#2-6-AllGather" class="headerlink" title="2.6 AllGather"></a>2.6 AllGather</h5><p>AllGather: 属于多对多通信。多个数据发送者，多个数据接收者。发送前每个数据有完整数据的一部分，先通过Gather操作把所有数据聚合到一个节点上，再通过Broadcast(广播)发送到所有节点上，AllGather后每个节点都有一份完整数据。</p>
<p> <img src="/2025/03/30/zero-optimizer/image-20250330171935551.png" alt="image-20250330171935551"></p>
<h5 id="2-7-AllReduce"><a href="#2-7-AllReduce" class="headerlink" title="2.7 AllReduce"></a>2.7 AllReduce</h5><p>AllReduce: 属于多对多通信。多个数据发送者，多个数据接收者。发送前每个节点有一份完整数据的旧版本，先归约更新为新版本，再发送到每个节点上。AllReduce后每个节点都有完整数据的新版本。AllReduce可以通过在主节点上进行Reduce+Broadcast(先归约更新数据, 再广播数据)实现, 但这种方式主节点负责所有数据的聚合和广播，会成为性能瓶颈。也可以通过ReduceScatter+allGather(先归约更新，再发散，最后先聚合后广播)，目前多用基于环的ReduceScatter和AllGather，高效实现AllReduce</p>
<p> <img src="/2025/03/30/zero-optimizer/image-20250330172057505.png" alt="image-20250330172057505"></p>
<h4 id="三-Zero优化器"><a href="#三-Zero优化器" class="headerlink" title="三. Zero优化器"></a>三. Zero优化器</h4><p>首先是将ModelStates （包含模型参数，梯度参数，优化器参数）划分到各个GPU上。</p>
<h5 id="3-1-Zero-1"><a href="#3-1-Zero-1" class="headerlink" title="3.1 Zero-1"></a>3.1 Zero-1</h5><p>ZeRO-1: 对优化器参数进行划分，具体步骤如下(N个GPU)：</p>
<ol>
<li>把batch分成N份, 每个GPU一份</li>
<li>执行一步前向和方向传播计算后，每个GPU各得到一份梯度</li>
<li>对梯度执行all-reduce操作，所有节点都有一份完整梯度</li>
<li>每个GPU得到完整梯度G后，对各自的权重进行更新，权重的更新由优化器状态和梯度共同决定，每个GPU只需要存储和更新1/N的优化器状态，并更新1/N的权重(模型参数)。</li>
<li><p>每个GPU维护各自优化器里面更新的参数，最后执行all-gather，使得每个GPU都有更新后的权重。</p>
<p><img src="/2025/03/30/zero-optimizer/image-20250330172409858.png" alt="image-20250330172409858"></p>
</li>
</ol>
<h5 id="3-2-Zero-2"><a href="#3-2-Zero-2" class="headerlink" title="3.2 Zero-2"></a>3.2 Zero-2</h5><p>ZeRO-2: 对优化器和梯度进行划分，具体步骤如下:</p>
<ol>
<li>把batch分成N份，每个GPU一份</li>
<li>执行一步前向和反向传播计算后，每个GPU得到一份梯度</li>
<li>对梯度执行all-reduce，所有节点都有一份完整梯度</li>
<li>聚合之后对梯度进行划分，比如GPU1只负责维护梯度G1，就只需要把G1的新梯度分为GPU1，其他的梯度就不需要保存。</li>
<li>每个GPU用所维护的优化器和梯度更新相应的权重，即每块GPU维护独立的权重。</li>
<li><p>最后对权重进行all-gather, 每个GPU都有完整的更新后的权重。</p>
<p><img src="/2025/03/30/zero-optimizer/image-20250330172704241.png" alt="image-20250330172704241"></p>
</li>
</ol>
<h5 id="3-3-Zero-3"><a href="#3-3-Zero-3" class="headerlink" title="3.3 Zero-3"></a>3.3 Zero-3</h5><p>ZeRO-3: 对优化器，梯度，权重进行划分，具体步骤如下：</p>
<ol>
<li>把batch分成N份，每个GPU一份，模型的权重参数也被分成N份。</li>
<li>在进行前向计算之前，对权重执行all-gather操作取回分布在各GPU上的权重，组成完整的参数进行前向计算，计算完成后，把不属于自身维护的权重抛弃。</li>
<li>在进行反向计算之前，对权重执行all-gather操作取回分布在各GPU上的权重，组成完整的参数进行反向传播计算，计算完成后，把不属于自身维护的权重抛弃。</li>
<li>backward之后得到各自的梯度，对梯度执行all-gather操作，得到聚合的梯度之后更新自身维护的权重，然后把不是自身维护的权重抛弃。</li>
<li><p>由于每个GPU只保存其自身维护的权重参数，因此无需对权重进行all-reduce。<br> 注: ZeRO-3是以通信换显存，ZeRO-3对权重参数进行了切分，但不是张量并行，因为在前向和反向计算时仍然用完整的权重来计算，张量并行在前向和反向时只用部分权重。</p>
<p><img src="/2025/03/30/zero-optimizer/image-20250330172824038.png" alt="image-20250330172824038"></p>
</li>
</ol>
<p>ZeRO-offload: 把显存占用多的部分卸载到CPU内存上，计算和激活值部分放到GPU上。<br>● 高计算: 前向传播和反向传播计算量高，相关权重参数计算和激活值计算仍然在GPU<br>● 低计算: 权重更新部分计算量低，以通信为主，且需要的显存大，放入CPU。</p>
]]></content>
      <categories>
        <category>大模型应用</category>
        <category>概念和原理</category>
      </categories>
  </entry>
</search>
